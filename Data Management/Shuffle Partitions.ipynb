{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf822dd0-cd00-41f1-a381-3a55a2af431a",
   "metadata": {},
   "source": [
    "# Spark Optimisation - Shuffle Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2c616-da17-4b8d-bfcc-2aad26564c81",
   "metadata": {},
   "source": [
    "## What's Shuffling?\n",
    "- It happens when there's a wide transformation\n",
    "- It happens when Spark needs to get the data that resides in different nodes in one place\n",
    "- Say for instance, you want to find the average transaction amount across all transactions then Spark will need to gather all it's partitions that are split across different nodes and then aggregate - this movement of data is called shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb939c-ec0c-4d11-a22a-f7b55b2f44bc",
   "metadata": {},
   "source": [
    "## What is Shuffle partitions\n",
    "- It simply refers to the partitions made during the shuffle operation\n",
    "- It is set using\n",
    "\n",
    "```python\n",
    "spark.sql.shuffle.partitions (default=200)\n",
    "spark.default.parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547712-b297-4c50-9a16-69b3e399c2cf",
   "metadata": {},
   "source": [
    "## So Why is it important to decide the No. of Shuffle Partitions?\n",
    "**Two Main Reasons**\n",
    "- In order to ensure optimal utlisation of resources\n",
    "- To improve job performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510cd0a-5c07-4058-a890-1d8e1d920f50",
   "metadata": {},
   "source": [
    "### Scenario 1\n",
    "- Number of executors = 5\n",
    "- Number of Cores in each Executor = 4\n",
    "- Shuffle Data Size = 300 GB\n",
    "\n",
    "- Total no. of cores = 5*4 = 20\n",
    "- Default no. of shuffle partition = 200\n",
    "- Size per shuffle partition = 300 GB / 200 = 1.5 GB\n",
    "  \n",
    "- ***Note that optimal partition size should be between 1 to 200 MB*** => So we need to tune the number of shuffle partitions\n",
    "    - If we want the size of partition to be 200 MB then we would need: (300 * 1000)/200 = **1500 Shuffle partitions**\n",
    "\n",
    "### Scenario 2\n",
    "- Number of executors = 3\n",
    "- Number of Cores in each Executor = 4\n",
    "- Shuffle Data Size = 50 MB\n",
    "- Total no. of cores = 3*4 = 12\n",
    "- Default no. of shuffle partition = 200\n",
    "- Size per shuffle partition = 50 MB / 200 = 250 KB\n",
    "- ***Now this size is too small***\n",
    "    - You can choose the size of partition and based on that get the no. of shuffle partitions\n",
    "    - **OR**\n",
    "    - In order to use all cores, 50 MB / 12 = 4.2 MB => with this shuffle partition size you can utlise all the cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9b24b-ac61-460d-aebe-8e68420704c2",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "### 1. **Understand the Factors Affecting Shuffle Partitions**\r\n",
    "   - **Data Size**:  \r\n",
    "     Larger datasets typically require more shuffle partitions to distribute the workload evenly and avoid memory bottlenecks.\r\n",
    "   - **Cluster Resources**:\r\n",
    "     - Number of cores: More cores allow for higher parallelism.\r\n",
    "     - Available memory: Ensure each partition is small enough to fit comfortably in memory to avoid disk spill.\r\n",
    "   - **Task Granularity**:\r\n",
    "     - Too few partitions: Results in under-utilization of cluster resources and large tasks that can cause out-of-memory errors.\r\n",
    "     - Too many partitions: Results in many small tasks, increasing scheduling overhead and shuffle I/O costs.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. **Start with Default Values**\r\n",
    "   - For **DataFrame and SQL API**:  \r\n",
    "     The default value for `spark.sql.shuffle.partitions` is **200**.\r\n",
    "   - For **RDD API**:  \r\n",
    "     The default parallelism is typically **2 Ã— the total number of cores** in the cluster.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. **Refine Based on Data Size**\r\n",
    "A general heuristic:\r\n",
    "   - **Partition Size**: Aim for **100 MB to 200 MB** per partition.  \r\n",
    "     This ensures partitions are large enough to minimize overhead but small enough to fit in memory.\r\n",
    "   - Formula:  \r\n",
    "     \\[\r\n",
    "     \\text{Shuffle Partitions} = \\frac{\\text{Total Data Size (in bytes)}}{\\text{Desired Partition Size (in bytes)}}\r\n",
    "     \\]\r\n",
    "     Example: If your data is 1 TB and you want ~128 MB partitions:\r\n",
    "     \\[\r\n",
    "     \\text{Shuffle Partitions} = \\frac{1 \\, \\text{TB}}{128 \\, \\text{MB}} = 8192 \\, \\text{partitions.}\r\n",
    "     \\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. **Analyze Task Performance**\r\n",
    "Use the **Spark UI** to monitor:\r\n",
    "   - **Task Duration**: \r\n",
    "     - Long-running tasks suggest partitions are too large.\r\n",
    "   - **Task Failure (Out-of-Memory Errors)**: \r\n",
    "     - Indicates partitions are too large to fit in memory.\r\n",
    "   - **Shuffle Write/Read Metrics**:\r\n",
    "     - Excessive shuffle write/read indicates unnecessary overhead, potentially from too many partitions.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. **Tune Dynamically**\r\n",
    "Adjust shuffle partitions dynamically based on runtime behavior:\r\n",
    "   - **DataFrame and SQL API**:\r\n",
    "     ```python\r\n",
    "     spark.conf.set(\"spark.sql.shuffle.partitions\", <new_value>)\r\n",
    "     ```\r\n",
    "   - **RDD API**:\r\n",
    "     Use `repartition()` or `coalesce()`:\r\n",
    "     - `repartition(n)`: Increases the number of partitions.\r\n",
    "     - `coalesce(n)`: Reduces the number of partitions without a shuffle.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. **Cluster Resource Awareness**\r\n",
    "   - Number of partitions should generally be **2-3 times the total number of cores** in the cluster.  \r\n",
    "     Example: For a cluster with 16 cores, aim for **32-48 partitions**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 7. **Workload-Specific Tuning**\r\n",
    "   - **Batch Processing**:  \r\n",
    "     Higher shuffle partitions to maximize parallelism (e.g., >200 for large datasets).\r\n",
    "   - **Interactive Queries**:  \r\n",
    "     Lower shuffle partitions to reduce latency.\r\n",
    "   - **Joins or Aggregations**:  \r\n",
    "     Fine-tune based on data skew and size. Use **adaptive query execution (AQE)** to dynamically adjust partitions.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 8. **Enable Adaptive Query Execution (AQE)**\r\n",
    "If using Spark 3.0 or later, enable **AQE** to let Spark dynamically optimize shuffle partitions based on runtime statistics:\r\n",
    "   ```python\r\n",
    "   spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\r\n",
    "   ```\r\n",
    "   Key AQE features:\r\n",
    "   - Dynamically adjusts the number of shuffle partitions.\r\n",
    "   - Handles data skew automatically.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Example: Tuning Shuffle Partitions\r\n",
    "1. Data Size: 1 TB\r\n",
    "2. Partition Size: ~128 MB\r\n",
    "3. Formula: \\( 1 \\, \\text{TB} / 128 \\, \\text{MB} = 8192 \\)\r\n",
    "4. Cluster Resources:  \r\n",
    "   -ssary.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Would you like help estimating the partitions for a specific Spark job or dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee269bb-3ad9-49f3-8302-7defaece13aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
