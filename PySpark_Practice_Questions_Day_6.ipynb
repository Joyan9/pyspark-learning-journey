{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6gPpuOTGRDQVbqyf6kPJz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "3d67dffc-15e0-4aab-aa60-cc31a36b28c2"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\u001b[33m\r0% [2 InRelease 88.0 kB/128 kB 69%] [3 InRelease 14.2 kB/129 kB 11%] [Connected\u001b[0m\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\u001b[0m\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,864 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.3 kB]\n",
            "Fetched 20.8 MB in 5s (3,837 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "c2c1ed4e-9a7f-4e04-8c6f-44feb7ae609c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩 **Day 6 - 2025/04/29**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Multi-level Aggregation and Prediction\n",
        "Build a recommendation engine system that analyzes purchase patterns. Your input is a series of user-product interactions with timestamps and you need to find products that are frequently bought together within a 30-day window, then calculate the probability of future purchases based on past behavior.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, count, sum, when, collect_list, explode, array\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# User purchase history data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A123\", \"2024-01-15\", 29.99),\n",
        "    (101, \"B456\", \"2024-01-18\", 15.50),\n",
        "    (101, \"C789\", \"2024-02-02\", 45.00),\n",
        "    (102, \"A123\", \"2024-01-05\", 29.99),\n",
        "    (102, \"D012\", \"2024-01-20\", 18.75),\n",
        "    (102, \"B456\", \"2024-02-10\", 15.50),\n",
        "    (103, \"B456\", \"2024-02-01\", 15.50),\n",
        "    (103, \"C789\", \"2024-02-05\", 45.00),\n",
        "    (103, \"E345\", \"2024-02-15\", 60.25),\n",
        "    (104, \"A123\", \"2024-01-10\", 29.99),\n",
        "    (104, \"C789\", \"2024-01-25\", 45.00),\n",
        "    (104, \"E345\", \"2024-02-20\", 60.25)\n",
        "], [\"user_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "```\n",
        "\n",
        "Expected output (showing product pairs that are purchased together and their future purchase probability):\n",
        "```\n",
        "+----------+----------+--------------------+--------------------+\n",
        "|product_1 |product_2 |purchase_frequency  |purchase_probability|\n",
        "+----------+----------+--------------------+--------------------+\n",
        "|A123      |C789      |2                   |0.667               |\n",
        "|B456      |C789      |2                   |0.667               |\n",
        "|A123      |B456      |1                   |0.333               |\n",
        "...\n",
        "+----------+----------+--------------------+--------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, count, sum, when, collect_list, explode, array\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# User purchase history data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A123\", \"2024-01-15\", 29.99),\n",
        "    (101, \"B456\", \"2024-01-18\", 15.50),\n",
        "    (101, \"C789\", \"2024-02-02\", 45.00),\n",
        "    (102, \"A123\", \"2024-01-05\", 29.99),\n",
        "    (102, \"D012\", \"2024-01-20\", 18.75),\n",
        "    (102, \"B456\", \"2024-02-10\", 15.50),\n",
        "    (103, \"B456\", \"2024-02-01\", 15.50),\n",
        "    (103, \"C789\", \"2024-02-05\", 45.00),\n",
        "    (103, \"E345\", \"2024-02-15\", 60.25),\n",
        "    (104, \"A123\", \"2024-01-10\", 29.99),\n",
        "    (104, \"C789\", \"2024-01-25\", 45.00),\n",
        "    (104, \"E345\", \"2024-02-20\", 60.25)\n",
        "], [\"user_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "\n",
        "purchases.printSchema()\n",
        "\n",
        "purchases.show()"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed002dc-863d-4b90-d62b-3c9e08717dff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- purchase_date: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            "\n",
            "+-------+----------+-------------+------+\n",
            "|user_id|product_id|purchase_date|amount|\n",
            "+-------+----------+-------------+------+\n",
            "|    101|      A123|   2024-01-15| 29.99|\n",
            "|    101|      B456|   2024-01-18|  15.5|\n",
            "|    101|      C789|   2024-02-02|  45.0|\n",
            "|    102|      A123|   2024-01-05| 29.99|\n",
            "|    102|      D012|   2024-01-20| 18.75|\n",
            "|    102|      B456|   2024-02-10|  15.5|\n",
            "|    103|      B456|   2024-02-01|  15.5|\n",
            "|    103|      C789|   2024-02-05|  45.0|\n",
            "|    103|      E345|   2024-02-15| 60.25|\n",
            "|    104|      A123|   2024-01-10| 29.99|\n",
            "|    104|      C789|   2024-01-25|  45.0|\n",
            "|    104|      E345|   2024-02-20| 60.25|\n",
            "+-------+----------+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, abs, datediff, least, greatest\n",
        "\n",
        "\n",
        "# convert to date\n",
        "purchases = purchases.withColumn(\"purchase_date\", F.to_date(\"purchase_date\"))\n",
        "\n",
        "p1 = purchases.alias(\"p1\")\n",
        "p2 = purchases.alias(\"p2\")\n",
        "\n",
        "# self join the purchase data for the same user where the difference between purchases is less 30 days and not identical products\n",
        "product_pairs = p1.join(\n",
        "    p2,\n",
        "    on=\"user_id\"\n",
        ").filter(\n",
        "    (col(\"p1.product_id\") != col(\"p2.product_id\")) &\n",
        "    (abs(datediff(col(\"p1.purchase_date\"), col(\"p2.purchase_date\"))) <= 30)\n",
        ")\n",
        "\n",
        "# we cannot directly use the p1.product_id and p2.product_id to generate product pairs as there can be duplicate pairs\n",
        "# A123-B456 is same as B456-A123\n",
        "# therefore we use least and greatest function which returns the min and max values from a list\n",
        "product_pairs = product_pairs.withColumn(\"product_1\", least(col(\"p1.product_id\"), col(\"p2.product_id\"))) \\\n",
        "                             .withColumn(\"product_2\", greatest(col(\"p1.product_id\"), col(\"p2.product_id\")))\n",
        "\n",
        "# for each user shows us only the distinct pair of products\n",
        "unique_product_pairs = product_pairs.select(\"user_id\", \"product_1\", \"product_2\").distinct()\n",
        "\n",
        "purchase_frequency = unique_product_pairs.groupby([\"product_1\", \"product_2\"]).agg(count(\"user_id\").alias(\"purchase_frequency\"))\n",
        "purchase_frequency.show()\n",
        "\n",
        "# purchase_probability = frequency / product_1_user_count\n",
        "product_1_user_count = unique_product_pairs.groupBy(\"product_1\").agg(count(\"user_id\").alias(\"product_1_user_count\"))"
      ],
      "metadata": {
        "id": "ThjqMyzPzvLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847399ba-8f6b-4732-f987-89697d78cdfe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+------------------+\n",
            "|product_1|product_2|purchase_frequency|\n",
            "+---------+---------+------------------+\n",
            "|     A123|     C789|                 2|\n",
            "|     B456|     C789|                 2|\n",
            "|     A123|     D012|                 1|\n",
            "|     A123|     B456|                 1|\n",
            "|     B456|     E345|                 1|\n",
            "|     C789|     E345|                 2|\n",
            "|     B456|     D012|                 1|\n",
            "+---------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're aiming to answer:\n",
        "\n",
        "> **“Given that a user bought `product_1`, what is the probability that they also bought `product_2` within 30 days?”**\n",
        "\n",
        "This is a **conditional probability**, written as:\n",
        "\n",
        "\\[\n",
        "P(\\text{product}_2 \\mid \\text{product}_1) = \\frac{\\text{Users who bought both}}{\\text{Users who bought product}_1}\n",
        "\\]\n",
        "\n",
        "So:\n",
        "\n",
        "- The **numerator** is the count of users who bought both products within the 30-day window.\n",
        "- The **denominator** is the number of users who bought `product_1` at all — which is what you computed in the `product_1_user_count`."
      ],
      "metadata": {
        "id": "xZgzadCqnxwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = purchase_frequency.join(product_1_user_count, on=\"product_1\") \\\n",
        "    .withColumn(\"purchase_probability\", F.round(F.col(\"purchase_frequency\") / F.col(\"product_1_user_count\"), 3)) \\\n",
        "    .select(\"product_1\", \"product_2\", \"purchase_frequency\", \"purchase_probability\")\n",
        "\n",
        "final_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l5ZcfWWn9pJ",
        "outputId": "5c30c50d-1c8a-45ca-cd20-1ad9e9fcec74"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+------------------+--------------------+\n",
            "|product_1|product_2|purchase_frequency|purchase_probability|\n",
            "+---------+---------+------------------+--------------------+\n",
            "|     B456|     D012|                 1|                0.25|\n",
            "|     B456|     E345|                 1|                0.25|\n",
            "|     B456|     C789|                 2|                 0.5|\n",
            "|     A123|     B456|                 1|                0.25|\n",
            "|     A123|     D012|                 1|                0.25|\n",
            "|     A123|     C789|                 2|                 0.5|\n",
            "|     C789|     E345|                 2|                 1.0|\n",
            "+---------+---------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Streaming Data Quality Monitoring\n",
        "Implement a streaming data quality monitoring system that validates incoming data against predefined rules, detects outliers, and tracks data drift over time. The system should maintain a dynamic baseline and alert when data deviates significantly from historical patterns.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean, stddev, abs, count, when, window\n",
        "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DoubleType\n",
        "\n",
        "# Schema for sensor readings\n",
        "schema = StructType([\n",
        "    StructField(\"device_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"temperature\", DoubleType(), True),\n",
        "    StructField(\"humidity\", DoubleType(), True),\n",
        "    StructField(\"pressure\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Rules for data quality checks\n",
        "rules = [\n",
        "    {\"field\": \"temperature\", \"min\": -30.0, \"max\": 50.0},\n",
        "    {\"field\": \"humidity\", \"min\": 0.0, \"max\": 100.0},\n",
        "    {\"field\": \"pressure\", \"min\": 900.0, \"max\": 1100.0}\n",
        "]\n",
        "\n",
        "# Sample of historical baseline statistics (pre-calculated)\n",
        "baseline_stats = spark.createDataFrame([\n",
        "    (\"temperature\", 22.5, 3.2),\n",
        "    (\"humidity\", 45.2, 8.5),\n",
        "    (\"pressure\", 1013.2, 5.1)\n",
        "], [\"field\", \"baseline_mean\", \"baseline_stddev\"])\n",
        "\n",
        "# Simulate streaming data\n",
        "sensor_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "sensor_data = sensor_data.selectExpr(\n",
        "    \"cast(value % 5 + 1 as string) as device_id\",\n",
        "    \"timestamp\",\n",
        "    \"cast(value % 100 as double) as temperature\",\n",
        "    \"cast(value % 100 * 0.8 + 10 as double) as humidity\",\n",
        "    \"cast(value % 10 + 1000 as double) as pressure\"\n",
        ")\n",
        "```\n",
        "\n",
        "The solution should:\n",
        "1. Apply rule-based validation for each field\n",
        "2. Detect statistical outliers (values > 3 standard deviations from baseline)\n",
        "3. Track data drift using a sliding window and compare with the baseline\n",
        "4. Generate alerts when significant deviations are detected"
      ],
      "metadata": {
        "id": "pi9E7iYAlhrj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_3Dd3fC6XAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Graph Analysis for Fraud Detection\n",
        "Implement a fraud detection system using graph analysis techniques to identify suspicious patterns in transaction data. The system should detect cycles (money moving in circles), unusual transaction frequencies, and anomalous transaction amounts.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, collect_list, struct, to_json, from_json\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, TimestampType, DoubleType\n",
        "\n",
        "# Transaction data\n",
        "transactions = spark.createDataFrame([\n",
        "    (\"T1001\", \"ACC_A\", \"ACC_B\", \"2024-03-01 08:32:15\", 5000.00),\n",
        "    (\"T1002\", \"ACC_B\", \"ACC_C\", \"2024-03-01 09:15:42\", 4800.00),\n",
        "    (\"T1003\", \"ACC_C\", \"ACC_D\", \"2024-03-01 10:05:30\", 4700.00),\n",
        "    (\"T1004\", \"ACC_D\", \"ACC_A\", \"2024-03-01 11:22:18\", 4650.00),  # Cycle completes\n",
        "    (\"T1005\", \"ACC_E\", \"ACC_F\", \"2024-03-01 13:45:22\", 12500.00),\n",
        "    (\"T1006\", \"ACC_G\", \"ACC_H\", \"2024-03-01 14:12:55\", 8750.00),\n",
        "    (\"T1007\", \"ACC_H\", \"ACC_I\", \"2024-03-01 15:30:17\", 8500.00),\n",
        "    (\"T1008\", \"ACC_I\", \"ACC_G\", \"2024-03-01 16:25:40\", 8250.00),  # Cycle completes\n",
        "    (\"T1009\", \"ACC_J\", \"ACC_K\", \"2024-03-01 17:10:33\", 3000.00),\n",
        "    (\"T1010\", \"ACC_L\", \"ACC_J\", \"2024-03-01 18:05:12\", 9500.00)\n",
        "], [\"transaction_id\", \"from_account\", \"to_account\", \"timestamp\", \"amount\"])\n",
        "\n",
        "# Account risk scores (precomputed based on history)\n",
        "account_risk = spark.createDataFrame([\n",
        "    (\"ACC_A\", 0.2),\n",
        "    (\"ACC_B\", 0.3),\n",
        "    (\"ACC_C\", 0.4),\n",
        "    (\"ACC_D\", 0.6),\n",
        "    (\"ACC_E\", 0.1),\n",
        "    (\"ACC_F\", 0.2),\n",
        "    (\"ACC_G\", 0.7),\n",
        "    (\"ACC_H\", 0.8),\n",
        "    (\"ACC_I\", 0.7),\n",
        "    (\"ACC_J\", 0.3),\n",
        "    (\"ACC_K\", 0.2),\n",
        "    (\"ACC_L\", 0.1)\n",
        "], [\"account_id\", \"risk_score\"])\n",
        "```\n",
        "\n",
        "Your solution should:\n",
        "1. Detect transaction cycles (money flowing in circular patterns)\n",
        "2. Identify accounts with high transaction frequency within short time periods\n",
        "3. Flag transactions with amounts that gradually decrease in a cycle\n",
        "4. Calculate a fraud risk score based on graph pattern detection\n",
        "5. Output suspicious transaction patterns ranked by risk"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTm-Krpq6X8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Real-time Anomaly Detection with Dynamic Thresholds\n",
        "Build a system that detects anomalies in time series data from multiple sensors, with dynamically adjusting thresholds based on seasonality patterns and contextual factors. The system should learn normal behavior patterns and adjust sensitivity based on time of day and operational conditions.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, window, avg, stddev, expr, hour, date_format\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
        "\n",
        "# Schema for sensor readings\n",
        "schema = StructType([\n",
        "    StructField(\"sensor_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"reading\", DoubleType(), False),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"operational_mode\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Historical summary statistics by hour and operational mode\n",
        "historical_patterns = spark.createDataFrame([\n",
        "    (\"S1\", 0, 1, 22.5, 3.2),  # Sensor S1, midnight hour, mode 1\n",
        "    (\"S1\", 0, 2, 21.2, 2.5),  # Sensor S1, midnight hour, mode 2\n",
        "    (\"S1\", 12, 1, 35.8, 4.5), # Sensor S1, noon hour, mode 1\n",
        "    (\"S1\", 12, 2, 32.5, 3.8), # Sensor S1, noon hour, mode 2\n",
        "    (\"S2\", 0, 1, 45.2, 5.1),\n",
        "    (\"S2\", 0, 2, 42.5, 4.8),\n",
        "    (\"S2\", 12, 1, 62.3, 7.2),\n",
        "    (\"S2\", 12, 2, 58.7, 6.5)\n",
        "], [\"sensor_id\", \"hour_of_day\", \"operational_mode\", \"historical_mean\", \"historical_stddev\"])\n",
        "\n",
        "# Context factors that influence thresholds\n",
        "context_adjustments = spark.createDataFrame([\n",
        "    (\"S1\", \"ZONE_A\", 1.2),  # Multiply threshold by 1.2 for S1 in ZONE_A\n",
        "    (\"S1\", \"ZONE_B\", 0.9),  # Reduce threshold by 10% for S1 in ZONE_B\n",
        "    (\"S2\", \"ZONE_A\", 1.1),\n",
        "    (\"S2\", \"ZONE_B\", 0.8)\n",
        "], [\"sensor_id\", \"location\", \"threshold_multiplier\"])\n",
        "\n",
        "# Sample batch of incoming sensor data for testing\n",
        "current_readings = spark.createDataFrame([\n",
        "    (\"S1\", \"2024-04-01 00:05:23\", 25.8, \"ZONE_A\", 1),\n",
        "    (\"S1\", \"2024-04-01 00:15:45\", 24.2, \"ZONE_A\", 1),\n",
        "    (\"S1\", \"2024-04-01 00:25:12\", 35.7, \"ZONE_A\", 1),  # Anomaly!\n",
        "    (\"S1\", \"2024-04-01 12:10:33\", 36.2, \"ZONE_B\", 1),\n",
        "    (\"S1\", \"2024-04-01 12:20:56\", 50.5, \"ZONE_B\", 1),  # Anomaly!\n",
        "    (\"S2\", \"2024-04-01 00:03:41\", 46.5, \"ZONE_A\", 2),\n",
        "    (\"S2\", \"2024-04-01 00:13:25\", 80.2, \"ZONE_A\", 2),  # Anomaly!\n",
        "    (\"S2\", \"2024-04-01 12:07:19\", 61.8, \"ZONE_B\", 2)\n",
        "], [\"sensor_id\", \"timestamp\", \"reading\", \"location\", \"operational_mode\"])\n",
        "```\n",
        "\n",
        "Your solution should:\n",
        "1. Detect point anomalies using dynamic Z-score thresholds adjusted by time and context\n",
        "2. Identify contextual anomalies by comparing with historical patterns\n",
        "3. Calculate an anomaly severity score for each detected anomaly\n",
        "4. Consider operational mode and location context when determining thresholds"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNhXJFlnOD2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Incremental ML Model Monitoring and Retraining\n",
        "Implement a system that monitors ML model performance in production, detects model drift, and triggers retraining when needed. The system should analyze prediction errors, feature distributions, and business KPIs to determine when model retraining is necessary.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, stddev, sum, abs, expr, datediff, to_date\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType, BooleanType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Model metadata - current production model\n",
        "model_metadata = {\n",
        "    \"model_id\": \"CLF-2024-03\",\n",
        "    \"deployment_date\": \"2024-03-01\",\n",
        "    \"features\": [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"],\n",
        "    \"baseline_auc\": 0.85,\n",
        "    \"baseline_precision\": 0.82,\n",
        "    \"baseline_recall\": 0.78,\n",
        "    \"baseline_feature_distributions\": {\n",
        "        \"feature_1\": {\"mean\": 0.52, \"stddev\": 0.21},\n",
        "        \"feature_2\": {\"mean\": 25.4, \"stddev\": 12.3},\n",
        "        \"feature_3\": {\"mean\": 0.65, \"stddev\": 0.18},\n",
        "        \"feature_4\": {\"mean\": 42.1, \"stddev\": 15.6}\n",
        "    },\n",
        "    \"performance_threshold\": 0.05  # Acceptable performance degradation threshold\n",
        "}\n",
        "\n",
        "# Historical model predictions and outcomes\n",
        "historical_predictions = spark.createDataFrame([\n",
        "    (\"2024-03-05\", \"user_123\", 0.82, 1, 0.51, 24.8, 0.67, 43.2, 500),\n",
        "    (\"2024-03-05\", \"user_456\", 0.35, 0, 0.48, 22.5, 0.62, 40.1, 250),\n",
        "    # ... more historical data\n",
        "], [\"prediction_date\", \"user_id\", \"probability\", \"actual_outcome\",\n",
        "    \"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"transaction_amount\"])\n",
        "\n",
        "# Recent model predictions and outcomes (for drift detection)\n",
        "recent_predictions = spark.createDataFrame([\n",
        "    (\"2024-04-15\", \"user_789\", 0.78, 1, 0.55, 26.2, 0.68, 44.5, 650),\n",
        "    (\"2024-04-15\", \"user_101\", 0.42, 1, 0.49, 21.8, 0.61, 38.9, 300),\n",
        "    (\"2024-04-16\", \"user_112\", 0.83, 0, 0.56, 27.5, 0.71, 45.2, 700),\n",
        "    (\"2024-04-16\", \"user_131\", 0.39, 1, 0.50, 23.2, 0.63, 41.3, 400),\n",
        "    (\"2024-04-17\", \"user_415\", 0.81, 1, 0.54, 26.8, 0.69, 44.8, 600),\n",
        "    (\"2024-04-17\", \"user_167\", 0.32, 0, 0.47, 21.5, 0.60, 39.5, 200),\n",
        "    (\"2024-04-18\", \"user_189\", 0.85, 1, 0.57, 28.2, 0.72, 46.0, 750),\n",
        "    (\"2024-04-18\", \"user_215\", 0.37, 0, 0.48, 22.0, 0.62, 40.5, 350),\n",
        "    (\"2024-04-19\", \"user_227\", 0.76, 0, 0.53, 25.5, 0.67, 43.5, 550),\n",
        "    (\"2024-04-19\", \"user_239\", 0.34, 1, 0.46, 21.2, 0.59, 38.7, 150),\n",
        "    # Several predictions with different feature values to test feature drift\n",
        "    (\"2024-04-20\", \"user_241\", 0.77, 1, 0.65, 35.2, 0.80, 58.5, 800),\n",
        "    (\"2024-04-20\", \"user_253\", 0.81, 0, 0.67, 36.8, 0.82, 59.2, 850),\n",
        "    (\"2024-04-20\", \"user_265\", 0.79, 1, 0.66, 35.5, 0.81, 58.8, 820),\n",
        "    (\"2024-04-20\", \"user_277\", 0.82, 1, 0.69, 37.0, 0.83, 60.0, 900),\n",
        "    (\"2024-04-20\", \"user_289\", 0.80, 0, 0.68, 36.5, 0.82, 59.5, 880)\n",
        "], [\"prediction_date\", \"user_id\", \"probability\", \"actual_outcome\",\n",
        "    \"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"transaction_amount\"])\n",
        "```\n",
        "\n",
        "Hint: Your solution should implement various drift detection techniques, including:\n",
        "1. Performance drift (AUC, precision, recall degradation)\n",
        "2. Feature distribution drift (statistical tests)\n",
        "3. Prediction distribution drift\n",
        "4. Error analysis by features and segments\n",
        "5. A retraining decision algorithm that considers multiple factors"
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lQRgsLU04FZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}