{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVZoqcGv3WbiDM7oVRhr8q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "71a03c43-b108-41de-b5db-f96a5a5c2bed"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,607 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,869 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Fetched 22.4 MB in 13s (1,772 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "60612678-139a-455c-ad83-44e0256054cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩 **Day 6 - 2025/04/29**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Multi-level Aggregation and Prediction\n",
        "Build a recommendation engine system that analyzes purchase patterns. Your input is a series of user-product interactions with timestamps and you need to find products that are frequently bought together within a 30-day window, then calculate the probability of future purchases based on past behavior.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, count, sum, when, collect_list, explode, array\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# User purchase history data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A123\", \"2024-01-15\", 29.99),\n",
        "    (101, \"B456\", \"2024-01-18\", 15.50),\n",
        "    (101, \"C789\", \"2024-02-02\", 45.00),\n",
        "    (102, \"A123\", \"2024-01-05\", 29.99),\n",
        "    (102, \"D012\", \"2024-01-20\", 18.75),\n",
        "    (102, \"B456\", \"2024-02-10\", 15.50),\n",
        "    (103, \"B456\", \"2024-02-01\", 15.50),\n",
        "    (103, \"C789\", \"2024-02-05\", 45.00),\n",
        "    (103, \"E345\", \"2024-02-15\", 60.25),\n",
        "    (104, \"A123\", \"2024-01-10\", 29.99),\n",
        "    (104, \"C789\", \"2024-01-25\", 45.00),\n",
        "    (104, \"E345\", \"2024-02-20\", 60.25)\n",
        "], [\"user_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "```\n",
        "\n",
        "Expected output (showing product pairs that are purchased together and their future purchase probability):\n",
        "```\n",
        "+----------+----------+--------------------+--------------------+\n",
        "|product_1 |product_2 |purchase_frequency  |purchase_probability|\n",
        "+----------+----------+--------------------+--------------------+\n",
        "|A123      |C789      |2                   |0.667               |\n",
        "|B456      |C789      |2                   |0.667               |\n",
        "|A123      |B456      |1                   |0.333               |\n",
        "...\n",
        "+----------+----------+--------------------+--------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, count, sum, when, collect_list, explode, array\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# User purchase history data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A123\", \"2024-01-15\", 29.99),\n",
        "    (101, \"B456\", \"2024-01-18\", 15.50),\n",
        "    (101, \"C789\", \"2024-02-02\", 45.00),\n",
        "    (102, \"A123\", \"2024-01-05\", 29.99),\n",
        "    (102, \"D012\", \"2024-01-20\", 18.75),\n",
        "    (102, \"B456\", \"2024-02-10\", 15.50),\n",
        "    (103, \"B456\", \"2024-02-01\", 15.50),\n",
        "    (103, \"C789\", \"2024-02-05\", 45.00),\n",
        "    (103, \"E345\", \"2024-02-15\", 60.25),\n",
        "    (104, \"A123\", \"2024-01-10\", 29.99),\n",
        "    (104, \"C789\", \"2024-01-25\", 45.00),\n",
        "    (104, \"E345\", \"2024-02-20\", 60.25)\n",
        "], [\"user_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "\n",
        "purchases.printSchema()\n",
        "\n",
        "purchases.show()"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951cee70-f87f-4463-d8aa-5d1447030d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- purchase_date: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            "\n",
            "+-------+----------+-------------+------+\n",
            "|user_id|product_id|purchase_date|amount|\n",
            "+-------+----------+-------------+------+\n",
            "|    101|      A123|   2024-01-15| 29.99|\n",
            "|    101|      B456|   2024-01-18|  15.5|\n",
            "|    101|      C789|   2024-02-02|  45.0|\n",
            "|    102|      A123|   2024-01-05| 29.99|\n",
            "|    102|      D012|   2024-01-20| 18.75|\n",
            "|    102|      B456|   2024-02-10|  15.5|\n",
            "|    103|      B456|   2024-02-01|  15.5|\n",
            "|    103|      C789|   2024-02-05|  45.0|\n",
            "|    103|      E345|   2024-02-15| 60.25|\n",
            "|    104|      A123|   2024-01-10| 29.99|\n",
            "|    104|      C789|   2024-01-25|  45.0|\n",
            "|    104|      E345|   2024-02-20| 60.25|\n",
            "+-------+----------+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, abs, datediff, least, greatest\n",
        "\n",
        "\n",
        "# convert to date\n",
        "purchases = purchases.withColumn(\"purchase_date\", F.to_date(\"purchase_date\"))\n",
        "\n",
        "p1 = purchases.alias(\"p1\")\n",
        "p2 = purchases.alias(\"p2\")\n",
        "\n",
        "# self join the purchase data for the same user where the difference between purchases is less 30 days and not identical products\n",
        "product_pairs = p1.join(\n",
        "    p2,\n",
        "    on=\"user_id\"\n",
        ").filter(\n",
        "    (col(\"p1.product_id\") != col(\"p2.product_id\")) &\n",
        "    (abs(datediff(col(\"p1.purchase_date\"), col(\"p2.purchase_date\"))) <= 30)\n",
        ")\n",
        "\n",
        "# we cannot directly use the p1.product_id and p2.product_id to generate product pairs as there can be duplicate pairs\n",
        "# A123-B456 is same as B456-A123\n",
        "# therefore we use least and greatest function which returns the min and max values from a list\n",
        "product_pairs = product_pairs.withColumn(\"product_1\", least(col(\"p1.product_id\"), col(\"p2.product_id\"))) \\\n",
        "                             .withColumn(\"product_2\", greatest(col(\"p1.product_id\"), col(\"p2.product_id\")))\n",
        "\n",
        "# for each user shows us only the distinct pair of products\n",
        "unique_product_pairs = product_pairs.select(\"user_id\", \"product_1\", \"product_2\").distinct()\n",
        "\n",
        "purchase_frequency = unique_product_pairs.groupby([\"product_1\", \"product_2\"]).agg(count(\"user_id\").alias(\"purchase_frequency\"))\n",
        "purchase_frequency.show()\n",
        "\n",
        "# purchase_probability = frequency / product_1_user_count\n",
        "product_1_user_count = unique_product_pairs.groupBy(\"product_1\").agg(count(\"user_id\").alias(\"product_1_user_count\"))"
      ],
      "metadata": {
        "id": "ThjqMyzPzvLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0c03c7-9830-47d8-8ca6-87ee797d7733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+------------------+\n",
            "|product_1|product_2|purchase_frequency|\n",
            "+---------+---------+------------------+\n",
            "|     A123|     C789|                 2|\n",
            "|     B456|     C789|                 2|\n",
            "|     A123|     D012|                 1|\n",
            "|     A123|     B456|                 1|\n",
            "|     B456|     E345|                 1|\n",
            "|     C789|     E345|                 2|\n",
            "|     B456|     D012|                 1|\n",
            "+---------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're aiming to answer:\n",
        "\n",
        "> **“Given that a user bought `product_1`, what is the probability that they also bought `product_2` within 30 days?”**\n",
        "\n",
        "This is a **conditional probability**, written as:\n",
        "```latex\n",
        "\\[\n",
        "P(\\text{product}_2 \\mid \\text{product}_1) = \\frac{\\text{Users who bought both}}{\\text{Users who bought product}_1}\n",
        "\\]\n",
        "```\n",
        "So:\n",
        "\n",
        "- The **numerator** is the count of users who bought both products within the 30-day window.\n",
        "- The **denominator** is the number of users who bought `product_1` at all — which is what you computed in the `product_1_user_count`."
      ],
      "metadata": {
        "id": "xZgzadCqnxwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = purchase_frequency.join(product_1_user_count, on=\"product_1\") \\\n",
        "    .withColumn(\"purchase_probability\", F.round(F.col(\"purchase_frequency\") / F.col(\"product_1_user_count\"), 3)) \\\n",
        "    .select(\"product_1\", \"product_2\", \"purchase_frequency\", \"purchase_probability\")\n",
        "\n",
        "final_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l5ZcfWWn9pJ",
        "outputId": "0e8028fc-7f20-403e-8e20-c999fdcdd7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+------------------+--------------------+\n",
            "|product_1|product_2|purchase_frequency|purchase_probability|\n",
            "+---------+---------+------------------+--------------------+\n",
            "|     B456|     D012|                 1|                0.25|\n",
            "|     B456|     E345|                 1|                0.25|\n",
            "|     B456|     C789|                 2|                 0.5|\n",
            "|     A123|     B456|                 1|                0.25|\n",
            "|     A123|     D012|                 1|                0.25|\n",
            "|     A123|     C789|                 2|                 0.5|\n",
            "|     C789|     E345|                 2|                 1.0|\n",
            "+---------+---------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Customer Purchase Segmentation\n",
        "Create a segmentation analysis for customers based on their purchase behavior. Identify customers who are \"high value,\" \"frequent buyers,\" \"occasional buyers,\" and \"one-time purchasers\" based on total spending and purchase frequency.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, sum, when, col, datediff, max, min\n",
        "\n",
        "# Sample customer purchase data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A1\", \"2024-01-15\", 120.50),\n",
        "    (101, \"B2\", \"2024-02-05\", 85.75),\n",
        "    (101, \"C3\", \"2024-03-20\", 210.25),\n",
        "    (102, \"A1\", \"2024-01-20\", 45.99),\n",
        "    (102, \"D4\", \"2024-01-25\", 15.25),\n",
        "    (103, \"B2\", \"2024-02-10\", 85.75),\n",
        "    (103, \"E5\", \"2024-03-05\", 150.00),\n",
        "    (103, \"F6\", \"2024-04-12\", 95.50),\n",
        "    (103, \"G7\", \"2024-04-25\", 120.25),\n",
        "    (104, \"H8\", \"2024-02-28\", 35.50),\n",
        "    (105, \"I9\", \"2024-03-15\", 450.75),\n",
        "    (105, \"J10\", \"2024-04-02\", 320.25)\n",
        "], [\"customer_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+------------+----------------+------------------+----------------+\n",
        "|customer_id |total_spent     |purchase_frequency|segment         |\n",
        "+------------+----------------+------------------+----------------+\n",
        "|101         |416.50          |3                 |high_value      |\n",
        "|102         |61.24           |2                 |occasional_buyer|\n",
        "|103         |451.50          |4                 |frequent_buyer  |\n",
        "|104         |35.50           |1                 |one_time        |\n",
        "|105         |771.00          |2                 |high_value      |\n",
        "+------------+----------------+------------------+----------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, sum, when, col, datediff, max, min\n",
        "\n",
        "# Sample customer purchase data\n",
        "purchases = spark.createDataFrame([\n",
        "    (101, \"A1\", \"2024-01-15\", 120.50),\n",
        "    (101, \"B2\", \"2024-02-05\", 85.75),\n",
        "    (101, \"C3\", \"2024-03-20\", 210.25),\n",
        "    (102, \"A1\", \"2024-01-20\", 45.99),\n",
        "    (102, \"D4\", \"2024-01-25\", 15.25),\n",
        "    (103, \"B2\", \"2024-02-10\", 85.75),\n",
        "    (103, \"E5\", \"2024-03-05\", 150.00),\n",
        "    (103, \"F6\", \"2024-04-12\", 95.50),\n",
        "    (103, \"G7\", \"2024-04-25\", 120.25),\n",
        "    (104, \"H8\", \"2024-02-28\", 35.50),\n",
        "    (105, \"I9\", \"2024-03-15\", 450.75),\n",
        "    (105, \"J10\", \"2024-04-02\", 320.25)\n",
        "], [\"customer_id\", \"product_id\", \"purchase_date\", \"amount\"])\n",
        "\n",
        "purchases.printSchema()"
      ],
      "metadata": {
        "id": "OTm-Krpq6X8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b19345e-9673-4671-8cc2-3ad260597700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- purchase_date: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change date type\n",
        "purchases = purchases.withColumn(\"purchase_date\", F.to_date(\"purchase_date\"))\n",
        "\n",
        "# aggregate per customer\n",
        "purchases_aggregated = purchases.groupBy(\"customer_id\").agg(\n",
        "                                                            F.sum(F.col(\"amount\")).alias(\"total_spent\"),\n",
        "                                                            F.count(F.col(\"customer_id\")).alias(\"purchase_frequency\")\n",
        "                                                        )\n",
        "\n",
        "# apply segmentation\n",
        "purchases_aggregated.withColumn(\"segment\",\n",
        "                                F.when(F.col(\"purchase_frequency\")==1, \"one-time\")\\\n",
        "                                .when(F.col(\"purchase_frequency\")<=2, \"occasional_buyer\")\\\n",
        "                                .when((F.col(\"total_spent\")/F.col(\"purchase_frequency\"))>130, \"high-value\")\\\n",
        "                                .when(F.col(\"purchase_frequency\")>2, \"frequent_buyer\")\n",
        "                                ).orderBy(\"customer_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSBpswG3hfbF",
        "outputId": "ea236319-fe72-4a94-b1bd-b684555f6510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+----------------+\n",
            "|customer_id|total_spent|purchase_frequency|         segment|\n",
            "+-----------+-----------+------------------+----------------+\n",
            "|        101|      416.5|                 3|      high-value|\n",
            "|        102|      61.24|                 2|occasional_buyer|\n",
            "|        103|      451.5|                 4|  frequent_buyer|\n",
            "|        104|       35.5|                 1|        one-time|\n",
            "|        105|      771.0|                 2|occasional_buyer|\n",
            "+-----------+-----------+------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Product Affinity Analysis\n",
        "Analyze which products are frequently purchased together. For each product, find its top 2 most commonly co-purchased products and their co-purchase frequency.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, collect_set, explode, size, array, lit, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample purchase data with order_id to group items purchased together\n",
        "orders = spark.createDataFrame([\n",
        "    (1001, \"P1\", \"2024-01-05\"),\n",
        "    (1001, \"P2\", \"2024-01-05\"),\n",
        "    (1001, \"P3\", \"2024-01-05\"),\n",
        "    (1002, \"P2\", \"2024-01-06\"),\n",
        "    (1002, \"P4\", \"2024-01-06\"),\n",
        "    (1003, \"P1\", \"2024-01-07\"),\n",
        "    (1003, \"P3\", \"2024-01-07\"),\n",
        "    (1004, \"P1\", \"2024-01-08\"),\n",
        "    (1004, \"P2\", \"2024-01-08\"),\n",
        "    (1004, \"P5\", \"2024-01-08\"),\n",
        "    (1005, \"P2\", \"2024-01-09\"),\n",
        "    (1005, \"P3\", \"2024-01-09\"),\n",
        "    (1005, \"P4\", \"2024-01-09\"),\n",
        "    (1006, \"P1\", \"2024-01-10\"),\n",
        "    (1006, \"P4\", \"2024-01-10\")\n",
        "], [\"order_id\", \"product_id\", \"order_date\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-----------+------------------+---------------+\n",
        "|product_id |co_purchased_with |co_purchase_count|\n",
        "+-----------+------------------+---------------+\n",
        "|P1         |P3                |2              |\n",
        "|P1         |P2                |2              |\n",
        "|P2         |P3                |2              |\n",
        "|P2         |P1                |2              |\n",
        "|P3         |P1                |2              |\n",
        "|P3         |P2                |2              |\n",
        "|P4         |P2                |2              |\n",
        "|P4         |P3                |1              |\n",
        "|P5         |P1                |1              |\n",
        "|P5         |P2                |1              |\n",
        "+-----------+------------------+---------------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, collect_set, explode, lit, row_number, expr\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample purchase data\n",
        "orders = spark.createDataFrame([\n",
        "    (1001, \"P1\", \"2024-01-05\"),\n",
        "    (1001, \"P2\", \"2024-01-05\"),\n",
        "    (1001, \"P3\", \"2024-01-05\"),\n",
        "    (1002, \"P2\", \"2024-01-06\"),\n",
        "    (1002, \"P4\", \"2024-01-06\"),\n",
        "    (1003, \"P1\", \"2024-01-07\"),\n",
        "    (1003, \"P3\", \"2024-01-07\"),\n",
        "    (1004, \"P1\", \"2024-01-08\"),\n",
        "    (1004, \"P2\", \"2024-01-08\"),\n",
        "    (1004, \"P5\", \"2024-01-08\"),\n",
        "    (1005, \"P2\", \"2024-01-09\"),\n",
        "    (1005, \"P3\", \"2024-01-09\"),\n",
        "    (1005, \"P4\", \"2024-01-09\"),\n",
        "    (1006, \"P1\", \"2024-01-10\"),\n",
        "    (1006, \"P4\", \"2024-01-10\")\n",
        "], [\"order_id\", \"product_id\", \"order_date\"])\n",
        "\n",
        "# Step 1: Get all products per order\n",
        "order_products = orders.groupBy(\"order_id\") \\\n",
        "    .agg(collect_set(\"product_id\").alias(\"products\"))\n",
        "\n",
        "order_products.show()"
      ],
      "metadata": {
        "id": "GNhXJFlnOD2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143413eb-9957-4256-f970-d914c9c67337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+\n",
            "|order_id|    products|\n",
            "+--------+------------+\n",
            "|    1002|    [P2, P4]|\n",
            "|    1001|[P2, P1, P3]|\n",
            "|    1003|    [P1, P3]|\n",
            "|    1005|[P2, P4, P3]|\n",
            "|    1004|[P2, P1, P5]|\n",
            "|    1006|    [P1, P4]|\n",
            "+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Explode each product as a base and list all co-products\n",
        "exploded = order_products.select(\"order_id\", explode(\"products\").alias(\"product_id\"), \"products\")\n",
        "\n",
        "exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa9KnOOKypp1",
        "outputId": "c106b2fe-cb9c-471d-a3ed-d551be37f713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+\n",
            "|order_id|product_id|    products|\n",
            "+--------+----------+------------+\n",
            "|    1002|        P2|    [P2, P4]|\n",
            "|    1002|        P4|    [P2, P4]|\n",
            "|    1001|        P2|[P2, P1, P3]|\n",
            "|    1001|        P1|[P2, P1, P3]|\n",
            "|    1001|        P3|[P2, P1, P3]|\n",
            "|    1003|        P1|    [P1, P3]|\n",
            "|    1003|        P3|    [P1, P3]|\n",
            "|    1005|        P2|[P2, P4, P3]|\n",
            "|    1005|        P4|[P2, P4, P3]|\n",
            "|    1005|        P3|[P2, P4, P3]|\n",
            "|    1004|        P2|[P2, P1, P5]|\n",
            "|    1004|        P1|[P2, P1, P5]|\n",
            "|    1004|        P5|[P2, P1, P5]|\n",
            "|    1006|        P1|    [P1, P4]|\n",
            "|    1006|        P4|    [P1, P4]|\n",
            "+--------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the product_id from the list of products to get only co-products (self exclusion)\n",
        "exploded = exploded.withColumn(\"co_products\",\n",
        "                               expr(\"filter(products, x -> x != product_id)\")).drop(\"products\")\n",
        "\n",
        "exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKKnC4ijyw8Y",
        "outputId": "27f3d70d-1563-4082-b525-6bdd356a35e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-----------+\n",
            "|order_id|product_id|co_products|\n",
            "+--------+----------+-----------+\n",
            "|    1002|        P2|       [P4]|\n",
            "|    1002|        P4|       [P2]|\n",
            "|    1001|        P2|   [P1, P3]|\n",
            "|    1001|        P1|   [P2, P3]|\n",
            "|    1001|        P3|   [P2, P1]|\n",
            "|    1003|        P1|       [P3]|\n",
            "|    1003|        P3|       [P1]|\n",
            "|    1005|        P2|   [P4, P3]|\n",
            "|    1005|        P4|   [P2, P3]|\n",
            "|    1005|        P3|   [P2, P4]|\n",
            "|    1004|        P2|   [P1, P5]|\n",
            "|    1004|        P1|   [P2, P5]|\n",
            "|    1004|        P5|   [P2, P1]|\n",
            "|    1006|        P1|       [P4]|\n",
            "|    1006|        P4|       [P1]|\n",
            "+--------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Explode co-products to get pair rows\n",
        "pairs = exploded.select(\"product_id\", explode(\"co_products\").alias(\"co_purchased_with\"))\n",
        "\n",
        "pairs.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txzVeAREzDr5",
        "outputId": "36a9eaa7-025d-4c39-a733-6cc83809a56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|product_id|co_purchased_with|\n",
            "+----------+-----------------+\n",
            "|        P2|               P4|\n",
            "|        P4|               P2|\n",
            "|        P2|               P1|\n",
            "|        P2|               P3|\n",
            "|        P1|               P2|\n",
            "|        P1|               P3|\n",
            "|        P3|               P2|\n",
            "|        P3|               P1|\n",
            "|        P1|               P3|\n",
            "|        P3|               P1|\n",
            "|        P2|               P4|\n",
            "|        P2|               P3|\n",
            "|        P4|               P2|\n",
            "|        P4|               P3|\n",
            "|        P3|               P2|\n",
            "|        P3|               P4|\n",
            "|        P2|               P1|\n",
            "|        P2|               P5|\n",
            "|        P1|               P2|\n",
            "|        P1|               P5|\n",
            "+----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Count the frequency of each pair\n",
        "pair_counts = pairs.groupBy(\"product_id\", \"co_purchased_with\").count() \\\n",
        "    .withColumnRenamed(\"count\", \"co_purchase_count\")\n",
        "\n",
        "pair_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvUB5zoQzP7s",
        "outputId": "c77429f1-e721-43e1-ceee-1b387532eccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+-----------------+\n",
            "|product_id|co_purchased_with|co_purchase_count|\n",
            "+----------+-----------------+-----------------+\n",
            "|        P3|               P2|                2|\n",
            "|        P1|               P3|                2|\n",
            "|        P2|               P4|                2|\n",
            "|        P1|               P4|                1|\n",
            "|        P1|               P5|                1|\n",
            "|        P3|               P1|                2|\n",
            "|        P3|               P4|                1|\n",
            "|        P5|               P1|                1|\n",
            "|        P2|               P1|                2|\n",
            "|        P4|               P3|                1|\n",
            "|        P4|               P2|                2|\n",
            "|        P4|               P1|                1|\n",
            "|        P2|               P5|                1|\n",
            "|        P2|               P3|                2|\n",
            "|        P1|               P2|                2|\n",
            "|        P5|               P2|                1|\n",
            "+----------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Get top 2 co-purchased products per product\n",
        "window = Window.partitionBy(\"product_id\").orderBy(col(\"co_purchase_count\").desc(), col(\"co_purchased_with\"))\n",
        "\n",
        "top2 = pair_counts.withColumn(\"rank\", row_number().over(window)) \\\n",
        "    .filter(col(\"rank\") <= 2).drop(\"rank\")\n",
        "\n",
        "# Display final result\n",
        "top2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6obsIleKzTvh",
        "outputId": "e86ca46e-95f6-4b15-caea-c3796155caa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+-----------------+\n",
            "|product_id|co_purchased_with|co_purchase_count|\n",
            "+----------+-----------------+-----------------+\n",
            "|P1        |P2               |2                |\n",
            "|P1        |P3               |2                |\n",
            "|P2        |P1               |2                |\n",
            "|P2        |P3               |2                |\n",
            "|P3        |P1               |2                |\n",
            "|P3        |P2               |2                |\n",
            "|P4        |P2               |2                |\n",
            "|P4        |P1               |1                |\n",
            "|P5        |P1               |1                |\n",
            "|P5        |P2               |1                |\n",
            "+----------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Time-based Cohort Analysis\n",
        "Perform a cohort analysis to understand customer retention over time. Group customers by their first purchase month (cohort) and analyze how many return to make purchases in subsequent months.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, month, year, datediff, min, count, lit, when\n",
        "\n",
        "# Sample customer purchase data\n",
        "customer_purchases = spark.createDataFrame([\n",
        "    (101, \"2024-01-15\", 120.50),\n",
        "    (101, \"2024-02-05\", 85.75),\n",
        "    (101, \"2024-03-20\", 210.25),\n",
        "    (102, \"2024-01-20\", 45.99),\n",
        "    (102, \"2024-03-15\", 65.25),\n",
        "    (103, \"2024-02-10\", 85.75),\n",
        "    (103, \"2024-03-05\", 150.00),\n",
        "    (103, \"2024-04-12\", 95.50),\n",
        "    (104, \"2024-01-28\", 35.50),\n",
        "    (104, \"2024-04-02\", 120.75),\n",
        "    (105, \"2024-02-15\", 450.75),\n",
        "    (105, \"2024-02-22\", 320.25),\n",
        "    (106, \"2024-03-10\", 75.50),\n",
        "    (107, \"2024-03-18\", 95.25),\n",
        "    (107, \"2024-04-05\", 35.75)\n",
        "], [\"customer_id\", \"purchase_date\", \"amount\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+---------------+------------------+------------------+------------------+------------------+\n",
        "|cohort_month   |month_0_retention |month_1_retention |month_2_retention |month_3_retention |\n",
        "+---------------+------------------+------------------+------------------+------------------+\n",
        "|2024-01        |3                 |1                 |1                 |1                 |\n",
        "|2024-02        |2                 |1                 |1                 |0                 |\n",
        "|2024-03        |2                 |1                 |0                 |0                 |\n",
        "|2024-04        |0                 |0                 |0                 |0                 |\n",
        "+---------------+------------------+------------------+------------------+------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, month, year, datediff, min, count, lit, when\n",
        "\n",
        "# Sample customer purchase data\n",
        "customer_purchases = spark.createDataFrame([\n",
        "    (101, \"2024-01-15\", 120.50),\n",
        "    (101, \"2024-02-05\", 85.75),\n",
        "    (101, \"2024-03-20\", 210.25),\n",
        "    (102, \"2024-01-20\", 45.99),\n",
        "    (102, \"2024-03-15\", 65.25),\n",
        "    (103, \"2024-02-10\", 85.75),\n",
        "    (103, \"2024-03-05\", 150.00),\n",
        "    (103, \"2024-04-12\", 95.50),\n",
        "    (104, \"2024-01-28\", 35.50),\n",
        "    (104, \"2024-04-02\", 120.75),\n",
        "    (105, \"2024-02-15\", 450.75),\n",
        "    (105, \"2024-02-22\", 320.25),\n",
        "    (106, \"2024-03-10\", 75.50),\n",
        "    (107, \"2024-03-18\", 95.25),\n",
        "    (107, \"2024-04-05\", 35.75)\n",
        "], [\"customer_id\", \"purchase_date\", \"amount\"])\n",
        "\n",
        "# convert to date type and extract purchase month\n",
        "customer_purchases = customer_purchases.withColumn(\"purchase_date\", F.to_date(\"purchase_date\"))\\\n",
        "                                        .withColumn(\"purchase_month\", F.date_format(\"purchase_date\", \"yyyy-MM\"))\n",
        "\n",
        "# get each customer's first purchase month which will be their cohort\n",
        "customer_cohorts = customer_purchases.groupBy(\"customer_id\").agg(F.min(\"purchase_date\").alias(\"first_purchase_date\"))\\\n",
        "                  .withColumn(\"cohort\", F.date_format(\"first_purchase_date\", \"yyyy-MM\"))\n",
        "\n",
        "# 1. join with purchases table in order to groupby cohort\n",
        "# 2. calc months between current purchase month and cohort that will be the cohort_index\n",
        "# 3. group by cohort and cohort_index\n",
        "customer_purchases.join(customer_cohorts, on=[\"customer_id\"], how=\"inner\")\\\n",
        "                  .withColumn(\"cohort_index\", F.floor(F.months_between(F.col(\"purchase_date\"), F.col(\"first_purchase_date\"))))\\\n",
        "                  .groupBy([\"cohort\", \"cohort_index\"]).agg(F.count_distinct(\"customer_id\").alias(\"customer_count\"))\\\n",
        "                  .groupBy(\"cohort\").pivot(\"cohort_index\").agg(F.first(\"customer_count\"))\\\n",
        "                  .show()"
      ],
      "metadata": {
        "id": "-lQRgsLU04FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085f5190-accd-4f7e-be5f-814b36b5bb5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----+----+\n",
            "| cohort|  0|   1|   2|\n",
            "+-------+---+----+----+\n",
            "|2024-02|  2|NULL|   1|\n",
            "|2024-03|  2|NULL|NULL|\n",
            "|2024-01|  3|   1|   2|\n",
            "+-------+---+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We use `F.first()` which basically picks the number as is since each cohort-cohort_index pair has a single customer count*"
      ],
      "metadata": {
        "id": "l5IVbIc-J9F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Advanced Funnel Analysis with Attribution\n",
        "Analyze a customer journey funnel and attribute conversions to different marketing channels based on customer touchpoints and their timing. Implement both first-touch and last-touch attribution models.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number, first, last, when, count, sum, lag, lead\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Customer touchpoint data\n",
        "touchpoints = spark.createDataFrame([\n",
        "    (1001, \"view\", \"organic_search\", \"2024-01-05 09:12:33\"),\n",
        "    (1001, \"click\", \"email\", \"2024-01-05 12:32:14\"),\n",
        "    (1001, \"cart_add\", \"direct\", \"2024-01-06 15:45:22\"),\n",
        "    (1001, \"purchase\", \"direct\", \"2024-01-06 16:03:45\"),\n",
        "    (1002, \"view\", \"social_media\", \"2024-01-07 10:17:52\"),\n",
        "    (1002, \"click\", \"social_media\", \"2024-01-07 10:19:04\"),\n",
        "    (1002, \"cart_add\", \"retargeting\", \"2024-01-08 14:22:16\"),\n",
        "    (1002, \"view\", \"email\", \"2024-01-10 08:47:31\"),\n",
        "    (1002, \"cart_add\", \"email\", \"2024-01-10 08:52:47\"),\n",
        "    (1002, \"purchase\", \"email\", \"2024-01-10 09:01:12\"),\n",
        "    (1003, \"view\", \"organic_search\", \"2024-01-06 11:23:45\"),\n",
        "    (1003, \"click\", \"organic_search\", \"2024-01-06 11:25:12\"),\n",
        "    (1003, \"cart_add\", \"retargeting\", \"2024-01-07 16:42:38\"),\n",
        "    (1003, \"view\", \"direct\", \"2024-01-09 20:14:22\"),\n",
        "    (1004, \"view\", \"paid_search\", \"2024-01-08 13:12:44\"),\n",
        "    (1004, \"click\", \"paid_search\", \"2024-01-08 13:14:25\"),\n",
        "    (1004, \"cart_add\", \"paid_search\", \"2024-01-08 13:17:50\"),\n",
        "    (1004, \"purchase\", \"paid_search\", \"2024-01-08 13:22:11\"),\n",
        "    (1005, \"view\", \"social_media\", \"2024-01-05 14:32:19\"),\n",
        "    (1005, \"click\", \"retargeting\", \"2024-01-06 09:41:53\"),\n",
        "    (1005, \"cart_add\", \"retargeting\", \"2024-01-06 09:44:27\"),\n",
        "    (1005, \"view\", \"email\", \"2024-01-07 12:12:36\")\n",
        "], [\"user_id\", \"event\", \"channel\", \"timestamp\"])\n",
        "\n",
        "# Purchase value data for attribution\n",
        "purchases = spark.createDataFrame([\n",
        "    (1001, \"2024-01-06 16:03:45\", 125.50),\n",
        "    (1002, \"2024-01-10 09:01:12\", 84.75),\n",
        "    (1004, \"2024-01-08 13:22:11\", 210.25)\n",
        "], [\"user_id\", \"purchase_time\", \"purchase_value\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+----------------+----------------+--------------------+---------------------+\n",
        "|channel         |funnel_views    |first_touch_value   |last_touch_value     |\n",
        "+----------------+----------------+--------------------+---------------------+\n",
        "|organic_search  |2               |125.50              |0.00                 |\n",
        "|social_media    |2               |84.75               |0.00                 |\n",
        "|paid_search     |1               |210.25              |210.25               |\n",
        "|email           |2               |0.00                |84.75                |\n",
        "|direct          |1               |0.00                |125.50               |\n",
        "|retargeting     |1               |0.00                |0.00                 |\n",
        "+----------------+----------------+--------------------+---------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "kcW-rIfoigy9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7exKSZCyihP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6: Slowly Changing Dimension Processing\n",
        "Implement a Type 2 slowly changing dimension (SCD) process to track historical changes in product data. For each product, maintain historical records with effective date ranges when attributes change.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, current_date, lag, lead, when, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Current product dimension table\n",
        "current_dim = spark.createDataFrame([\n",
        "    (101, \"Tablet Pro\", \"Electronics\", 499.99, \"Active\", \"2023-12-01\", \"9999-12-31\"),\n",
        "    (102, \"Coffee Maker\", \"Kitchen\", 89.99, \"Active\", \"2023-12-01\", \"9999-12-31\"),\n",
        "    (103, \"Wireless Headphones\", \"Electronics\", 129.99, \"Active\", \"2023-12-15\", \"9999-12-31\"),\n",
        "    (104, \"Fitness Watch\", \"Wearables\", 159.99, \"Active\", \"2024-01-10\", \"9999-12-31\"),\n",
        "    (105, \"Blender\", \"Kitchen\", 79.99, \"Active\", \"2024-01-15\", \"9999-12-31\")\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\", \"status\", \"effective_start\", \"effective_end\"])\n",
        "\n",
        "# New product data with some changes\n",
        "new_data = spark.createDataFrame([\n",
        "    (101, \"Tablet Pro\", \"Electronics\", 479.99, \"Active\"),    # Price changed\n",
        "    (102, \"Coffee Maker\", \"Kitchen\", 89.99, \"Discontinued\"), # Status changed\n",
        "    (103, \"Wireless Headphones\", \"Electronics\", 129.99, \"Active\"), # No change\n",
        "    (104, \"Fitness Watch\", \"Electronics\", 159.99, \"Active\"), # Category changed\n",
        "    (105, \"Blender\", \"Kitchen\", 79.99, \"Active\"),            # No change\n",
        "    (106, \"Smart Speaker\", \"Electronics\", 89.99, \"Active\")   # New product\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\", \"status\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+----------+------------------+------------+-------+------------+----------------+-------------+----------+\n",
        "|product_id|product_name      |category    |price  |status      |effective_start |effective_end|is_current|\n",
        "+----------+------------------+------------+-------+------------+----------------+-------------+----------+\n",
        "|101       |Tablet Pro        |Electronics |499.99 |Active      |2023-12-01      |2024-04-30   |false     |\n",
        "|101       |Tablet Pro        |Electronics |479.99 |Active      |2024-04-30      |9999-12-31   |true      |\n",
        "|102       |Coffee Maker      |Kitchen     |89.99  |Active      |2023-12-01      |2024-04-30   |false     |\n",
        "|102       |Coffee Maker      |Kitchen     |89.99  |Discontinued|2024-04-30      |9999-12-31   |true      |\n",
        "|103       |Wireless Headphones|Electronics|129.99 |Active      |2023-12-15      |9999-12-31   |true      |\n",
        "|104       |Fitness Watch     |Wearables   |159.99 |Active      |2024-01-10      |2024-04-30   |false     |\n",
        "|104       |Fitness Watch     |Electronics |159.99 |Active      |2024-04-30      |9999-12-31   |true      |\n",
        "|105       |Blender           |Kitchen     |79.99  |Active      |2024-01-15      |9999-12-31   |true      |\n",
        "|106       |Smart Speaker     |Electronics |89.99  |Active      |2024-04-30      |9999-12-31   |true      |\n",
        "+----------+------------------+------------+-------+------------+----------------+-------------+----------+\n",
        "```"
      ],
      "metadata": {
        "id": "ii0jDLGwilja"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DhoRto5imHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}