{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ5aSX2pvVv9QpB4rpJQ+i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "d00d39b1-a9d9-4069-b2f1-f07e4ace6104"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Fetched 22.2 MB in 8s (2,622 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7efe8c1d6210>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://23b27d6c9ab7:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>daily_practice</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "6cec792b-d642-427e-c90c-8c906dc3dad8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 5 - 2025/04/21**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Missing Record Detection\n",
        "Find records that are expected to exist but are missing from a dataset. You have sales data for multiple stores over several months, but some stores didn't report data for certain months. Identify all store/month combinations that are missing.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, lit, sequence, to_date\n",
        "\n",
        "# Sample data\n",
        "sales = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\", 1200),\n",
        "    (1, \"2024-02-15\", 1450),\n",
        "    (1, \"2024-04-15\", 1300),\n",
        "    (2, \"2024-01-15\", 950),\n",
        "    (2, \"2024-03-15\", 1100),\n",
        "    (3, \"2024-01-15\", 800),\n",
        "    (3, \"2024-02-15\", 850),\n",
        "    (3, \"2024-03-15\", 900),\n",
        "    (3, \"2024-04-15\", 950)\n",
        "], [\"store_id\", \"report_date\", \"sales_amount\"])\n",
        "```\n",
        "\n",
        "Expected output (all store/month combinations that are missing):\n",
        "```\n",
        "+--------+----------------+\n",
        "|store_id|missing_month   |\n",
        "+--------+----------------+\n",
        "|1       |2024-03-15      |\n",
        "|2       |2024-02-15      |\n",
        "|2       |2024-04-15      |\n",
        "+--------+----------------+\n",
        "```"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNiQEYIPk88-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Time Series Forecasting Preparation\n",
        "Prepare data for a time series forecasting model by creating a complete series with moving averages and lagged values for multiple time windows. Convert daily temperature readings into features suitable for predicting future temperatures.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, avg, date_format, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data - daily temperature readings\n",
        "temp_readings = spark.createDataFrame([\n",
        "    (\"2024-01-01\", 32.5),\n",
        "    (\"2024-01-02\", 31.8),\n",
        "    (\"2024-01-03\", 33.2),\n",
        "    (\"2024-01-04\", 34.0),\n",
        "    (\"2024-01-05\", 32.1),\n",
        "    (\"2024-01-06\", 30.9),\n",
        "    (\"2024-01-07\", 31.5),\n",
        "    (\"2024-01-08\", 32.7),\n",
        "    (\"2024-01-09\", 33.8),\n",
        "    (\"2024-01-10\", 34.5)\n",
        "], [\"date\", \"temperature\"])\n",
        "```\n",
        "\n",
        "Expected output (should include 3-day moving average, 7-day moving average, 1-day lag, 3-day lag, and 7-day lag):\n",
        "```\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|date      |temperature |avg_temp_3day     |avg_temp_7day     |lag_1day     |lag_3day     |lag_7day     |\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|2024-01-10|34.5        |34.3              |32.9              |33.8         |32.1         |32.5         |\n",
        "|2024-01-09|33.8        |33.7              |32.7              |32.7         |30.9         |31.8         |\n",
        "|2024-01-08|32.7        |32.7              |32.1              |31.5         |34.0         |null         |\n",
        "...\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "pi9E7iYAlhrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Incremental ETL with Change Data Capture\n",
        "Implement a change data capture (CDC) process that accurately merges a source table into a target table, accounting for inserts, updates, and deletes based on operation flags.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Target table - current state of data\n",
        "target = spark.createDataFrame([\n",
        "    (1, \"Widget A\", 19.99, \"2023-12-01\"),\n",
        "    (2, \"Widget B\", 24.99, \"2023-12-05\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\"),\n",
        "    (4, \"Widget D\", 29.99, \"2023-12-15\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\"])\n",
        "\n",
        "# Source CDC table with change operations (I=Insert, U=Update, D=Delete)\n",
        "source_cdc = spark.createDataFrame([\n",
        "    (2, \"Widget B Premium\", 27.99, \"2024-01-05\", \"U\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\", \"D\"),\n",
        "    (5, \"Widget E\", 22.99, \"2024-01-07\", \"I\"),\n",
        "    (6, \"Widget F\", 17.99, \"2024-01-10\", \"I\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\", \"operation\"])\n",
        "```\n",
        "\n",
        "Expected output (after merging changes):\n",
        "```\n",
        "+---+----------------+-----+------------+\n",
        "|id |product_name    |price|last_updated|\n",
        "+---+----------------+-----+------------+\n",
        "|1  |Widget A        |19.99|2023-12-01  |\n",
        "|2  |Widget B Premium|27.99|2024-01-05  |\n",
        "|4  |Widget D        |29.99|2023-12-15  |\n",
        "|5  |Widget E        |22.99|2024-01-07  |\n",
        "|6  |Widget F        |17.99|2024-01-10  |\n",
        "+---+----------------+-----+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Complex Sessionization Algorithm\n",
        "Implement a custom sessionization algorithm for web clickstream data that groups user activities into sessions based on both time gaps between consecutive events and page type transitions. A new session starts when either the time gap exceeds 30 minutes OR when a user transitions from a product page to the homepage.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, when, sum, lit, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample clickstream data\n",
        "clicks = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-01-01 10:00:00\", \"homepage\"),\n",
        "    (\"user1\", \"2024-01-01 10:05:30\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:08:45\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:30:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user1\", \"2024-01-01 10:32:15\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 11:15:00\", \"product_page\"),  # New session (time gap > 30 min)\n",
        "    (\"user2\", \"2024-01-01 14:20:10\", \"homepage\"),\n",
        "    (\"user2\", \"2024-01-01 14:22:30\", \"product_page\"),\n",
        "    (\"user2\", \"2024-01-01 14:55:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user2\", \"2024-01-01 14:57:20\", \"product_page\")\n",
        "], [\"user_id\", \"timestamp\", \"page_type\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user_id|timestamp          |page_type   |session_id|session_duration  |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user1  |2024-01-01 10:00:00|homepage    |1         |8.75              |\n",
        "|user1  |2024-01-01 10:05:30|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:08:45|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:30:00|homepage    |2         |2.25              |\n",
        "|user1  |2024-01-01 10:32:15|product_page|2         |2.25              |\n",
        "|user1  |2024-01-01 11:15:00|product_page|3         |0.0               |\n",
        "|user2  |2024-01-01 14:20:10|homepage    |1         |2.33              |\n",
        "|user2  |2024-01-01 14:22:30|product_page|1         |2.33              |\n",
        "|user2  |2024-01-01 14:55:00|homepage    |2         |2.33              |\n",
        "|user2  |2024-01-01 14:57:20|product_page|2         |2.33              |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Nested ETL and Schema Evolution Detection\n",
        "You're building a data pipeline that must detect schema changes in nested JSON data structures and report differences between the current data schema and the target table schema. Design a solution that identifies added, modified, or removed fields (including nested fields) and suggests ALTER TABLE statements to evolve the schema.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, MapType\n",
        "import json\n",
        "\n",
        "# Target table schema (current)\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", StringType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"category\", StringType(), True),\n",
        "        StructField(\"price\", FloatType(), True),\n",
        "        StructField(\"tags\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"inventory\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "# New data with schema changes\n",
        "new_data_json = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"item1\",\n",
        "    \"name\": \"Product X\",\n",
        "    \"details\": {\n",
        "      \"category\": \"Electronics\",\n",
        "      \"price\": 299.99,\n",
        "      \"tags\": [\"new\", \"featured\"],\n",
        "      \"weight\": 1.5,\n",
        "      \"dimensions\": {\"length\": 10, \"width\": 5, \"height\": 2}\n",
        "    },\n",
        "    \"inventory\": {\"store1\": 20, \"store2\": 15},\n",
        "    \"reviews\": [{\"user\": \"user1\", \"rating\": 4.5, \"comment\": \"Great product\"}]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "new_data = spark.read.json(sc.parallelize([new_data_json]))\n",
        "```\n",
        "\n",
        "Hint: You'll need to write a recursive function to compare schema objects and identify differences at all levels of nesting. Your output should include a DataFrame showing all schema differences and the SQL ALTER TABLE statements needed to evolve the schema."
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07Kn_V9vliQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}