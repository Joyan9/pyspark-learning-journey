{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjE1Cgsu7z0PRd+0MSRctJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "caced3ea-74ee-454b-e8c9-869b893010b3"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,864 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.3 kB]\n",
            "Fetched 20.8 MB in 8s (2,603 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "617d9fed-74a0-4c9d-c44c-ced0ffc53cbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 5 - 2025/04/21**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Missing Record Detection\n",
        "Find records that are expected to exist but are missing from a dataset. You have sales data for multiple stores over several months, but some stores didn't report data for certain months. Identify all store/month combinations that are missing.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, lit, sequence, to_date\n",
        "\n",
        "# Sample data\n",
        "sales = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\", 1200),\n",
        "    (1, \"2024-02-15\", 1450),\n",
        "    (1, \"2024-04-15\", 1300),\n",
        "    (2, \"2024-01-15\", 950),\n",
        "    (2, \"2024-03-15\", 1100),\n",
        "    (3, \"2024-01-15\", 800),\n",
        "    (3, \"2024-02-15\", 850),\n",
        "    (3, \"2024-03-15\", 900),\n",
        "    (3, \"2024-04-15\", 950)\n",
        "], [\"store_id\", \"report_date\", \"sales_amount\"])\n",
        "```\n",
        "\n",
        "Expected output (all store/month combinations that are missing):\n",
        "```\n",
        "+--------+----------------+\n",
        "|store_id|missing_month   |\n",
        "+--------+----------------+\n",
        "|1       |2024-03-15      |\n",
        "|2       |2024-02-15      |\n",
        "|2       |2024-04-15      |\n",
        "+--------+----------------+\n",
        "```"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "sales = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\", 1200),\n",
        "    (1, \"2024-02-15\", 1450),\n",
        "    (1, \"2024-04-15\", 1300),\n",
        "    (2, \"2024-01-15\", 950),\n",
        "    (2, \"2024-03-15\", 1100),\n",
        "    (3, \"2024-01-15\", 800),\n",
        "    (3, \"2024-02-15\", 850),\n",
        "    (3, \"2024-03-15\", 900),\n",
        "    (3, \"2024-04-15\", 950)\n",
        "], [\"store_id\", \"report_date\", \"sales_amount\"])"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgZBcaWZzvTW",
        "outputId": "a253b3fb-5738-4d16-d58e-2f228f1ae578"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- store_id: long (nullable = true)\n",
            " |-- report_date: string (nullable = true)\n",
            " |-- sales_amount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, lit, sequence, to_date, expr\n",
        "\n",
        "# Get all distinct store IDs\n",
        "store_ids = sales.select(\"store_id\").distinct()\n",
        "\n",
        "# Create a single row DataFrame with date range\n",
        "date_range = spark.createDataFrame([(\"2024-01-15\", \"2024-04-15\")], [\"start_date\", \"end_date\"])\n",
        "date_range = date_range.withColumn(\"start_date\", to_date(\"start_date\"))\n",
        "date_range = date_range.withColumn(\"end_date\", to_date(\"end_date\"))\n",
        "\n",
        "# Cross join to get all stores with the date range\n",
        "expected_dates_base = store_ids.crossJoin(date_range)\n",
        "\n",
        "# Generate sequence of monthly dates for each store\n",
        "expected_report_dates = expected_dates_base.withColumn(\n",
        "    \"date_sequence\",\n",
        "    sequence(\"start_date\", \"end_date\", expr(\"interval 1 month\"))\n",
        ").withColumn(\n",
        "    \"expected_date\",\n",
        "    explode(\"date_sequence\")\n",
        ").select(\"store_id\", \"expected_date\")\n",
        "\n",
        "# Anti-join to find missing dates\n",
        "missing_reports = expected_report_dates.join(\n",
        "    sales.withColumn(\"date\", to_date(\"report_date\")),\n",
        "    (expected_report_dates.store_id == sales.store_id) &\n",
        "    (expected_report_dates.expected_date == to_date(sales.report_date)),\n",
        "    \"leftanti\"\n",
        ").select(\n",
        "    expected_report_dates.store_id,\n",
        "    expected_report_dates.expected_date.alias(\"missing_month\")\n",
        ")\n",
        "\n",
        "missing_reports.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThjqMyzPzvLo",
        "outputId": "9a458ab3-2453-418b-b30d-84d6c6c19189"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+\n",
            "|store_id|missing_month|\n",
            "+--------+-------------+\n",
            "|       1|   2024-03-15|\n",
            "|       2|   2024-02-15|\n",
            "|       2|   2024-04-15|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Time Series Forecasting Preparation\n",
        "Prepare data for a time series forecasting model by creating a complete series with moving averages and lagged values for multiple time windows. Convert daily temperature readings into features suitable for predicting future temperatures.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, avg, date_format, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data - daily temperature readings\n",
        "temp_readings = spark.createDataFrame([\n",
        "    (\"2024-01-01\", 32.5),\n",
        "    (\"2024-01-02\", 31.8),\n",
        "    (\"2024-01-03\", 33.2),\n",
        "    (\"2024-01-04\", 34.0),\n",
        "    (\"2024-01-05\", 32.1),\n",
        "    (\"2024-01-06\", 30.9),\n",
        "    (\"2024-01-07\", 31.5),\n",
        "    (\"2024-01-08\", 32.7),\n",
        "    (\"2024-01-09\", 33.8),\n",
        "    (\"2024-01-10\", 34.5)\n",
        "], [\"date\", \"temperature\"])\n",
        "```\n",
        "\n",
        "Expected output (should include 3-day moving average, 7-day moving average, 1-day lag, 3-day lag, and 7-day lag):\n",
        "```\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|date      |temperature |avg_temp_3day     |avg_temp_7day     |lag_1day     |lag_3day     |lag_7day     |\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|2024-01-10|34.5        |34.3              |32.9              |33.8         |32.1         |32.5         |\n",
        "|2024-01-09|33.8        |33.7              |32.7              |32.7         |30.9         |31.8         |\n",
        "|2024-01-08|32.7        |32.7              |32.1              |31.5         |34.0         |null         |\n",
        "...\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "pi9E7iYAlhrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, avg, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data - daily temperature readings\n",
        "temp_readings = spark.createDataFrame([\n",
        "    (\"2024-01-01\", 32.5),\n",
        "    (\"2024-01-02\", 31.8),\n",
        "    (\"2024-01-03\", 33.2),\n",
        "    (\"2024-01-04\", 34.0),\n",
        "    (\"2024-01-05\", 32.1),\n",
        "    (\"2024-01-06\", 30.9),\n",
        "    (\"2024-01-07\", 31.5),\n",
        "    (\"2024-01-08\", 32.7),\n",
        "    (\"2024-01-09\", 33.8),\n",
        "    (\"2024-01-10\", 34.5)\n",
        "], [\"date\", \"temperature\"])\n",
        "\n",
        "# Convert date to date type\n",
        "temp_readings = temp_readings.withColumn(\"date\", to_date(\"date\"))\n",
        "\n",
        "# Define windows\n",
        "window_3day = Window.orderBy(\"date\").rowsBetween(-2, 0)  # Current + 2 previous days\n",
        "window_7day = Window.orderBy(\"date\").rowsBetween(-6, 0)  # Current + 6 previous days\n",
        "window_lag = Window.orderBy(\"date\")  # For lag calculations\n",
        "\n",
        "# Apply transformations\n",
        "result = temp_readings \\\n",
        "    .withColumn(\"avg_temp_3day\", avg(\"temperature\").over(window_3day)) \\\n",
        "    .withColumn(\"avg_temp_7day\", avg(\"temperature\").over(window_7day)) \\\n",
        "    .withColumn(\"lag_1day\", lag(\"temperature\", 1).over(window_lag)) \\\n",
        "    .withColumn(\"lag_3day\", lag(\"temperature\", 3).over(window_lag)) \\\n",
        "    .withColumn(\"lag_7day\", lag(\"temperature\", 7).over(window_lag)) \\\n",
        "    .orderBy(col(\"date\").desc())  # Optional: show most recent first\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_3Dd3fC6XAK",
        "outputId": "37d92287-1d1a-41e0-e7a8-290241e9d418"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "|      date|temperature|     avg_temp_3day|     avg_temp_7day|lag_1day|lag_3day|lag_7day|\n",
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "|2024-01-10|       34.5|33.666666666666664|32.785714285714285|    33.8|    31.5|    33.2|\n",
            "|2024-01-09|       33.8|32.666666666666664| 32.60000000000001|    32.7|    30.9|    31.8|\n",
            "|2024-01-08|       32.7|              31.7| 32.31428571428571|    31.5|    32.1|    32.5|\n",
            "|2024-01-07|       31.5|              31.5|32.285714285714285|    30.9|    34.0|    NULL|\n",
            "|2024-01-06|       30.9|32.333333333333336|32.416666666666664|    32.1|    33.2|    NULL|\n",
            "|2024-01-05|       32.1|              33.1|             32.72|    34.0|    31.8|    NULL|\n",
            "|2024-01-04|       34.0|              33.0|            32.875|    33.2|    32.5|    NULL|\n",
            "|2024-01-03|       33.2|              32.5|              32.5|    31.8|    NULL|    NULL|\n",
            "|2024-01-02|       31.8|             32.15|             32.15|    32.5|    NULL|    NULL|\n",
            "|2024-01-01|       32.5|              32.5|              32.5|    NULL|    NULL|    NULL|\n",
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Incremental ETL with Change Data Capture\n",
        "Implement a change data capture (CDC) process that accurately merges a source table into a target table, accounting for inserts, updates, and deletes based on operation flags.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Target table - current state of data\n",
        "target = spark.createDataFrame([\n",
        "    (1, \"Widget A\", 19.99, \"2023-12-01\"),\n",
        "    (2, \"Widget B\", 24.99, \"2023-12-05\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\"),\n",
        "    (4, \"Widget D\", 29.99, \"2023-12-15\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\"])\n",
        "\n",
        "# Source CDC table with change operations (I=Insert, U=Update, D=Delete)\n",
        "source_cdc = spark.createDataFrame([\n",
        "    (2, \"Widget B Premium\", 27.99, \"2024-01-05\", \"U\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\", \"D\"),\n",
        "    (5, \"Widget E\", 22.99, \"2024-01-07\", \"I\"),\n",
        "    (6, \"Widget F\", 17.99, \"2024-01-10\", \"I\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\", \"operation\"])\n",
        "```\n",
        "\n",
        "Expected output (after merging changes):\n",
        "```\n",
        "+---+----------------+-----+------------+\n",
        "|id |product_name    |price|last_updated|\n",
        "+---+----------------+-----+------------+\n",
        "|1  |Widget A        |19.99|2023-12-01  |\n",
        "|2  |Widget B Premium|27.99|2024-01-05  |\n",
        "|4  |Widget D        |29.99|2023-12-15  |\n",
        "|5  |Widget E        |22.99|2024-01-07  |\n",
        "|6  |Widget F        |17.99|2024-01-10  |\n",
        "+---+----------------+-----+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Target table - current state of data\n",
        "target = spark.createDataFrame([\n",
        "    (1, \"Widget A\", 19.99, \"2023-12-01\"),\n",
        "    (2, \"Widget B\", 24.99, \"2023-12-05\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\"),\n",
        "    (4, \"Widget D\", 29.99, \"2023-12-15\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\"])\n",
        "\n",
        "# Source CDC table with change operations (I=Insert, U=Update, D=Delete)\n",
        "source_cdc = spark.createDataFrame([\n",
        "    (2, \"Widget B Premium\", 27.99, \"2024-01-05\", \"U\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\", \"D\"),\n",
        "    (5, \"Widget E\", 22.99, \"2024-01-07\", \"I\"),\n",
        "    (6, \"Widget F\", 17.99, \"2024-01-10\", \"I\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\", \"operation\"])\n",
        "\n",
        "inserts = source_cdc.filter(F.col(\"operation\") == \"I\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "updates = source_cdc.filter(F.col(\"operation\") == \"U\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "deletes = source_cdc.filter(F.col(\"operation\") == \"D\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "# first select only those target records that are not being updated and then just add the updated records\n",
        "target_updated = target.join(updates, updates.id == target.id, \"leftanti\")\\\n",
        "                        .union(updates)\n",
        "\n",
        "# perform a left anti join to remove deleted records and then add the inserts\n",
        "target_final = target_updated.join(deletes, deletes.id == target_updated.id, \"leftanti\")\\\n",
        "                      .union(inserts)\n",
        "\n",
        "target_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTm-Krpq6X8-",
        "outputId": "7987d23c-4b27-4444-d349-5d2486a60962"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-----+------------+\n",
            "| id|    product_name|price|last_updated|\n",
            "+---+----------------+-----+------------+\n",
            "|  1|        Widget A|19.99|  2023-12-01|\n",
            "|  4|        Widget D|29.99|  2023-12-15|\n",
            "|  2|Widget B Premium|27.99|  2024-01-05|\n",
            "|  5|        Widget E|22.99|  2024-01-07|\n",
            "|  6|        Widget F|17.99|  2024-01-10|\n",
            "+---+----------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oihtZow6X2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Complex Sessionization Algorithm\n",
        "Implement a custom sessionization algorithm for web clickstream data that groups user activities into sessions based on both time gaps between consecutive events and page type transitions. A new session starts when either the time gap exceeds 30 minutes OR when a user transitions from a product page to the homepage.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, when, sum, lit, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample clickstream data\n",
        "clicks = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-01-01 10:00:00\", \"homepage\"),\n",
        "    (\"user1\", \"2024-01-01 10:05:30\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:08:45\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:30:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user1\", \"2024-01-01 10:32:15\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 11:15:00\", \"product_page\"),  # New session (time gap > 30 min)\n",
        "    (\"user2\", \"2024-01-01 14:20:10\", \"homepage\"),\n",
        "    (\"user2\", \"2024-01-01 14:22:30\", \"product_page\"),\n",
        "    (\"user2\", \"2024-01-01 14:55:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user2\", \"2024-01-01 14:57:20\", \"product_page\")\n",
        "], [\"user_id\", \"timestamp\", \"page_type\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user_id|timestamp          |page_type   |session_id|session_duration  |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user1  |2024-01-01 10:00:00|homepage    |1         |8.75              |\n",
        "|user1  |2024-01-01 10:05:30|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:08:45|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:30:00|homepage    |2         |2.25              |\n",
        "|user1  |2024-01-01 10:32:15|product_page|2         |2.25              |\n",
        "|user1  |2024-01-01 11:15:00|product_page|3         |0.0               |\n",
        "|user2  |2024-01-01 14:20:10|homepage    |1         |2.33              |\n",
        "|user2  |2024-01-01 14:22:30|product_page|1         |2.33              |\n",
        "|user2  |2024-01-01 14:55:00|homepage    |2         |2.33              |\n",
        "|user2  |2024-01-01 14:57:20|product_page|2         |2.33              |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mf20Dt0q6YwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KpEKulpe6Ypa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Nested ETL and Schema Evolution Detection\n",
        "You're building a data pipeline that must detect schema changes in nested JSON data structures and report differences between the current data schema and the target table schema. Design a solution that identifies added, modified, or removed fields (including nested fields) and suggests ALTER TABLE statements to evolve the schema.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, MapType\n",
        "import json\n",
        "\n",
        "# Target table schema (current)\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", StringType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"category\", StringType(), True),\n",
        "        StructField(\"price\", FloatType(), True),\n",
        "        StructField(\"tags\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"inventory\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "# New data with schema changes\n",
        "new_data_json = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"item1\",\n",
        "    \"name\": \"Product X\",\n",
        "    \"details\": {\n",
        "      \"category\": \"Electronics\",\n",
        "      \"price\": 299.99,\n",
        "      \"tags\": [\"new\", \"featured\"],\n",
        "      \"weight\": 1.5,\n",
        "      \"dimensions\": {\"length\": 10, \"width\": 5, \"height\": 2}\n",
        "    },\n",
        "    \"inventory\": {\"store1\": 20, \"store2\": 15},\n",
        "    \"reviews\": [{\"user\": \"user1\", \"rating\": 4.5, \"comment\": \"Great product\"}]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "new_data = spark.read.json(sc.parallelize([new_data_json]))\n",
        "```\n",
        "\n",
        "Hint: You'll need to write a recursive function to compare schema objects and identify differences at all levels of nesting. Your output should include a DataFrame showing all schema differences and the SQL ALTER TABLE statements needed to evolve the schema."
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07Kn_V9vliQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}