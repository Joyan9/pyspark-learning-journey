{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdFCg2JgvFdkPaK0DsW4sk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "e4844836-1e1a-4aad-ac21-be6f2e361f2e"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,864 kB]\n",
            "Fetched 20.8 MB in 9s (2,228 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "9c0d6a51-f116-4783-ad9a-429949f46178"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 5 - 2025/04/21**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Missing Record Detection\n",
        "Find records that are expected to exist but are missing from a dataset. You have sales data for multiple stores over several months, but some stores didn't report data for certain months. Identify all store/month combinations that are missing.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, lit, sequence, to_date\n",
        "\n",
        "# Sample data\n",
        "sales = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\", 1200),\n",
        "    (1, \"2024-02-15\", 1450),\n",
        "    (1, \"2024-04-15\", 1300),\n",
        "    (2, \"2024-01-15\", 950),\n",
        "    (2, \"2024-03-15\", 1100),\n",
        "    (3, \"2024-01-15\", 800),\n",
        "    (3, \"2024-02-15\", 850),\n",
        "    (3, \"2024-03-15\", 900),\n",
        "    (3, \"2024-04-15\", 950)\n",
        "], [\"store_id\", \"report_date\", \"sales_amount\"])\n",
        "```\n",
        "\n",
        "Expected output (all store/month combinations that are missing):\n",
        "```\n",
        "+--------+----------------+\n",
        "|store_id|missing_month   |\n",
        "+--------+----------------+\n",
        "|1       |2024-03-15      |\n",
        "|2       |2024-02-15      |\n",
        "|2       |2024-04-15      |\n",
        "+--------+----------------+\n",
        "```"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "sales = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\", 1200),\n",
        "    (1, \"2024-02-15\", 1450),\n",
        "    (1, \"2024-04-15\", 1300),\n",
        "    (2, \"2024-01-15\", 950),\n",
        "    (2, \"2024-03-15\", 1100),\n",
        "    (3, \"2024-01-15\", 800),\n",
        "    (3, \"2024-02-15\", 850),\n",
        "    (3, \"2024-03-15\", 900),\n",
        "    (3, \"2024-04-15\", 950)\n",
        "], [\"store_id\", \"report_date\", \"sales_amount\"])"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgZBcaWZzvTW",
        "outputId": "a253b3fb-5738-4d16-d58e-2f228f1ae578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- store_id: long (nullable = true)\n",
            " |-- report_date: string (nullable = true)\n",
            " |-- sales_amount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, lit, sequence, to_date, expr\n",
        "\n",
        "# Get all distinct store IDs\n",
        "store_ids = sales.select(\"store_id\").distinct()\n",
        "\n",
        "# Create a single row DataFrame with date range\n",
        "date_range = spark.createDataFrame([(\"2024-01-15\", \"2024-04-15\")], [\"start_date\", \"end_date\"])\n",
        "date_range = date_range.withColumn(\"start_date\", to_date(\"start_date\"))\n",
        "date_range = date_range.withColumn(\"end_date\", to_date(\"end_date\"))\n",
        "\n",
        "# Cross join to get all stores with the date range\n",
        "expected_dates_base = store_ids.crossJoin(date_range)\n",
        "\n",
        "# Generate sequence of monthly dates for each store\n",
        "expected_report_dates = expected_dates_base.withColumn(\n",
        "    \"date_sequence\",\n",
        "    sequence(\"start_date\", \"end_date\", expr(\"interval 1 month\"))\n",
        ").withColumn(\n",
        "    \"expected_date\",\n",
        "    explode(\"date_sequence\")\n",
        ").select(\"store_id\", \"expected_date\")\n",
        "\n",
        "# Anti-join to find missing dates\n",
        "missing_reports = expected_report_dates.join(\n",
        "    sales.withColumn(\"date\", to_date(\"report_date\")),\n",
        "    (expected_report_dates.store_id == sales.store_id) &\n",
        "    (expected_report_dates.expected_date == to_date(sales.report_date)),\n",
        "    \"leftanti\"\n",
        ").select(\n",
        "    expected_report_dates.store_id,\n",
        "    expected_report_dates.expected_date.alias(\"missing_month\")\n",
        ")\n",
        "\n",
        "missing_reports.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThjqMyzPzvLo",
        "outputId": "9a458ab3-2453-418b-b30d-84d6c6c19189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+\n",
            "|store_id|missing_month|\n",
            "+--------+-------------+\n",
            "|       1|   2024-03-15|\n",
            "|       2|   2024-02-15|\n",
            "|       2|   2024-04-15|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Time Series Forecasting Preparation\n",
        "Prepare data for a time series forecasting model by creating a complete series with moving averages and lagged values for multiple time windows. Convert daily temperature readings into features suitable for predicting future temperatures.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, avg, date_format, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data - daily temperature readings\n",
        "temp_readings = spark.createDataFrame([\n",
        "    (\"2024-01-01\", 32.5),\n",
        "    (\"2024-01-02\", 31.8),\n",
        "    (\"2024-01-03\", 33.2),\n",
        "    (\"2024-01-04\", 34.0),\n",
        "    (\"2024-01-05\", 32.1),\n",
        "    (\"2024-01-06\", 30.9),\n",
        "    (\"2024-01-07\", 31.5),\n",
        "    (\"2024-01-08\", 32.7),\n",
        "    (\"2024-01-09\", 33.8),\n",
        "    (\"2024-01-10\", 34.5)\n",
        "], [\"date\", \"temperature\"])\n",
        "```\n",
        "\n",
        "Expected output (should include 3-day moving average, 7-day moving average, 1-day lag, 3-day lag, and 7-day lag):\n",
        "```\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|date      |temperature |avg_temp_3day     |avg_temp_7day     |lag_1day     |lag_3day     |lag_7day     |\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "|2024-01-10|34.5        |34.3              |32.9              |33.8         |32.1         |32.5         |\n",
        "|2024-01-09|33.8        |33.7              |32.7              |32.7         |30.9         |31.8         |\n",
        "|2024-01-08|32.7        |32.7              |32.1              |31.5         |34.0         |null         |\n",
        "...\n",
        "+----------+------------+------------------+------------------+-------------+-------------+-------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "pi9E7iYAlhrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, avg, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data - daily temperature readings\n",
        "temp_readings = spark.createDataFrame([\n",
        "    (\"2024-01-01\", 32.5),\n",
        "    (\"2024-01-02\", 31.8),\n",
        "    (\"2024-01-03\", 33.2),\n",
        "    (\"2024-01-04\", 34.0),\n",
        "    (\"2024-01-05\", 32.1),\n",
        "    (\"2024-01-06\", 30.9),\n",
        "    (\"2024-01-07\", 31.5),\n",
        "    (\"2024-01-08\", 32.7),\n",
        "    (\"2024-01-09\", 33.8),\n",
        "    (\"2024-01-10\", 34.5)\n",
        "], [\"date\", \"temperature\"])\n",
        "\n",
        "# Convert date to date type\n",
        "temp_readings = temp_readings.withColumn(\"date\", to_date(\"date\"))\n",
        "\n",
        "# Define windows\n",
        "window_3day = Window.orderBy(\"date\").rowsBetween(-2, 0)  # Current + 2 previous days\n",
        "window_7day = Window.orderBy(\"date\").rowsBetween(-6, 0)  # Current + 6 previous days\n",
        "window_lag = Window.orderBy(\"date\")  # For lag calculations\n",
        "\n",
        "# Apply transformations\n",
        "result = temp_readings \\\n",
        "    .withColumn(\"avg_temp_3day\", avg(\"temperature\").over(window_3day)) \\\n",
        "    .withColumn(\"avg_temp_7day\", avg(\"temperature\").over(window_7day)) \\\n",
        "    .withColumn(\"lag_1day\", lag(\"temperature\", 1).over(window_lag)) \\\n",
        "    .withColumn(\"lag_3day\", lag(\"temperature\", 3).over(window_lag)) \\\n",
        "    .withColumn(\"lag_7day\", lag(\"temperature\", 7).over(window_lag)) \\\n",
        "    .orderBy(col(\"date\").desc())  # Optional: show most recent first\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_3Dd3fC6XAK",
        "outputId": "37d92287-1d1a-41e0-e7a8-290241e9d418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "|      date|temperature|     avg_temp_3day|     avg_temp_7day|lag_1day|lag_3day|lag_7day|\n",
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "|2024-01-10|       34.5|33.666666666666664|32.785714285714285|    33.8|    31.5|    33.2|\n",
            "|2024-01-09|       33.8|32.666666666666664| 32.60000000000001|    32.7|    30.9|    31.8|\n",
            "|2024-01-08|       32.7|              31.7| 32.31428571428571|    31.5|    32.1|    32.5|\n",
            "|2024-01-07|       31.5|              31.5|32.285714285714285|    30.9|    34.0|    NULL|\n",
            "|2024-01-06|       30.9|32.333333333333336|32.416666666666664|    32.1|    33.2|    NULL|\n",
            "|2024-01-05|       32.1|              33.1|             32.72|    34.0|    31.8|    NULL|\n",
            "|2024-01-04|       34.0|              33.0|            32.875|    33.2|    32.5|    NULL|\n",
            "|2024-01-03|       33.2|              32.5|              32.5|    31.8|    NULL|    NULL|\n",
            "|2024-01-02|       31.8|             32.15|             32.15|    32.5|    NULL|    NULL|\n",
            "|2024-01-01|       32.5|              32.5|              32.5|    NULL|    NULL|    NULL|\n",
            "+----------+-----------+------------------+------------------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Incremental ETL with Change Data Capture\n",
        "Implement a change data capture (CDC) process that accurately merges a source table into a target table, accounting for inserts, updates, and deletes based on operation flags.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Target table - current state of data\n",
        "target = spark.createDataFrame([\n",
        "    (1, \"Widget A\", 19.99, \"2023-12-01\"),\n",
        "    (2, \"Widget B\", 24.99, \"2023-12-05\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\"),\n",
        "    (4, \"Widget D\", 29.99, \"2023-12-15\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\"])\n",
        "\n",
        "# Source CDC table with change operations (I=Insert, U=Update, D=Delete)\n",
        "source_cdc = spark.createDataFrame([\n",
        "    (2, \"Widget B Premium\", 27.99, \"2024-01-05\", \"U\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\", \"D\"),\n",
        "    (5, \"Widget E\", 22.99, \"2024-01-07\", \"I\"),\n",
        "    (6, \"Widget F\", 17.99, \"2024-01-10\", \"I\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\", \"operation\"])\n",
        "```\n",
        "\n",
        "Expected output (after merging changes):\n",
        "```\n",
        "+---+----------------+-----+------------+\n",
        "|id |product_name    |price|last_updated|\n",
        "+---+----------------+-----+------------+\n",
        "|1  |Widget A        |19.99|2023-12-01  |\n",
        "|2  |Widget B Premium|27.99|2024-01-05  |\n",
        "|4  |Widget D        |29.99|2023-12-15  |\n",
        "|5  |Widget E        |22.99|2024-01-07  |\n",
        "|6  |Widget F        |17.99|2024-01-10  |\n",
        "+---+----------------+-----+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Target table - current state of data\n",
        "target = spark.createDataFrame([\n",
        "    (1, \"Widget A\", 19.99, \"2023-12-01\"),\n",
        "    (2, \"Widget B\", 24.99, \"2023-12-05\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\"),\n",
        "    (4, \"Widget D\", 29.99, \"2023-12-15\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\"])\n",
        "\n",
        "# Source CDC table with change operations (I=Insert, U=Update, D=Delete)\n",
        "source_cdc = spark.createDataFrame([\n",
        "    (2, \"Widget B Premium\", 27.99, \"2024-01-05\", \"U\"),\n",
        "    (3, \"Widget C\", 14.99, \"2023-12-10\", \"D\"),\n",
        "    (5, \"Widget E\", 22.99, \"2024-01-07\", \"I\"),\n",
        "    (6, \"Widget F\", 17.99, \"2024-01-10\", \"I\")\n",
        "], [\"id\", \"product_name\", \"price\", \"last_updated\", \"operation\"])\n",
        "\n",
        "inserts = source_cdc.filter(F.col(\"operation\") == \"I\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "updates = source_cdc.filter(F.col(\"operation\") == \"U\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "deletes = source_cdc.filter(F.col(\"operation\") == \"D\").select(\"id\", \"product_name\", \"price\", \"last_updated\")\n",
        "\n",
        "# first select only those target records that are not being updated and then just add the updated records\n",
        "target_updated = target.join(updates, updates.id == target.id, \"leftanti\")\\\n",
        "                        .union(updates)\n",
        "\n",
        "# perform a left anti join to remove deleted records and then add the inserts\n",
        "target_final = target_updated.join(deletes, deletes.id == target_updated.id, \"leftanti\")\\\n",
        "                      .union(inserts)\n",
        "\n",
        "target_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTm-Krpq6X8-",
        "outputId": "7987d23c-4b27-4444-d349-5d2486a60962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-----+------------+\n",
            "| id|    product_name|price|last_updated|\n",
            "+---+----------------+-----+------------+\n",
            "|  1|        Widget A|19.99|  2023-12-01|\n",
            "|  4|        Widget D|29.99|  2023-12-15|\n",
            "|  2|Widget B Premium|27.99|  2024-01-05|\n",
            "|  5|        Widget E|22.99|  2024-01-07|\n",
            "|  6|        Widget F|17.99|  2024-01-10|\n",
            "+---+----------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Complex Sessionization Algorithm\n",
        "Implement a custom sessionization algorithm for web clickstream data that groups user activities into sessions based on both time gaps between consecutive events and page type transitions. A new session starts when either the time gap exceeds 30 minutes OR when a user transitions from a product page to the homepage.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, when, sum, lit, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample clickstream data\n",
        "clicks = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-01-01 10:00:00\", \"homepage\"),\n",
        "    (\"user1\", \"2024-01-01 10:05:30\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:08:45\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:30:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user1\", \"2024-01-01 10:32:15\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 11:15:00\", \"product_page\"),  # New session (time gap > 30 min)\n",
        "    (\"user2\", \"2024-01-01 14:20:10\", \"homepage\"),\n",
        "    (\"user2\", \"2024-01-01 14:22:30\", \"product_page\"),\n",
        "    (\"user2\", \"2024-01-01 14:55:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user2\", \"2024-01-01 14:57:20\", \"product_page\")\n",
        "], [\"user_id\", \"timestamp\", \"page_type\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user_id|timestamp          |page_type   |session_id|session_duration  |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "|user1  |2024-01-01 10:00:00|homepage    |1         |8.75              |\n",
        "|user1  |2024-01-01 10:05:30|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:08:45|product_page|1         |8.75              |\n",
        "|user1  |2024-01-01 10:30:00|homepage    |2         |2.25              |\n",
        "|user1  |2024-01-01 10:32:15|product_page|2         |2.25              |\n",
        "|user1  |2024-01-01 11:15:00|product_page|3         |0.0               |\n",
        "|user2  |2024-01-01 14:20:10|homepage    |1         |2.33              |\n",
        "|user2  |2024-01-01 14:22:30|product_page|1         |2.33              |\n",
        "|user2  |2024-01-01 14:55:00|homepage    |2         |2.33              |\n",
        "|user2  |2024-01-01 14:57:20|product_page|2         |2.33              |\n",
        "+-------+-------------------+------------+----------+------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, when, sum, lit, row_number, expr\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample clickstream data\n",
        "clicks = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-01-01 10:00:00\", \"homepage\"),\n",
        "    (\"user1\", \"2024-01-01 10:05:30\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:08:45\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 10:30:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user1\", \"2024-01-01 10:32:15\", \"product_page\"),\n",
        "    (\"user1\", \"2024-01-01 11:15:00\", \"product_page\"),  # New session (time gap > 30 min)\n",
        "    (\"user2\", \"2024-01-01 14:20:10\", \"homepage\"),\n",
        "    (\"user2\", \"2024-01-01 14:22:30\", \"product_page\"),\n",
        "    (\"user2\", \"2024-01-01 14:55:00\", \"homepage\"),      # New session (page transition)\n",
        "    (\"user2\", \"2024-01-01 14:57:20\", \"product_page\")\n",
        "], [\"user_id\", \"timestamp\", \"page_type\"])\n",
        "\n",
        "clicks = clicks.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
        "clicks.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpEKulpe6Ypa",
        "outputId": "1f2990bb-978d-4d3d-8988-1baf5cebab59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- page_type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, expr, sum, first, last, round\n",
        "\n",
        "# Create window for session analysis\n",
        "window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
        "\n",
        "# Identify new sessions\n",
        "sessions_df = clicks \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
        "    .withColumn(\"prev_page_type\", lag(\"page_type\", 1).over(window)) \\\n",
        "    .withColumn(\"prev_timestamp\", lag(\"timestamp\", 1).over(window)) \\\n",
        "    .withColumn(\"time_gap\", expr(\"(unix_timestamp(timestamp) - unix_timestamp(prev_timestamp)) / 60\")) \\\n",
        "    .withColumn(\"is_new_session\",\n",
        "                when((col(\"time_gap\") > 30) |\n",
        "                     ((col(\"prev_page_type\") == \"product_page\") & (col(\"page_type\") == \"homepage\")),\n",
        "                     1).otherwise(0))\n",
        "\n",
        "# Create cumulative session ID\n",
        "sessions_df = sessions_df \\\n",
        "    .withColumn(\"session_id\",\n",
        "                sum(\"is_new_session\").over(Window.partitionBy(\"user_id\").orderBy(\"timestamp\")) + 1)\n",
        "\n",
        "# Calculate session duration\n",
        "window_session = Window.partitionBy(\"user_id\", \"session_id\")\n",
        "\n",
        "final_df = sessions_df \\\n",
        "    .withColumn(\"session_start\", first(\"timestamp\").over(window_session)) \\\n",
        "    .withColumn(\"session_end\", last(\"timestamp\").over(window_session)) \\\n",
        "    .withColumn(\"session_duration\",\n",
        "                round(expr(\"(unix_timestamp(session_end) - unix_timestamp(session_start)) / 60\"), 2)) \\\n",
        "    .select(\"user_id\", \"timestamp\", \"page_type\", \"session_id\", \"session_duration\") \\\n",
        "    .orderBy(\"user_id\", \"timestamp\")\n",
        "\n",
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNhXJFlnOD2n",
        "outputId": "4af94eb4-41f5-4987-e005-73b50f7239d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------------+----------+----------------+\n",
            "|user_id|          timestamp|   page_type|session_id|session_duration|\n",
            "+-------+-------------------+------------+----------+----------------+\n",
            "|  user1|2024-01-01 10:00:00|    homepage|         1|            8.75|\n",
            "|  user1|2024-01-01 10:05:30|product_page|         1|            8.75|\n",
            "|  user1|2024-01-01 10:08:45|product_page|         1|            8.75|\n",
            "|  user1|2024-01-01 10:30:00|    homepage|         2|            2.25|\n",
            "|  user1|2024-01-01 10:32:15|product_page|         2|            2.25|\n",
            "|  user1|2024-01-01 11:15:00|product_page|         3|             0.0|\n",
            "|  user2|2024-01-01 14:20:10|    homepage|         1|            2.33|\n",
            "|  user2|2024-01-01 14:22:30|product_page|         1|            2.33|\n",
            "|  user2|2024-01-01 14:55:00|    homepage|         2|            2.33|\n",
            "|  user2|2024-01-01 14:57:20|product_page|         2|            2.33|\n",
            "+-------+-------------------+------------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Nested ETL and Schema Evolution Detection\n",
        "You're building a data pipeline that must detect schema changes in nested JSON data structures and report differences between the current data schema and the target table schema. Design a solution that identifies added, modified, or removed fields (including nested fields) and suggests ALTER TABLE statements to evolve the schema.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, MapType\n",
        "import json\n",
        "\n",
        "# Target table schema (current)\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", StringType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"category\", StringType(), True),\n",
        "        StructField(\"price\", FloatType(), True),\n",
        "        StructField(\"tags\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"inventory\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "# New data with schema changes\n",
        "new_data_json = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"item1\",\n",
        "    \"name\": \"Product X\",\n",
        "    \"details\": {\n",
        "      \"category\": \"Electronics\",\n",
        "      \"price\": 299.99,\n",
        "      \"tags\": [\"new\", \"featured\"],\n",
        "      \"weight\": 1.5,\n",
        "      \"dimensions\": {\"length\": 10, \"width\": 5, \"height\": 2}\n",
        "    },\n",
        "    \"inventory\": {\"store1\": 20, \"store2\": 15},\n",
        "    \"reviews\": [{\"user\": \"user1\", \"rating\": 4.5, \"comment\": \"Great product\"}]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "new_data = spark.read.json(sc.parallelize([new_data_json]))\n",
        "```\n",
        "\n",
        "Hint: You'll need to write a recursive function to compare schema objects and identify differences at all levels of nesting. Your output should include a DataFrame showing all schema differences and the SQL ALTER TABLE statements needed to evolve the schema."
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, MapType\n",
        "import json\n",
        "\n",
        "sparkContext=spark.sparkContext\n",
        "\n",
        "# Target table schema (current)\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", StringType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"category\", StringType(), True),\n",
        "        StructField(\"price\", FloatType(), True),\n",
        "        StructField(\"tags\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"inventory\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "# New data with schema changes\n",
        "new_data_json = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"item1\",\n",
        "    \"name\": \"Product X\",\n",
        "    \"details\": {\n",
        "      \"category\": \"Electronics\",\n",
        "      \"price\": 299.99,\n",
        "      \"tags\": [\"new\", \"featured\"],\n",
        "      \"weight\": 1.5,\n",
        "      \"dimensions\": {\"length\": 10, \"width\": 5, \"height\": 2}\n",
        "    },\n",
        "    \"inventory\": {\"store1\": 20, \"store2\": 15},\n",
        "    \"reviews\": [{\"user\": \"user1\", \"rating\": 4.5, \"comment\": \"Great product\"}]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "new_data = spark.read.json(sparkContext.parallelize([new_data_json]))\n",
        "new_data.printSchema()"
      ],
      "metadata": {
        "id": "07Kn_V9vliQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3faee83-df19-485e-c8a3-ae47e3a34534"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- details: struct (nullable = true)\n",
            " |    |-- category: string (nullable = true)\n",
            " |    |-- dimensions: struct (nullable = true)\n",
            " |    |    |-- height: long (nullable = true)\n",
            " |    |    |-- length: long (nullable = true)\n",
            " |    |    |-- width: long (nullable = true)\n",
            " |    |-- price: double (nullable = true)\n",
            " |    |-- tags: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- weight: double (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- inventory: struct (nullable = true)\n",
            " |    |-- store1: long (nullable = true)\n",
            " |    |-- store2: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- reviews: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- comment: string (nullable = true)\n",
            " |    |    |-- rating: double (nullable = true)\n",
            " |    |    |-- user: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_schema = new_data.schema\n",
        "new_data_schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4pcjv1QoRka",
        "outputId": "5d715eab-68ec-4b2d-c23f-a0d4be770cee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('details', StructType([StructField('category', StringType(), True), StructField('dimensions', StructType([StructField('height', LongType(), True), StructField('length', LongType(), True), StructField('width', LongType(), True)]), True), StructField('price', DoubleType(), True), StructField('tags', ArrayType(StringType(), True), True), StructField('weight', DoubleType(), True)]), True), StructField('id', StringType(), True), StructField('inventory', StructType([StructField('store1', LongType(), True), StructField('store2', LongType(), True)]), True), StructField('name', StringType(), True), StructField('reviews', ArrayType(StructType([StructField('comment', StringType(), True), StructField('rating', DoubleType(), True), StructField('user', StringType(), True)]), True), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5i_eYd7or3J",
        "outputId": "fa5ea172-a465-4b07-b394-6866bfa7bb3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('id', StringType(), False), StructField('name', StringType(), True), StructField('details', StructType([StructField('category', StringType(), True), StructField('price', FloatType(), True), StructField('tags', ArrayType(StringType(), True), True)]), True), StructField('inventory', MapType(StringType(), IntegerType(), True), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_schema(schema, prefix=\"\"):\n",
        "    fields = []\n",
        "    for field in schema.fields:\n",
        "        field_path = f\"{prefix}.{field.name}\" if prefix else field.name\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            fields += flatten_schema(field.dataType, field_path)\n",
        "        else:\n",
        "            fields.append((field_path, field.dataType, field.nullable))\n",
        "    return fields\n",
        "\n",
        "flatten_schema(target_schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKagdUZsqVn7",
        "outputId": "5ab91ecd-f840-427d-b581-7fe185397a5d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', StringType(), False),\n",
              " ('name', StringType(), True),\n",
              " ('details.category', StringType(), True),\n",
              " ('details.price', FloatType(), True),\n",
              " ('details.tags', ArrayType(StringType(), True), True),\n",
              " ('inventory', MapType(StringType(), IntegerType(), True), True)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_fields = dict((path, (dtype, nullable)) for path, dtype, nullable in flatten_schema(target_schema))\n",
        "new_fields = dict((path, (dtype, nullable)) for path, dtype, nullable in flatten_schema(new_data.schema))\n",
        "\n",
        "target_fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwQFcvXZq5sR",
        "outputId": "73f175fb-0d74-414e-d7fc-2c990d1449f8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': (StringType(), False),\n",
              " 'name': (StringType(), True),\n",
              " 'details.category': (StringType(), True),\n",
              " 'details.price': (FloatType(), True),\n",
              " 'details.tags': (ArrayType(StringType(), True), True),\n",
              " 'inventory': (MapType(StringType(), IntegerType(), True), True)}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "added = set(new_fields) - set(target_fields)\n",
        "removed = set(target_fields) - set(new_fields)\n",
        "modified = {k for k in (set(new_fields) & set(target_fields))\n",
        "            if new_fields[k] != target_fields[k]}\n"
      ],
      "metadata": {
        "id": "irsDKMUQzJK9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Fields added: {added}\\nFields removed: {removed}\\nFields modified: {modified}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sWoReszzPYI",
        "outputId": "85849965-70b0-4945-d1e2-481a139274fc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fields added: {'inventory.store1', 'reviews', 'details.dimensions.height', 'inventory.store2', 'details.dimensions.width', 'details.dimensions.length', 'details.weight'}\n",
            "Fields removed: {'inventory'}\n",
            "Fields modified: {'details.price', 'id'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "def pyspark_type_to_sql(dtype):\n",
        "    if isinstance(dtype, StringType):\n",
        "        return \"STRING\"\n",
        "    elif isinstance(dtype, IntegerType):\n",
        "        return \"INT\"\n",
        "    elif isinstance(dtype, FloatType):\n",
        "        return \"FLOAT\"\n",
        "    elif isinstance(dtype, DoubleType):\n",
        "        return \"DOUBLE\"\n",
        "    elif isinstance(dtype, LongType):\n",
        "        return \"BIGINT\"\n",
        "    elif isinstance(dtype, BooleanType):\n",
        "        return \"BOOLEAN\"\n",
        "    elif isinstance(dtype, ArrayType):\n",
        "        return f\"ARRAY<{pyspark_type_to_sql(dtype.elementType)}>\"\n",
        "    elif isinstance(dtype, MapType):\n",
        "        return f\"MAP<{pyspark_type_to_sql(dtype.keyType)}, {pyspark_type_to_sql(dtype.valueType)}>\"\n",
        "    elif isinstance(dtype, StructType):\n",
        "        fields = [f\"{f.name}: {pyspark_type_to_sql(f.dataType)}\" for f in dtype.fields]\n",
        "        return f\"STRUCT<{', '.join(fields)}>\"\n",
        "    else:\n",
        "        return \"UNKNOWN\"\n",
        "\n",
        "\n",
        "for field in added:\n",
        "    dtype = new_fields[field][0]\n",
        "    sql_type = pyspark_type_to_sql(dtype)\n",
        "    print(f\"ALTER TABLE my_table ADD COLUMN {field} {sql_type};\")\n",
        "\n",
        "for field in removed:\n",
        "    dtype = target_fields[field][0]\n",
        "    sql_type = pyspark_type_to_sql(dtype)\n",
        "    print(f\"ALTER TABLE my_table DROP COLUMN {field};\")\n",
        "\n",
        "for field in modified:\n",
        "    dtype = new_fields[field][0]\n",
        "    sql_type = pyspark_type_to_sql(dtype)\n",
        "    print(f\"ALTER TABLE my_table ALTER COLUMN {field} {sql_type};\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY_hs4zazUxZ",
        "outputId": "7214438c-e95e-4df1-8e19-d09ad41bfb16"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER TABLE my_table ADD COLUMN inventory.store1 BIGINT;\n",
            "ALTER TABLE my_table ADD COLUMN reviews ARRAY<STRUCT<comment: STRING, rating: DOUBLE, user: STRING>>;\n",
            "ALTER TABLE my_table ADD COLUMN details.dimensions.height BIGINT;\n",
            "ALTER TABLE my_table ADD COLUMN inventory.store2 BIGINT;\n",
            "ALTER TABLE my_table ADD COLUMN details.dimensions.width BIGINT;\n",
            "ALTER TABLE my_table ADD COLUMN details.dimensions.length BIGINT;\n",
            "ALTER TABLE my_table ADD COLUMN details.weight DOUBLE;\n",
            "ALTER TABLE my_table DROP COLUMN inventory;\n",
            "ALTER TABLE my_table ALTER COLUMN details.price DOUBLE;\n",
            "ALTER TABLE my_table ALTER COLUMN id STRING;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lQRgsLU04FZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}