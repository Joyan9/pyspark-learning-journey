{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmkuy9bWlJgZGeLMB4f4WA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "ed5d0b07-51a2-4ab8-a6ec-fccebbf35aaf"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,659 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,718 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,907 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Fetched 24.1 MB in 8s (2,985 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "fe773a45-5c1a-44d5-d691-c3f3f25a609b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 7 - 2025/05/02**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Window Function for Moving Calculations\n",
        "Calculate a moving average of stock prices for multiple companies. For each stock, show the current day's closing price along with the 3-day and 7-day moving averages.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample stock price data\n",
        "stock_data = spark.createDataFrame([\n",
        "    (\"AAPL\", \"2024-04-01\", 175.20),\n",
        "    (\"AAPL\", \"2024-04-02\", 177.50),\n",
        "    (\"AAPL\", \"2024-04-03\", 176.80),\n",
        "    (\"AAPL\", \"2024-04-04\", 179.30),\n",
        "    (\"AAPL\", \"2024-04-05\", 180.25),\n",
        "    (\"AAPL\", \"2024-04-08\", 182.15),\n",
        "    (\"AAPL\", \"2024-04-09\", 181.45),\n",
        "    (\"AAPL\", \"2024-04-10\", 183.75),\n",
        "    (\"MSFT\", \"2024-04-01\", 410.50),\n",
        "    (\"MSFT\", \"2024-04-02\", 412.80),\n",
        "    (\"MSFT\", \"2024-04-03\", 408.25),\n",
        "    (\"MSFT\", \"2024-04-04\", 415.75),\n",
        "    (\"MSFT\", \"2024-04-05\", 417.90),\n",
        "    (\"MSFT\", \"2024-04-08\", 420.10),\n",
        "    (\"MSFT\", \"2024-04-09\", 419.25),\n",
        "    (\"MSFT\", \"2024-04-10\", 422.50)\n",
        "], [\"ticker\", \"date\", \"close_price\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "|ticker |date      |close_price|moving_avg_3day   |moving_avg_7day   |\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "|AAPL   |2024-04-10|183.75     |182.45            |181.02            |\n",
        "|AAPL   |2024-04-09|181.45     |181.28            |179.61            |\n",
        "|AAPL   |2024-04-08|182.15     |180.88            |178.74            |\n",
        "|...    |...       |...        |...               |...               |\n",
        "|MSFT   |2024-04-10|422.50     |420.62            |416.68            |\n",
        "|...    |...       |...        |...               |...               |\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# current day's closing price along with the 3-day and 7-day moving averages.\n",
        "from pyspark.sql.functions import col, avg, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample stock price data\n",
        "stock_data = spark.createDataFrame([\n",
        "    (\"AAPL\", \"2024-04-01\", 175.20),\n",
        "    (\"AAPL\", \"2024-04-02\", 177.50),\n",
        "    (\"AAPL\", \"2024-04-03\", 176.80),\n",
        "    (\"AAPL\", \"2024-04-04\", 179.30),\n",
        "    (\"AAPL\", \"2024-04-05\", 180.25),\n",
        "    (\"AAPL\", \"2024-04-08\", 182.15),\n",
        "    (\"AAPL\", \"2024-04-09\", 181.45),\n",
        "    (\"AAPL\", \"2024-04-10\", 183.75),\n",
        "    (\"MSFT\", \"2024-04-01\", 410.50),\n",
        "    (\"MSFT\", \"2024-04-02\", 412.80),\n",
        "    (\"MSFT\", \"2024-04-03\", 408.25),\n",
        "    (\"MSFT\", \"2024-04-04\", 415.75),\n",
        "    (\"MSFT\", \"2024-04-05\", 417.90),\n",
        "    (\"MSFT\", \"2024-04-08\", 420.10),\n",
        "    (\"MSFT\", \"2024-04-09\", 419.25),\n",
        "    (\"MSFT\", \"2024-04-10\", 422.50)\n",
        "], [\"ticker\", \"date\", \"close_price\"])\n",
        "\n",
        "# last two prices and current price\n",
        "window_3_day = Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(Window.currentRow - 2, Window.currentRow)\n",
        "\n",
        "# last six prices and current price\n",
        "window_7_day = Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(Window.currentRow - 6, Window.currentRow)\n",
        "\n",
        "\n",
        "stock_data = stock_data.withColumn(\"date\", F.to_date(\"date\"))\n",
        "\n",
        "moving_averages = stock_data.withColumn(\"moving_avg_3day\", F.round(F.avg(\"close_price\").over(window_3_day), 2))\\\n",
        "                            .withColumn(\"moving_avg_7day\", F.round(F.avg(\"close_price\").over(window_7_day), 2))\\\n",
        "                            .select(\"ticker\", \"date\", \"close_price\", \"moving_avg_3day\", \"moving_avg_7day\").orderBy(F.col(\"ticker\").asc(), F.col(\"date\").desc())\n",
        "\n",
        "moving_averages.show()\n"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773603d6-5996-4662-bb43-b6c5a0a8f283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-----------+---------------+---------------+\n",
            "|ticker|      date|close_price|moving_avg_3day|moving_avg_7day|\n",
            "+------+----------+-----------+---------------+---------------+\n",
            "|  AAPL|2024-04-10|     183.75|         182.45|         180.17|\n",
            "|  AAPL|2024-04-09|     181.45|         181.28|         178.95|\n",
            "|  AAPL|2024-04-08|     182.15|         180.57|         178.53|\n",
            "|  AAPL|2024-04-05|     180.25|         178.78|         177.81|\n",
            "|  AAPL|2024-04-04|      179.3|         177.87|          177.2|\n",
            "|  AAPL|2024-04-03|      176.8|          176.5|          176.5|\n",
            "|  AAPL|2024-04-02|      177.5|         176.35|         176.35|\n",
            "|  AAPL|2024-04-01|      175.2|          175.2|          175.2|\n",
            "|  MSFT|2024-04-10|      422.5|         420.62|         416.65|\n",
            "|  MSFT|2024-04-09|     419.25|         419.08|         414.94|\n",
            "|  MSFT|2024-04-08|      420.1|         417.92|         414.22|\n",
            "|  MSFT|2024-04-05|      417.9|         413.97|         413.04|\n",
            "|  MSFT|2024-04-04|     415.75|         412.27|         411.83|\n",
            "|  MSFT|2024-04-03|     408.25|         410.52|         410.52|\n",
            "|  MSFT|2024-04-02|      412.8|         411.65|         411.65|\n",
            "|  MSFT|2024-04-01|      410.5|          410.5|          410.5|\n",
            "+------+----------+-----------+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Data Deduplication with Business Rules\n",
        "Remove duplicate customer records based on complex business rules: if multiple records exist for the same customer, keep the most complete record with preference to more recent updates.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, coalesce, sum, max, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample customer data with duplicates and varying completeness\n",
        "customer_data = spark.createDataFrame([\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", \"123-456-7890\", \"123 Main St\", \"New York\", \"NY\", \"2024-01-15\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", None, \"123 Main St\", \"New York\", \"NY\", \"2024-02-20\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.s@email.com\", \"123-456-7890\", \"123 Main St\", None, \"NY\", \"2024-03-05\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", None, \"Los Angeles\", \"CA\", \"2024-01-10\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", \"456 Oak Ave\", \"Los Angeles\", \"CA\", \"2024-02-25\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.j@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-01-20\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.johnson@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-02-10\")\n",
        "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"last_updated\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "|customer_id |first_name|last_name|email                 |phone        |address      |city         |state|last_updated|\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "|1001        |John      |Smith    |john.smith@email.com  |123-456-7890 |123 Main St  |New York     |NY   |2024-03-05  |\n",
        "|1002        |Jane      |Doe      |jane.doe@email.com    |987-654-3210 |456 Oak Ave  |Los Angeles  |CA   |2024-02-25  |\n",
        "|1003        |Michael   |Johnson   |michael.johnson@email.com|555-123-4567|789 Pine St|Chicago      |IL   |2024-02-10  |\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when, coalesce, sum, max, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample customer data with duplicates and varying completeness\n",
        "customer_data = spark.createDataFrame([\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", \"123-456-7890\", \"123 Main St\", \"New York\", \"NY\", \"2024-01-15\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", None, \"123 Main St\", \"New York\", \"NY\", \"2024-02-20\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.s@email.com\", \"123-456-7890\", \"123 Main St\", None, \"NY\", \"2024-03-05\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", None, \"Los Angeles\", \"CA\", \"2024-01-10\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", \"456 Oak Ave\", \"Los Angeles\", \"CA\", \"2024-02-25\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.j@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-01-20\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.johnson@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-02-10\")\n",
        "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"last_updated\"])\n",
        "\n",
        "customer_data = customer_data.withColumn(\"last_updated\", F.to_date(\"last_updated\"))\n",
        "\n",
        "customer_data.select(\"*\").orderBy(F.col(\"customer_id\").asc(), F.col(\"last_updated\").desc()).show()\n"
      ],
      "metadata": {
        "id": "OTm-Krpq6X8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd6eb7e-c3b4-40b7-a564-94872ddbc4be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|customer_id|first_name|last_name|               email|       phone|    address|       city|state|last_updated|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|       1001|      John|    Smith|    john.s@email.com|123-456-7890|123 Main St|       NULL|   NY|  2024-03-05|\n",
            "|       1001|      John|    Smith|john.smith@email.com|        NULL|123 Main St|   New York|   NY|  2024-02-20|\n",
            "|       1001|      John|    Smith|john.smith@email.com|123-456-7890|123 Main St|   New York|   NY|  2024-01-15|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|456 Oak Ave|Los Angeles|   CA|  2024-02-25|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|       NULL|Los Angeles|   CA|  2024-01-10|\n",
            "|       1003|   Michael|  Johnson|michael.johnson@e...|555-123-4567|789 Pine St|    Chicago|   IL|  2024-02-10|\n",
            "|       1003|   Michael|  Johnson| michael.j@email.com|555-123-4567|789 Pine St|    Chicago|   IL|  2024-01-20|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Step 1: Calculate completeness score\n",
        "df_with_score = customer_data.withColumn(\"completeness_score\",\n",
        "    (F.col(\"customer_id\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"first_name\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"last_name\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"email\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"phone\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"address\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"city\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"state\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"last_updated\").isNotNull().cast(\"int\"))\n",
        ")\n",
        "\n",
        "# Step 2: Define window spec - ensure last_updated is correctly ordered\n",
        "window_spec = Window.partitionBy(\"customer_id\")\\\n",
        "                   .orderBy(F.col(\"completeness_score\").desc(),\n",
        "                            F.to_date(F.col(\"last_updated\"), \"yyyy-MM-dd\").desc())\n",
        "\n",
        "# Step 3: Rank the records\n",
        "df_ranked = df_with_score.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "# Step 4: Filter to keep only the top-ranked record\n",
        "df_deduplicated = df_ranked.filter(F.col(\"rank\") == 1).drop(\"completeness_score\", \"rank\")"
      ],
      "metadata": {
        "id": "Ig1gEBmIFSr7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduplicated.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLk2DN1GI2FK",
        "outputId": "ba051ae3-15cf-4e2a-abb0-9c26c960d0fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|customer_id|first_name|last_name|               email|       phone|    address|       city|state|last_updated|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|       1001|      John|    Smith|john.smith@email.com|123-456-7890|123 Main St|   New York|   NY|  2024-01-15|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|456 Oak Ave|Los Angeles|   CA|  2024-02-25|\n",
            "|       1003|   Michael|  Johnson|michael.johnson@e...|555-123-4567|789 Pine St|    Chicago|   IL|  2024-02-10|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Dynamic Pivot and Unpivot\n",
        "Create a solution that can dynamically pivot and unpivot data without hardcoding column names. Transform sales data from a normalized format to a crosstab view by product category and quarter, then transform it back.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr, lit, concat, collect_list, struct, first, avg\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample sales data in normalized format\n",
        "sales_data = spark.createDataFrame([\n",
        "    (\"Electronics\", \"Q1\", 1250000),\n",
        "    (\"Electronics\", \"Q2\", 1380000),\n",
        "    (\"Electronics\", \"Q3\", 1120000),\n",
        "    (\"Electronics\", \"Q4\", 1680000),\n",
        "    (\"Clothing\", \"Q1\", 850000),\n",
        "    (\"Clothing\", \"Q2\", 940000),\n",
        "    (\"Clothing\", \"Q3\", 1020000),\n",
        "    (\"Clothing\", \"Q4\", 1350000),\n",
        "    (\"Home Goods\", \"Q1\", 720000),\n",
        "    (\"Home Goods\", \"Q2\", 680000),\n",
        "    (\"Home Goods\", \"Q3\", 790000),\n",
        "    (\"Home Goods\", \"Q4\", 950000),\n",
        "    (\"Sports\", \"Q1\", 450000),\n",
        "    (\"Sports\", \"Q2\", 520000),\n",
        "    (\"Sports\", \"Q3\", 670000),\n",
        "    (\"Sports\", \"Q4\", 540000)\n",
        "], [\"category\", \"quarter\", \"sales\"])\n",
        "\n",
        "# You need to create two transformations:\n",
        "# 1. Pivoted data - categories as rows, quarters as columns\n",
        "# 2. Unpivoted data - return to original format from the pivoted data\n",
        "```\n",
        "\n",
        "Expected output for pivoted data:\n",
        "```\n",
        "+-------------+--------+--------+--------+--------+\n",
        "|category     |Q1_sales|Q2_sales|Q3_sales|Q4_sales|\n",
        "+-------------+--------+--------+--------+--------+\n",
        "|Electronics  |1250000 |1380000 |1120000 |1680000 |\n",
        "|Clothing     |850000  |940000  |1020000 |1350000 |\n",
        "|Home Goods   |720000  |680000  |790000  |950000  |\n",
        "|Sports       |450000  |520000  |670000  |540000  |\n",
        "+-------------+--------+--------+--------+--------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr, lit, concat, collect_list, struct, first, avg\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample sales data in normalized format\n",
        "sales_data = spark.createDataFrame([\n",
        "    (\"Electronics\", \"Q1\", 1250000),\n",
        "    (\"Electronics\", \"Q2\", 1380000),\n",
        "    (\"Electronics\", \"Q3\", 1120000),\n",
        "    (\"Electronics\", \"Q4\", 1680000),\n",
        "    (\"Clothing\", \"Q1\", 850000),\n",
        "    (\"Clothing\", \"Q2\", 940000),\n",
        "    (\"Clothing\", \"Q3\", 1020000),\n",
        "    (\"Clothing\", \"Q4\", 1350000),\n",
        "    (\"Home Goods\", \"Q1\", 720000),\n",
        "    (\"Home Goods\", \"Q2\", 680000),\n",
        "    (\"Home Goods\", \"Q3\", 790000),\n",
        "    (\"Home Goods\", \"Q4\", 950000),\n",
        "    (\"Sports\", \"Q1\", 450000),\n",
        "    (\"Sports\", \"Q2\", 520000),\n",
        "    (\"Sports\", \"Q3\", 670000),\n",
        "    (\"Sports\", \"Q4\", 540000)\n",
        "], [\"category\", \"quarter\", \"sales\"])\n",
        "\n",
        "sales_data.show()\n",
        "\n",
        "# 1. Pivoted data - categories as rows, quarters as columns\n",
        "sales_data_pivoted = sales_data.groupBy(\"category\").pivot(\"quarter\").agg(F.sum(\"sales\"))\n",
        "sales_data_pivoted.show()"
      ],
      "metadata": {
        "id": "GNhXJFlnOD2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c1dff4-d81b-4646-fa40-cbd8a9ecfa21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------+\n",
            "|   category|quarter|  sales|\n",
            "+-----------+-------+-------+\n",
            "|Electronics|     Q1|1250000|\n",
            "|Electronics|     Q2|1380000|\n",
            "|Electronics|     Q3|1120000|\n",
            "|Electronics|     Q4|1680000|\n",
            "|   Clothing|     Q1| 850000|\n",
            "|   Clothing|     Q2| 940000|\n",
            "|   Clothing|     Q3|1020000|\n",
            "|   Clothing|     Q4|1350000|\n",
            "| Home Goods|     Q1| 720000|\n",
            "| Home Goods|     Q2| 680000|\n",
            "| Home Goods|     Q3| 790000|\n",
            "| Home Goods|     Q4| 950000|\n",
            "|     Sports|     Q1| 450000|\n",
            "|     Sports|     Q2| 520000|\n",
            "|     Sports|     Q3| 670000|\n",
            "|     Sports|     Q4| 540000|\n",
            "+-----------+-------+-------+\n",
            "\n",
            "+-----------+-------+-------+-------+-------+\n",
            "|   category|     Q1|     Q2|     Q3|     Q4|\n",
            "+-----------+-------+-------+-------+-------+\n",
            "|     Sports| 450000| 520000| 670000| 540000|\n",
            "|Electronics|1250000|1380000|1120000|1680000|\n",
            "|   Clothing| 850000| 940000|1020000|1350000|\n",
            "| Home Goods| 720000| 680000| 790000| 950000|\n",
            "+-----------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Unpivoted data - return to original format from the pivoted data\n",
        "sales_data_pivoted.unpivot(\"category\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\"quarter\", \"sales\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho3PG_1cPEAj",
        "outputId": "fb61a4c5-ef41-44fe-f1c5-52f6725a82d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------+\n",
            "|   category|quarter|  sales|\n",
            "+-----------+-------+-------+\n",
            "|     Sports|     Q1| 450000|\n",
            "|     Sports|     Q2| 520000|\n",
            "|     Sports|     Q3| 670000|\n",
            "|     Sports|     Q4| 540000|\n",
            "|Electronics|     Q1|1250000|\n",
            "|Electronics|     Q2|1380000|\n",
            "|Electronics|     Q3|1120000|\n",
            "|Electronics|     Q4|1680000|\n",
            "|   Clothing|     Q1| 850000|\n",
            "|   Clothing|     Q2| 940000|\n",
            "|   Clothing|     Q3|1020000|\n",
            "|   Clothing|     Q4|1350000|\n",
            "| Home Goods|     Q1| 720000|\n",
            "| Home Goods|     Q2| 680000|\n",
            "| Home Goods|     Q3| 790000|\n",
            "| Home Goods|     Q4| 950000|\n",
            "+-----------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Sessionization with Complex Rules\n",
        "Implement a sessionization algorithm that groups user activity into sessions. A new session starts after 30 minutes of inactivity OR when a user switches device types, regardless of time.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, when, sum, lit, coalesce, unix_timestamp, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample user activity data\n",
        "user_logs = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-04-01 10:15:20\", \"mobile\", \"/home\"),\n",
        "    (\"user1\", \"2024-04-01 10:18:35\", \"mobile\", \"/products\"),\n",
        "    (\"user1\", \"2024-04-01 10:25:42\", \"mobile\", \"/product/123\"),\n",
        "    (\"user1\", \"2024-04-01 10:30:21\", \"desktop\", \"/product/123\"),  # Device change\n",
        "    (\"user1\", \"2024-04-01 10:32:15\", \"desktop\", \"/cart\"),\n",
        "    (\"user1\", \"2024-04-01 10:35:10\", \"desktop\", \"/checkout\"),\n",
        "    (\"user1\", \"2024-04-01 11:20:30\", \"desktop\", \"/home\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:05:12\", \"tablet\", \"/home\"),\n",
        "    (\"user2\", \"2024-04-01 14:08:25\", \"tablet\", \"/search\"),\n",
        "    (\"user2\", \"2024-04-01 14:12:30\", \"tablet\", \"/product/456\"),\n",
        "    (\"user2\", \"2024-04-01 14:45:12\", \"tablet\", \"/product/789\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:50:30\", \"mobile\", \"/product/789\"),  # Device change\n",
        "    (\"user2\", \"2024-04-01 14:52:45\", \"mobile\", \"/cart\")\n",
        "], [\"user_id\", \"timestamp\", \"device\", \"page\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "|user_id|timestamp          |device |page         |session_id|session_duration_min|\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "|user1  |2024-04-01 10:15:20|mobile |/home        |1         |10.37               |\n",
        "|user1  |2024-04-01 10:18:35|mobile |/products    |1         |10.37               |\n",
        "|user1  |2024-04-01 10:25:42|mobile |/product/123 |1         |10.37               |\n",
        "|user1  |2024-04-01 10:30:21|desktop|/product/123 |2         |4.82                |\n",
        "|user1  |2024-04-01 10:32:15|desktop|/cart        |2         |4.82                |\n",
        "|user1  |2024-04-01 10:35:10|desktop|/checkout    |2         |4.82                |\n",
        "|user1  |2024-04-01 11:20:30|desktop|/home        |3         |0.00                |\n",
        "|user2  |2024-04-01 14:05:12|tablet |/home        |1         |7.30                |\n",
        "|user2  |2024-04-01 14:08:25|tablet |/search      |1         |7.30                |\n",
        "|user2  |2024-04-01 14:12:30|tablet |/product/456 |1         |7.30                |\n",
        "|user2  |2024-04-01 14:45:12|tablet |/product/789 |2         |0.00                |\n",
        "|user2  |2024-04-01 14:50:30|mobile |/product/789 |3         |2.25                |\n",
        "|user2  |2024-04-01 14:52:45|mobile |/cart        |3         |2.25                |\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, when, sum, lit, coalesce, unix_timestamp, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample user activity data\n",
        "user_logs = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-04-01 10:15:20\", \"mobile\", \"/home\"),\n",
        "    (\"user1\", \"2024-04-01 10:18:35\", \"mobile\", \"/products\"),\n",
        "    (\"user1\", \"2024-04-01 10:25:42\", \"mobile\", \"/product/123\"),\n",
        "    (\"user1\", \"2024-04-01 10:30:21\", \"desktop\", \"/product/123\"),  # Device change\n",
        "    (\"user1\", \"2024-04-01 10:32:15\", \"desktop\", \"/cart\"),\n",
        "    (\"user1\", \"2024-04-01 10:35:10\", \"desktop\", \"/checkout\"),\n",
        "    (\"user1\", \"2024-04-01 11:20:30\", \"desktop\", \"/home\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:05:12\", \"tablet\", \"/home\"),\n",
        "    (\"user2\", \"2024-04-01 14:08:25\", \"tablet\", \"/search\"),\n",
        "    (\"user2\", \"2024-04-01 14:12:30\", \"tablet\", \"/product/456\"),\n",
        "    (\"user2\", \"2024-04-01 14:45:12\", \"tablet\", \"/product/789\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:50:30\", \"mobile\", \"/product/789\"),  # Device change\n",
        "    (\"user2\", \"2024-04-01 14:52:45\", \"mobile\", \"/cart\")\n",
        "], [\"user_id\", \"timestamp\", \"device\", \"page\"])\n",
        "\n",
        "user_logs = user_logs.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
        "\n",
        "user_logs.show()"
      ],
      "metadata": {
        "id": "-lQRgsLU04FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa42b4d-cf8e-4535-9262-32ca1e0b57ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+------------+\n",
            "|user_id|          timestamp| device|        page|\n",
            "+-------+-------------------+-------+------------+\n",
            "|  user1|2024-04-01 10:15:20| mobile|       /home|\n",
            "|  user1|2024-04-01 10:18:35| mobile|   /products|\n",
            "|  user1|2024-04-01 10:25:42| mobile|/product/123|\n",
            "|  user1|2024-04-01 10:30:21|desktop|/product/123|\n",
            "|  user1|2024-04-01 10:32:15|desktop|       /cart|\n",
            "|  user1|2024-04-01 10:35:10|desktop|   /checkout|\n",
            "|  user1|2024-04-01 11:20:30|desktop|       /home|\n",
            "|  user2|2024-04-01 14:05:12| tablet|       /home|\n",
            "|  user2|2024-04-01 14:08:25| tablet|     /search|\n",
            "|  user2|2024-04-01 14:12:30| tablet|/product/456|\n",
            "|  user2|2024-04-01 14:45:12| tablet|/product/789|\n",
            "|  user2|2024-04-01 14:50:30| mobile|/product/789|\n",
            "|  user2|2024-04-01 14:52:45| mobile|       /cart|\n",
            "+-------+-------------------+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for new session\n",
        "window_user_id = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
        "\n",
        "sessions = user_logs.withColumn(\"prev_timestamp\", F.lag(\"timestamp\", 1).over(window_user_id))\\\n",
        "                    .withColumn(\"prev_device\", F.lag(\"device\", 1).over(window_user_id))\\\n",
        "                    .withColumn(\"time_gap\", F.expr(\"(unix_timestamp(timestamp) - unix_timestamp(prev_timestamp))/60\"))\\\n",
        "                    .withColumn(\"is_new_session\", F.when((F.col(\"time_gap\")>30)|(F.col(\"device\")!=F.col(\"prev_device\")), 1).otherwise(0))\n",
        "\n",
        "\n",
        "# Create cumulative session ID by incrementing by 1 when there's a new session\n",
        "sessions = sessions \\\n",
        "                .withColumn(\"session_id\",\n",
        "                sum(\"is_new_session\").over(Window.partitionBy(\"user_id\").orderBy(\"timestamp\")) + 1)\n",
        "\n",
        "\n",
        "# create a session and user based window for session duration calc\n",
        "window_session = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
        "\n",
        "\n",
        "sessions.withColumn(\"session_start\", F.first(\"timestamp\").over(window_session))\\\n",
        "        .withColumn(\"session_end\", F.last(\"timestamp\").over(window_session))\\\n",
        "        .withColumn(\"session_duration\", F.round(F.expr(\"(unix_timestamp(session_end) - unix_timestamp(session_start))/60\"), 2))\\\n",
        "        .select(\"user_id\", \"timestamp\", \"device\", \"page\", \"session_id\", \"session_duration\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChSmUdc_QQjn",
        "outputId": "556516ea-6437-47db-edcc-bbde44763f0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+------------+----------+----------------+\n",
            "|user_id|          timestamp| device|        page|session_id|session_duration|\n",
            "+-------+-------------------+-------+------------+----------+----------------+\n",
            "|  user1|2024-04-01 10:15:20| mobile|       /home|         1|             0.0|\n",
            "|  user1|2024-04-01 10:18:35| mobile|   /products|         1|            3.25|\n",
            "|  user1|2024-04-01 10:25:42| mobile|/product/123|         1|           10.37|\n",
            "|  user1|2024-04-01 10:30:21|desktop|/product/123|         2|             0.0|\n",
            "|  user1|2024-04-01 10:32:15|desktop|       /cart|         2|             1.9|\n",
            "|  user1|2024-04-01 10:35:10|desktop|   /checkout|         2|            4.82|\n",
            "|  user1|2024-04-01 11:20:30|desktop|       /home|         3|             0.0|\n",
            "|  user2|2024-04-01 14:05:12| tablet|       /home|         1|             0.0|\n",
            "|  user2|2024-04-01 14:08:25| tablet|     /search|         1|            3.22|\n",
            "|  user2|2024-04-01 14:12:30| tablet|/product/456|         1|             7.3|\n",
            "|  user2|2024-04-01 14:45:12| tablet|/product/789|         2|             0.0|\n",
            "|  user2|2024-04-01 14:50:30| mobile|/product/789|         3|             0.0|\n",
            "|  user2|2024-04-01 14:52:45| mobile|       /cart|         3|            2.25|\n",
            "+-------+-------------------+-------+------------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hwy7edvQP7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: Data Quality Monitoring\n",
        "Develop a data quality monitoring framework that checks for various types of data quality issues including null values, out-of-range values, duplicate records, and pattern violations, then generates a comprehensive data quality report.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, isnan, isnull, length, regexp_extract, min, max, countDistinct\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample dataset with various data quality issues\n",
        "customer_orders = spark.createDataFrame([\n",
        "    (1001, \"John Smith\", \"jsmith@email.com\", \"2024-03-15\", 120.50, \"credit_card\", \"completed\"),\n",
        "    (1002, \"Jane Doe\", \"jane.doe@email.com\", \"2024-03-16\", 85.75, \"paypal\", \"completed\"),\n",
        "    (1003, \"Bob Johnson\", None, \"2024-03-16\", 45.99, \"credit_card\", \"completed\"),\n",
        "    (1004, \"Mary Williams\", \"mw@invalid\", \"2024-03-17\", 210.25, \"credit_card\", \"completed\"),\n",
        "    (1005, \"Steve Brown\", \"steve.brown@email.com\", \"2024-03-18\", -15.99, \"paypal\", \"failed\"),\n",
        "    (1006, \"Emily Davis\", \"emily.davis@email.com\", \"2024-03-18\", 75.50, \"credit_card\", \"pending\"),\n",
        "    (1007, \"David Miller\", \"david.miller@email.com\", \"2024-03-19\", 0.0, \"gift_card\", \"completed\"),\n",
        "    (1008, \"Sarah Wilson\", \"sarah.wilson@email.com\", \"2024-33-19\", 125.99, \"credit_card\", \"completed\"),\n",
        "    (1009, \"Michael Taylor\", \"michael.taylor@email.com\", \"2024-03-20\", 50.75, \"unknown\", \"completed\"),\n",
        "    (1010, \"Jennifer Garcia\", \"jennifer.garcia@email.com\", \"2024-03-20\", 95.25, \"credit_card\", \"completed\"),\n",
        "    (1010, \"Jennifer Garcia\", \"jennifer.garcia@email.com\", \"2024-03-20\", 95.25, \"credit_card\", \"completed\")  # Duplicate\n",
        "], [\"order_id\", \"customer_name\", \"email\", \"order_date\", \"order_amount\", \"payment_method\", \"order_status\"])\n",
        "\n",
        "# Define data quality rules\n",
        "rules = [\n",
        "    {\"type\": \"not_null\", \"column\": \"email\", \"description\": \"Email should not be null\"},\n",
        "    {\"type\": \"format\", \"column\": \"email\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\", \"description\": \"Email format invalid\"},\n",
        "    {\"type\": \"range\", \"column\": \"order_amount\", \"min\": 0.01, \"description\": \"Order amount should be positive\"},\n",
        "    {\"type\": \"valid_values\", \"column\": \"payment_method\", \"values\": [\"credit_card\", \"paypal\", \"gift_card\"], \"description\": \"Unknown payment method\"},\n",
        "    {\"type\": \"date_format\", \"column\": \"order_date\", \"format\": \"yyyy-MM-dd\", \"description\": \"Invalid date format\"},\n",
        "    {\"type\": \"duplicates\", \"columns\": [\"order_id\"], \"description\": \"Duplicate order ID\"}\n",
        "]\n",
        "```\n",
        "\n",
        "Expected output should be a data quality report with metrics for each rule:\n",
        "```\n",
        "+-------------------------+--------+--------+----------+--------------------+\n",
        "|rule_description         |failures|total   |failure_pct|failing_examples   |\n",
        "+-------------------------+--------+--------+----------+--------------------+\n",
        "|Email should not be null |1       |11      |9.09      |[1003: Bob Johnson] |\n",
        "|Email format invalid     |1       |11      |9.09      |[1004: mw@invalid]  |\n",
        "|Order amount should be...|2       |11      |18.18     |[1005: -15.99, ...]|\n",
        "|Unknown payment method   |1       |11      |9.09      |[1009: unknown]     |\n",
        "|Invalid date format      |1       |11      |9.09      |[1008: 2024-33-19]  |\n",
        "|Duplicate order ID       |1       |11      |9.09      |[1010]              |\n",
        "+-------------------------+--------+--------+----------+--------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "X5ocerM7PFvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when, isnan, isnull, length, regexp_extract, min, max, countDistinct\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample dataset with various data quality issues\n",
        "customer_orders = spark.createDataFrame([\n",
        "    (1001, \"John Smith\", \"jsmith@email.com\", \"2024-03-15\", 120.50, \"credit_card\", \"completed\"),\n",
        "    (1002, \"Jane Doe\", \"jane.doe@email.com\", \"2024-03-16\", 85.75, \"paypal\", \"completed\"),\n",
        "    (1003, \"Bob Johnson\", None, \"2024-03-16\", 45.99, \"credit_card\", \"completed\"),\n",
        "    (1004, \"Mary Williams\", \"mw@invalid\", \"2024-03-17\", 210.25, \"credit_card\", \"completed\"),\n",
        "    (1005, \"Steve Brown\", \"steve.brown@email.com\", \"2024-03-18\", -15.99, \"paypal\", \"failed\"),\n",
        "    (1006, \"Emily Davis\", \"emily.davis@email.com\", \"2024-03-18\", 75.50, \"credit_card\", \"pending\"),\n",
        "    (1007, \"David Miller\", \"david.miller@email.com\", \"2024-03-19\", 0.0, \"gift_card\", \"completed\"),\n",
        "    (1008, \"Sarah Wilson\", \"sarah.wilson@email.com\", \"2024-33-19\", 125.99, \"credit_card\", \"completed\"),\n",
        "    (1009, \"Michael Taylor\", \"michael.taylor@email.com\", \"2024-03-20\", 50.75, \"unknown\", \"completed\"),\n",
        "    (1010, \"Jennifer Garcia\", \"jennifer.garcia@email.com\", \"2024-03-20\", 95.25, \"credit_card\", \"completed\"),\n",
        "    (1010, \"Jennifer Garcia\", \"jennifer.garcia@email.com\", \"2024-03-20\", 95.25, \"credit_card\", \"completed\")  # Duplicate\n",
        "], [\"order_id\", \"customer_name\", \"email\", \"order_date\", \"order_amount\", \"payment_method\", \"order_status\"])\n",
        "\n",
        "customer_orders = customer_orders.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
        "\n",
        "# Define data quality rules\n",
        "rules = [\n",
        "    {\"type\": \"not_null\", \"column\": \"email\", \"description\": \"Email should not be null\"},\n",
        "    {\"type\": \"format\", \"column\": \"email\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\", \"description\": \"Email format invalid\"},\n",
        "    {\"type\": \"range\", \"column\": \"order_amount\", \"min\": 0.01, \"description\": \"Order amount should be positive\"},\n",
        "    {\"type\": \"valid_values\", \"column\": \"payment_method\", \"values\": [\"credit_card\", \"paypal\", \"gift_card\"], \"description\": \"Unknown payment method\"},\n",
        "    {\"type\": \"date_format\", \"column\": \"order_date\", \"format\": \"yyyy-MM-dd\", \"description\": \"Invalid date format\"},\n",
        "    {\"type\": \"duplicates\", \"columns\": [\"order_id\"], \"description\": \"Duplicate order ID\"}\n",
        "]"
      ],
      "metadata": {
        "id": "KfIa_2POPGNr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK3KPrBRYLxw",
        "outputId": "09372433-12dd-4e7c-e35f-c2b8aa7de053"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+--------------------+----------+------------+--------------+------------+\n",
            "|order_id|  customer_name|               email|order_date|order_amount|payment_method|order_status|\n",
            "+--------+---------------+--------------------+----------+------------+--------------+------------+\n",
            "|    1001|     John Smith|    jsmith@email.com|2024-03-15|       120.5|   credit_card|   completed|\n",
            "|    1002|       Jane Doe|  jane.doe@email.com|2024-03-16|       85.75|        paypal|   completed|\n",
            "|    1003|    Bob Johnson|                NULL|2024-03-16|       45.99|   credit_card|   completed|\n",
            "|    1004|  Mary Williams|          mw@invalid|2024-03-17|      210.25|   credit_card|   completed|\n",
            "|    1005|    Steve Brown|steve.brown@email...|2024-03-18|      -15.99|        paypal|      failed|\n",
            "|    1006|    Emily Davis|emily.davis@email...|2024-03-18|        75.5|   credit_card|     pending|\n",
            "|    1007|   David Miller|david.miller@emai...|2024-03-19|         0.0|     gift_card|   completed|\n",
            "|    1008|   Sarah Wilson|sarah.wilson@emai...|      NULL|      125.99|   credit_card|   completed|\n",
            "|    1009| Michael Taylor|michael.taylor@em...|2024-03-20|       50.75|       unknown|   completed|\n",
            "|    1010|Jennifer Garcia|jennifer.garcia@e...|2024-03-20|       95.25|   credit_card|   completed|\n",
            "|    1010|Jennifer Garcia|jennifer.garcia@e...|2024-03-20|       95.25|   credit_card|   completed|\n",
            "+--------+---------------+--------------------+----------+------------+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_orders = customer_orders.withColumn(\"is_email_null\", F.col(\"email\").isNull())\\\n",
        "                                .withColumn(\"is_email_valid\", F.coalesce(F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"), F.lit(False)))\\\n",
        "                                .withColumn(\"is_order_amount_valid\", F.when(F.col(\"order_amount\")<0.01, False).otherwise(True))\\\n",
        "                                .withColumn(\"is_payment_method_valid\", F.col(\"payment_method\").isin([\"credit_card\", \"paypal\", \"gift_card\"]))\\\n",
        "                                .withColumn(\"is_date_valid\", F.when(F.to_date(\"order_date\").isNull(), False).otherwise(True))\\\n",
        "                                .withColumn(\"is_duplicate\", F.when(F.row_number().over(Window.partitionBy(\"order_id\").orderBy(\"order_date\"))>1, True).otherwise(False))\n",
        ""
      ],
      "metadata": {
        "id": "MGi7afEgU0D8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_records = customer_orders.count()\n",
        "report_rows = []\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_email_null\") == True)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\", \"customer_name\").limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Email should not be null\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [f\"{row['order_id']}: {row['customer_name']}\" for row in examples]\n",
        "})\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_email_valid\") == False)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\", \"email\").limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Email format invalid\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [f\"{row['order_id']}: {row['email']}\" for row in examples]\n",
        "})\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_order_amount_valid\") == False)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\", \"order_amount\").limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Order amount should be positive\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [f\"{row['order_id']}: {row['order_amount']}\" for row in examples]\n",
        "})\n",
        "\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_payment_method_valid\") == False)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\", \"payment_method\").limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Unknown payment method\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [f\"{row['order_id']}: {row['payment_method']}\" for row in examples]\n",
        "})\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_date_valid\") == False)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\", \"order_date\").limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Invalid date format\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [f\"{row['order_id']}: {row['order_date']}\" for row in examples]\n",
        "})\n",
        "\n",
        "fail_df = customer_orders.filter(F.col(\"is_duplicate\") == True)\n",
        "fail_count = fail_df.count()\n",
        "examples = fail_df.select(\"order_id\").distinct().limit(5).collect()\n",
        "\n",
        "report_rows.append({\n",
        "    \"rule_description\": \"Duplicate order ID\",\n",
        "    \"failures\": fail_count,\n",
        "    \"total\": total_records,\n",
        "    \"failure_pct\": round((fail_count / total_records) * 100, 2),\n",
        "    \"failing_examples\": [str(row['order_id']) for row in examples]\n",
        "})\n",
        "\n",
        "\n",
        "report_df = spark.createDataFrame(report_rows)\n",
        "report_df.select(\"rule_description\", \"failures\", \"total\", \"failure_pct\", \"failing_examples\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqCf6ZHaUz8u",
        "outputId": "f7926863-5ed2-4589-8df0-31b00c5b094a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+--------+-----+-----------+------------------------------+\n",
            "|rule_description               |failures|total|failure_pct|failing_examples              |\n",
            "+-------------------------------+--------+-----+-----------+------------------------------+\n",
            "|Email should not be null       |1       |11   |9.09       |[1003: Bob Johnson]           |\n",
            "|Email format invalid           |2       |11   |18.18      |[1003: None, 1004: mw@invalid]|\n",
            "|Order amount should be positive|2       |11   |18.18      |[1005: -15.99, 1007: 0.0]     |\n",
            "|Unknown payment method         |1       |11   |9.09       |[1009: unknown]               |\n",
            "|Invalid date format            |1       |11   |9.09       |[1008: None]                  |\n",
            "|Duplicate order ID             |1       |11   |9.09       |[1010]                        |\n",
            "+-------------------------------+--------+-----+-----------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6: Advanced ETL with Error Handling and Recovery\n",
        "Implement a robust ETL process that handles various types of data errors gracefully, routing invalid records to error tables with appropriate error messages while successfully processing valid data.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, lit, to_date, regexp_extract, trim, length, udf\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType, BooleanType, DateType\n",
        "\n",
        "# Sample source data with various issues\n",
        "source_data = spark.createDataFrame([\n",
        "    (\"1001\", \"John Smith\", \"1985-04-15\", \"US\", \"12345\", \"100,000.50\", \"Y\"),\n",
        "    (\"1002\", \"Jane Doe\", \"1990-08-22\", \"CA\", \"ABC123\", \"75000.25\", \"N\"),\n",
        "    (\"ABC\", \"Bob Johnson\", \"1975/12/30\", \"UK\", \"SW1A 1AA\", \"82,500.75\", \"Y\"),\n",
        "    (\"1004\", \"Mary Williams\", \"2005-02-18\", \"US\", \"90210\", \"-25000.00\", \"Y\"),\n",
        "    (\"1005\", \"Steve Brown\", \"1982:07:11\", \"AU\", \"2000\", \"invalid\", \"Z\"),\n",
        "    (\"1006\", \"Emily Davis\", \"2000-13-45\", \"US\", \"60610\", \"65000.30\", \"N\"),\n",
        "    (\"1007\", \"\", \"1995-05-20\", \"DE\", \"10115\", \"120000.00\", \"Y\"),\n",
        "    (\"1008\", \"David Miller\", \"1988-03-10\", \"FR\", \"75001\", \"95,500.80\", \"X\"),\n",
        "    (\"1009\", \"Sarah Wilson\", \"1979-11-30\", None, \"Unknown\", \"82000.45\", \"N\"),\n",
        "    (\"1010\", \"Michael Taylor\", \"future\", \"US\", \"20500\", \"110,000.00\", \"Y\")\n",
        "], [\"id\", \"name\", \"birth_date\", \"country\", \"postal_code\", \"salary\", \"active\"])\n",
        "\n",
        "# Target schema with proper data types\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), False),\n",
        "    StructField(\"birth_date\", DateType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"postal_code\", StringType(), True),\n",
        "    StructField(\"salary\", DoubleType(), True),\n",
        "    StructField(\"active\", BooleanType(), True)\n",
        "])\n",
        "```\n",
        "\n",
        "Your solution should:\n",
        "1. Transform and validate each field according to the target schema requirements\n",
        "2. Create three dataframes: successfully processed records, records with errors, and an error log with detailed information\n",
        "3. Include proper error messages for each type of validation failure\n",
        "4. Implement a way to recover and fix simple errors automatically"
      ],
      "metadata": {
        "id": "YFCbg78APKaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit, to_date, regexp_extract, trim, length, udf, regexp_replace\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType, BooleanType, DateType\n",
        "\n",
        "# Sample source data with various issues\n",
        "source_data = spark.createDataFrame([\n",
        "    (\"1001\", \"John Smith\", \"1985-04-15\", \"US\", \"12345\", \"100,000.50\", \"Y\"),\n",
        "    (\"1002\", \"Jane Doe\", \"1990-08-22\", \"CA\", \"ABC123\", \"75000.25\", \"N\"),\n",
        "    (\"ABC\", \"Bob Johnson\", \"1975/12/30\", \"UK\", \"SW1A 1AA\", \"82,500.75\", \"Y\"),\n",
        "    (\"1004\", \"Mary Williams\", \"2005-02-18\", \"US\", \"90210\", \"-25000.00\", \"Y\"),\n",
        "    (\"1005\", \"Steve Brown\", \"1982:07:11\", \"AU\", \"2000\", \"invalid\", \"Z\"),\n",
        "    (\"1006\", \"Emily Davis\", \"2000-13-45\", \"US\", \"60610\", \"65000.30\", \"N\"),\n",
        "    (\"1007\", \"\", \"1995-05-20\", \"DE\", \"10115\", \"120000.00\", \"Y\"),\n",
        "    (\"1008\", \"David Miller\", \"1988-03-10\", \"FR\", \"75001\", \"95,500.80\", \"X\"),\n",
        "    (\"1009\", \"Sarah Wilson\", \"1979-11-30\", None, \"Unknown\", \"82000.45\", \"N\"),\n",
        "    (\"1010\", \"Michael Taylor\", \"future\", \"US\", \"20500\", \"110,000.00\", \"Y\")\n",
        "], [\"id\", \"name\", \"birth_date\", \"country\", \"postal_code\", \"salary\", \"active\"])\n",
        "\n",
        "# Target schema with proper data types\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), False),\n",
        "    StructField(\"birth_date\", DateType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"postal_code\", StringType(), True),\n",
        "    StructField(\"salary\", DoubleType(), True),\n",
        "    StructField(\"active\", BooleanType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "yaO4U7fePLBL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Validate and clean up the `salary` field (numeric issues)\n",
        "df = source_data.withColumn(\n",
        "    \"salary_cleaned\",\n",
        "    when(col(\"salary\").rlike(\"^[0-9,]+(\\.[0-9]+)?$\"), regexp_replace(col(\"salary\"), \",\", \"\").cast(DoubleType()))\n",
        "    .otherwise(lit(None))\n",
        ")\n",
        "\n",
        "# Step 2: Handle invalid salary errors\n",
        "df = df.withColumn(\n",
        "    \"salary_error\",\n",
        "    when(col(\"salary_cleaned\").isNull() & col(\"salary\").isNotNull(), \"Invalid salary value\")\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 3: Handle the `birth_date` field\n",
        "df = df.withColumn(\"parsed_birth_date\", to_date(\"birth_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Step 4: Identify and log invalid `birth_date` format errors\n",
        "df = df.withColumn(\n",
        "    \"birth_date_error\",\n",
        "    when(col(\"parsed_birth_date\").isNull() & col(\"birth_date\").isNotNull(), \"Invalid date format\").otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 5: Handle `country` field - checking for missing or invalid country codes\n",
        "df = df.withColumn(\n",
        "    \"country_error\",\n",
        "    when(col(\"country\").isNull() | (trim(col(\"country\")) == \"\"), \"Missing country\").otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 6: Handle `postal_code` field - checking for invalid postal codes (e.g., non-numeric or malformed)\n",
        "df = df.withColumn(\n",
        "    \"postal_code_error\",\n",
        "    when(col(\"postal_code\").rlike(\"^[A-Za-z0-9]+$\"), lit(None)).otherwise(\"Invalid postal code\")\n",
        ")\n",
        "\n",
        "# Step 7: Handle the `active` field\n",
        "df = df.withColumn(\n",
        "    \"active_error\",\n",
        "    when(~col(\"active\").isin(\"Y\", \"N\"), \"Invalid active status\").otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 8: Split into valid and invalid data\n",
        "valid_data = df.filter(\n",
        "    (col(\"salary_cleaned\").isNotNull()) &\n",
        "    (col(\"parsed_birth_date\").isNotNull()) &\n",
        "    (col(\"country_error\").isNull()) &\n",
        "    (col(\"postal_code_error\").isNull()) &\n",
        "    (col(\"active_error\").isNull())\n",
        ")\n",
        "\n",
        "# Invalid records are those that have any error\n",
        "invalid_data = df.filter(\n",
        "    (col(\"salary_error\").isNotNull()) |\n",
        "    (col(\"birth_date_error\").isNotNull()) |\n",
        "    (col(\"country_error\").isNotNull()) |\n",
        "    (col(\"postal_code_error\").isNotNull()) |\n",
        "    (col(\"active_error\").isNotNull())\n",
        ")\n",
        "\n",
        "# Step 9: Create error log with detailed messages\n",
        "error_log = invalid_data.select(\n",
        "    \"id\", \"salary_error\", \"birth_date_error\", \"country_error\", \"postal_code_error\", \"active_error\"\n",
        ").withColumn(\n",
        "    \"error_messages\",\n",
        "    when(col(\"salary_error\").isNotNull(), col(\"salary_error\"))\n",
        "    .otherwise(lit(\"\")) +\n",
        "    when(col(\"birth_date_error\").isNotNull(), lit(\" | \") + col(\"birth_date_error\"))\n",
        "    .otherwise(lit(\"\")) +\n",
        "    when(col(\"country_error\").isNotNull(), lit(\" | \") + col(\"country_error\"))\n",
        "    .otherwise(lit(\"\")) +\n",
        "    when(col(\"postal_code_error\").isNotNull(), lit(\" | \") + col(\"postal_code_error\"))\n",
        "    .otherwise(lit(\"\")) +\n",
        "    when(col(\"active_error\").isNotNull(), lit(\" | \") + col(\"active_error\"))\n",
        "    .otherwise(lit(\"\"))\n",
        ")\n",
        "\n",
        "# Step 10: Show results\n",
        "valid_data.show(truncate=False)\n",
        "invalid_data.show(truncate=False)\n",
        "error_log.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbxF3TLYbxBQ",
        "outputId": "5c42e96a-7c64-4105-fd8d-5aad68bed837"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+-------+-----------+----------+------+--------------+------------+-----------------+----------------+-------------+-----------------+------------+\n",
            "|id  |name      |birth_date|country|postal_code|salary    |active|salary_cleaned|salary_error|parsed_birth_date|birth_date_error|country_error|postal_code_error|active_error|\n",
            "+----+----------+----------+-------+-----------+----------+------+--------------+------------+-----------------+----------------+-------------+-----------------+------------+\n",
            "|1001|John Smith|1985-04-15|US     |12345      |100,000.50|Y     |100000.5      |NULL        |1985-04-15       |NULL            |NULL         |NULL             |NULL        |\n",
            "|1002|Jane Doe  |1990-08-22|CA     |ABC123     |75000.25  |N     |75000.25      |NULL        |1990-08-22       |NULL            |NULL         |NULL             |NULL        |\n",
            "|1007|          |1995-05-20|DE     |10115      |120000.00 |Y     |120000.0      |NULL        |1995-05-20       |NULL            |NULL         |NULL             |NULL        |\n",
            "+----+----------+----------+-------+-----------+----------+------+--------------+------------+-----------------+----------------+-------------+-----------------+------------+\n",
            "\n",
            "+----+--------------+----------+-------+-----------+----------+------+--------------+--------------------+-----------------+-------------------+---------------+-------------------+---------------------+\n",
            "|id  |name          |birth_date|country|postal_code|salary    |active|salary_cleaned|salary_error        |parsed_birth_date|birth_date_error   |country_error  |postal_code_error  |active_error         |\n",
            "+----+--------------+----------+-------+-----------+----------+------+--------------+--------------------+-----------------+-------------------+---------------+-------------------+---------------------+\n",
            "|ABC |Bob Johnson   |1975/12/30|UK     |SW1A 1AA   |82,500.75 |Y     |82500.75      |NULL                |NULL             |Invalid date format|NULL           |Invalid postal code|NULL                 |\n",
            "|1004|Mary Williams |2005-02-18|US     |90210      |-25000.00 |Y     |NULL          |Invalid salary value|2005-02-18       |NULL               |NULL           |NULL               |NULL                 |\n",
            "|1005|Steve Brown   |1982:07:11|AU     |2000       |invalid   |Z     |NULL          |Invalid salary value|NULL             |Invalid date format|NULL           |NULL               |Invalid active status|\n",
            "|1006|Emily Davis   |2000-13-45|US     |60610      |65000.30  |N     |65000.3       |NULL                |NULL             |Invalid date format|NULL           |NULL               |NULL                 |\n",
            "|1008|David Miller  |1988-03-10|FR     |75001      |95,500.80 |X     |95500.8       |NULL                |1988-03-10       |NULL               |NULL           |NULL               |Invalid active status|\n",
            "|1009|Sarah Wilson  |1979-11-30|NULL   |Unknown    |82000.45  |N     |82000.45      |NULL                |1979-11-30       |NULL               |Missing country|NULL               |NULL                 |\n",
            "|1010|Michael Taylor|future    |US     |20500      |110,000.00|Y     |110000.0      |NULL                |NULL             |Invalid date format|NULL           |NULL               |NULL                 |\n",
            "+----+--------------+----------+-------+-----------+----------+------+--------------+--------------------+-----------------+-------------------+---------------+-------------------+---------------------+\n",
            "\n",
            "+----+--------------------+-------------------+---------------+-------------------+---------------------+--------------+\n",
            "|id  |salary_error        |birth_date_error   |country_error  |postal_code_error  |active_error         |error_messages|\n",
            "+----+--------------------+-------------------+---------------+-------------------+---------------------+--------------+\n",
            "|ABC |NULL                |Invalid date format|NULL           |Invalid postal code|NULL                 |NULL          |\n",
            "|1004|Invalid salary value|NULL               |NULL           |NULL               |NULL                 |NULL          |\n",
            "|1005|Invalid salary value|Invalid date format|NULL           |NULL               |Invalid active status|NULL          |\n",
            "|1006|NULL                |Invalid date format|NULL           |NULL               |NULL                 |NULL          |\n",
            "|1008|NULL                |NULL               |NULL           |NULL               |Invalid active status|NULL          |\n",
            "|1009|NULL                |NULL               |Missing country|NULL               |NULL                 |NULL          |\n",
            "|1010|NULL                |Invalid date format|NULL           |NULL               |NULL                 |NULL          |\n",
            "+----+--------------------+-------------------+---------------+-------------------+---------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7: Customer Churn Analysis and Prediction Features\n",
        "Create features for a customer churn prediction model by analyzing customer behavior patterns in the months leading up to either churning or remaining active.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, avg, sum, count, stddev, when, max, min, lag, dense_rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Customer subscription data\n",
        "subscriptions = spark.createDataFrame([\n",
        "    (101, \"active\", \"2023-01-15\", None),\n",
        "    (102, \"churned\", \"2023-02-10\", \"2023-11-15\"),\n",
        "    (103, \"active\", \"2023-03-20\", None),\n",
        "    (104, \"churned\", \"2023-01-05\", \"2023-08-20\"),\n",
        "    (105, \"active\", \"2023-04-12\", None),\n",
        "    (106, \"churned\", \"2023-02-28\", \"2023-06-05\"),\n",
        "    (107, \"active\", \"2023-05-15\", None),\n",
        "    (108, \"churned\", \"2023-03-10\", \"2023-12-22\"),\n",
        "    (109, \"active\", \"2023-01-20\", None),\n",
        "    (110, \"churned\", \"2023-04-05\", \"2023-10-10\")\n",
        "], [\"customer_id\", \"status\", \"start_date\", \"end_date\"])\n",
        "\n",
        "# Customer monthly activity data\n",
        "customer_activity = spark.createDataFrame([\n",
        "    # Customer 101 - Active\n",
        "    (101, \"2023-05\", 5, 450.00, 8, 2),\n",
        "    (101, \"2023-06\", 4, 380.00, 6, 1),\n",
        "    (101, \"2023-07\", 3, 310.00, 5, 0),\n",
        "    (101, \"2023-08\", 5, 420.00, 7, 1),\n",
        "    (101, \"2023-09\", 4, 350.00, 6, 0),\n",
        "    (101, \"2023-10\", 6, 520.00, 9, 2),\n",
        "    (101, \"2023-11\", 5, 480.00, 7, 1),\n",
        "    (101, \"2023-12\", 4, 420.00, 8, 0),\n",
        "    \n",
        "    # Customer 102 - Churned\n",
        "    (102, \"2023-05\", 5, 400.00, 7, 1),\n",
        "    (102, \"2023-06\", 3, 250.00, 5, 0),\n",
        "    (102, \"2023-07\", 2, 180.00, 4, 2),\n",
        "    (102, \"2023-08\", 2, 150.00, 3, 1),\n",
        "    (102, \"2023-09\", 1, 120.00, 2, 2),\n",
        "    (102, \"2023-10\", 1, 100.00, 1, 3),\n",
        "    (102, \"2023-11\", 0, 0.00, 0, 1),\n",
        "    \n",
        "    # More customers with similar patterns...\n",
        "    (103, \"2023-06\", 4, 350.00, 6, 0),\n",
        "    (103, \"2023-07\", 5, 420.00, 8, 1),\n",
        "    (103, \"2023-08\", 6, 510.00, 9, 0),\n",
        "    (103, \"2023-09\", 5, 480.00, 7, 1),\n",
        "    (103, \"2023-10\", 7, 580.00, 10, 0),\n",
        "    (103, \"2023-11\", 6, 520.00, 8, 0),\n",
        "    (103, \"2023-12\", 5, 490.00, 9, 1),\n",
        "    \n",
        "    (104, \"2023-04\", 4, 350.00, 6, 1),\n",
        "    (104, \"2023-05\", 3, 280.00, 5, 2),\n",
        "    (104, \"2023-06\", 2, 220.00, 4, 1),\n",
        "    (104, \"2023-07\", 1, 150.00, 3, 3),\n",
        "    (104, \"2023-08\", 0, 0.00, 0, 2)\n",
        "    \n",
        "    # Additional customers would be included here...\n",
        "], [\"customer_id\", \"month\", \"order_count\", \"spend_amount\", \"site_visits\", \"support_tickets\"])\n",
        "\n",
        "# Customer demographic data\n",
        "customer_demographics = spark.createDataFrame([\n",
        "    (101, \"Individual\", \"Premium\", 12),\n",
        "    (102, \"Business\", \"Basic\", 9),\n",
        "    (103, \"Individual\", \"Premium\", 10),\n",
        "    (104, \"Business\", \"Standard\", 7),\n",
        "    (105, \"Individual\", \"Basic\", 5),\n",
        "    (106, \"Business\", \"Premium\", 4),\n",
        "    (107, \"Individual\", \"Standard\", 8),\n",
        "    (108, \"Business\", \"Basic\", 11),\n",
        "    (109, \"Individual\", \"Premium\", 14),\n",
        "    (110, \"Business\", \"Standard\", 6)\n",
        "], [\"customer_id\", \"customer_type\", \"subscription_tier\", \"tenure_months\"])\n",
        "```\n",
        "\n",
        "Your solution should create features including:\n",
        "1. Activity trends in the 3 months before churning or the most recent 3 months for active customers\n",
        "2. Spending decline rate\n",
        "3. Customer engagement metrics\n",
        "4. Support ticket frequency\n",
        "5. Early warning indicators"
      ],
      "metadata": {
        "id": "N328BPRLPPYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, datediff, avg, sum, count, stddev, when, max, min, lag, dense_rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Customer subscription data\n",
        "subscriptions = spark.createDataFrame([\n",
        "    (101, \"active\", \"2023-01-15\", None),\n",
        "    (102, \"churned\", \"2023-02-10\", \"2023-11-15\"),\n",
        "    (103, \"active\", \"2023-03-20\", None),\n",
        "    (104, \"churned\", \"2023-01-05\", \"2023-08-20\"),\n",
        "    (105, \"active\", \"2023-04-12\", None),\n",
        "    (106, \"churned\", \"2023-02-28\", \"2023-06-05\"),\n",
        "    (107, \"active\", \"2023-05-15\", None),\n",
        "    (108, \"churned\", \"2023-03-10\", \"2023-12-22\"),\n",
        "    (109, \"active\", \"2023-01-20\", None),\n",
        "    (110, \"churned\", \"2023-04-05\", \"2023-10-10\")\n",
        "], [\"customer_id\", \"status\", \"start_date\", \"end_date\"])\n",
        "\n",
        "# Customer monthly activity data\n",
        "customer_activity = spark.createDataFrame([\n",
        "    # Customer 101 - Active\n",
        "    (101, \"2023-05\", 5, 450.00, 8, 2),\n",
        "    (101, \"2023-06\", 4, 380.00, 6, 1),\n",
        "    (101, \"2023-07\", 3, 310.00, 5, 0),\n",
        "    (101, \"2023-08\", 5, 420.00, 7, 1),\n",
        "    (101, \"2023-09\", 4, 350.00, 6, 0),\n",
        "    (101, \"2023-10\", 6, 520.00, 9, 2),\n",
        "    (101, \"2023-11\", 5, 480.00, 7, 1),\n",
        "    (101, \"2023-12\", 4, 420.00, 8, 0),\n",
        "\n",
        "    # Customer 102 - Churned\n",
        "    (102, \"2023-05\", 5, 400.00, 7, 1),\n",
        "    (102, \"2023-06\", 3, 250.00, 5, 0),\n",
        "    (102, \"2023-07\", 2, 180.00, 4, 2),\n",
        "    (102, \"2023-08\", 2, 150.00, 3, 1),\n",
        "    (102, \"2023-09\", 1, 120.00, 2, 2),\n",
        "    (102, \"2023-10\", 1, 100.00, 1, 3),\n",
        "    (102, \"2023-11\", 0, 0.00, 0, 1),\n",
        "\n",
        "    # More customers with similar patterns...\n",
        "    (103, \"2023-06\", 4, 350.00, 6, 0),\n",
        "    (103, \"2023-07\", 5, 420.00, 8, 1),\n",
        "    (103, \"2023-08\", 6, 510.00, 9, 0),\n",
        "    (103, \"2023-09\", 5, 480.00, 7, 1),\n",
        "    (103, \"2023-10\", 7, 580.00, 10, 0),\n",
        "    (103, \"2023-11\", 6, 520.00, 8, 0),\n",
        "    (103, \"2023-12\", 5, 490.00, 9, 1),\n",
        "\n",
        "    (104, \"2023-04\", 4, 350.00, 6, 1),\n",
        "    (104, \"2023-05\", 3, 280.00, 5, 2),\n",
        "    (104, \"2023-06\", 2, 220.00, 4, 1),\n",
        "    (104, \"2023-07\", 1, 150.00, 3, 3),\n",
        "    (104, \"2023-08\", 0, 0.00, 0, 2)\n",
        "\n",
        "    # Additional customers would be included here...\n",
        "], [\"customer_id\", \"month\", \"order_count\", \"spend_amount\", \"site_visits\", \"support_tickets\"])\n",
        "\n",
        "# Customer demographic data\n",
        "customer_demographics = spark.createDataFrame([\n",
        "    (101, \"Individual\", \"Premium\", 12),\n",
        "    (102, \"Business\", \"Basic\", 9),\n",
        "    (103, \"Individual\", \"Premium\", 10),\n",
        "    (104, \"Business\", \"Standard\", 7),\n",
        "    (105, \"Individual\", \"Basic\", 5),\n",
        "    (106, \"Business\", \"Premium\", 4),\n",
        "    (107, \"Individual\", \"Standard\", 8),\n",
        "    (108, \"Business\", \"Basic\", 11),\n",
        "    (109, \"Individual\", \"Premium\", 14),\n",
        "    (110, \"Business\", \"Standard\", 6)\n",
        "], [\"customer_id\", \"customer_type\", \"subscription_tier\", \"tenure_months\"])"
      ],
      "metadata": {
        "id": "53P_JeUEPQNL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activity trends in the 3 months before churning or the most recent 3 months for active customers\n",
        "# activity trend can be captured by the number of orders, amount spent, site visits, tickets raised\n",
        "churned_users = subscriptions.filter(F.col(\"status\")==\"churned\").join(customer_activity, on = [\"customer_id\"])\n",
        "\n",
        "active_users = subscriptions.filter(F.col(\"status\")==\"active\").join(customer_activity, on = [\"customer_id\"])\n",
        "\n",
        "# add reference month\n",
        "churned_users = churned_users.withColumn(\"reference_month\", F.date_format(\"end_date\", \"yyyy-MM\"))\n",
        "active_users = active_users.groupBy(\"customer_id\").agg(F.max(F.to_date(\"month\", \"yyyy-MM\")).alias(\"reference_month\"))\\\n",
        "                            .join(active_users, on = [\"customer_id\"])\n",
        "\n",
        "churned_users = churned_users.withColumn(\"month_date\", F.to_date(\"month\", \"yyyy-MM\"))\\\n",
        "                             .withColumn(\"reference_date\", F.to_date(F.col(\"reference_month\"), \"yyyy-MM\"))\n",
        "\n",
        "active_users = active_users.withColumn(\"month_date\", F.to_date(\"month\", \"yyyy-MM\"))\\\n",
        "                           .withColumn(\"reference_date\", F.to_date(F.col(\"reference_month\"), \"yyyy-MM\"))\n",
        "\n",
        "\n",
        "# filter for last 3 months\n",
        "churned_users_last3_months = churned_users.withColumn(\n",
        "    \"months_between\", F.abs(F.months_between(\"month_date\", \"reference_date\"))\n",
        ").filter(F.col(\"months_between\") <= 3)\n",
        "\n",
        "active_users_last3_months = active_users.withColumn(\n",
        "    \"months_between\", F.abs(F.months_between(\"month_date\", \"reference_date\"))\n",
        ").filter(F.col(\"months_between\") <= 3)\n"
      ],
      "metadata": {
        "id": "D_GWMhIXi2dW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_metrics_active = active_users_last3_months.groupBy(\"customer_id\").agg(\n",
        "    F.avg(\"order_count\").alias(\"avg_order_count\"),\n",
        "    F.avg(\"spend_amount\").alias(\"avg_spend\"),\n",
        "    F.avg(\"site_visits\").alias(\"avg_site_visits\"),\n",
        "    F.avg(\"support_tickets\").alias(\"avg_support_tickets\")\n",
        ")\n",
        "agg_metrics_active.show()\n",
        "\n",
        "agg_metrics_churned = churned_users_last3_months.groupBy(\"customer_id\").agg(\n",
        "    F.avg(\"order_count\").alias(\"avg_order_count\"),\n",
        "    F.avg(\"spend_amount\").alias(\"avg_spend\"),\n",
        "    F.avg(\"site_visits\").alias(\"avg_site_visits\"),\n",
        "    F.avg(\"support_tickets\").alias(\"avg_support_tickets\")\n",
        ")\n",
        "agg_metrics_churned.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "appJxNXBo2dr",
        "outputId": "fac79424-a085-4674-e7b3-bef85ab4937c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "|customer_id|avg_order_count|avg_spend|avg_site_visits|avg_support_tickets|\n",
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "|        101|           4.75|    442.5|            7.5|               0.75|\n",
            "|        103|           5.75|    517.5|            8.5|                0.5|\n",
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "\n",
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "|customer_id|avg_order_count|avg_spend|avg_site_visits|avg_support_tickets|\n",
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "|        102|            1.0|     92.5|            1.5|               1.75|\n",
            "|        104|            1.5|    162.5|            3.0|                2.0|\n",
            "+-----------+---------------+---------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"month\")\n",
        "\n",
        "active_trend_df = active_users_last3_months.withColumn(\"prev_order_count\", F.lag(\"order_count\").over(window_spec)) \\\n",
        "    .withColumn(\"order_count_diff\", F.col(\"order_count\") - F.col(\"prev_order_count\")) \\\n",
        "    .filter(F.col(\"order_count_diff\").isNotNull()) \\\n",
        "    .groupBy(\"customer_id\").agg(F.avg(\"order_count_diff\").alias(\"avg_order_count_diff\"))\n",
        "\n",
        "active_trend_df.show()\n",
        "\n",
        "\n",
        "churned_trend_df = churned_users_last3_months.withColumn(\"prev_order_count\", F.lag(\"order_count\").over(window_spec)) \\\n",
        "    .withColumn(\"order_count_diff\", F.col(\"order_count\") - F.col(\"prev_order_count\")) \\\n",
        "    .filter(F.col(\"order_count_diff\").isNotNull()) \\\n",
        "    .groupBy(\"customer_id\").agg(F.avg(\"order_count_diff\").alias(\"avg_order_count_diff\"))\n",
        "\n",
        "churned_trend_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ipxd1HepNxm",
        "outputId": "0aedc30d-c056-4b34-c9d3-ca55f631a575"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+\n",
            "|customer_id|avg_order_count_diff|\n",
            "+-----------+--------------------+\n",
            "|        101|                 0.0|\n",
            "|        103|                 0.0|\n",
            "+-----------+--------------------+\n",
            "\n",
            "+-----------+--------------------+\n",
            "|customer_id|avg_order_count_diff|\n",
            "+-----------+--------------------+\n",
            "|        102| -0.6666666666666666|\n",
            "|        104|                -1.0|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join all features together\n",
        "features_active = agg_metrics_active.join(active_trend_df, on=\"customer_id\")\n",
        "features_churned = agg_metrics_churned.join(churned_trend_df, on=\"customer_id\")\n",
        "\n",
        "# Show results\n",
        "features_active.show()\n",
        "features_churned.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAU8xTkAp-N-",
        "outputId": "1d9157b2-b216-4247-95d8-5d01b39f15cc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "|customer_id|avg_order_count|avg_spend|avg_site_visits|avg_support_tickets|avg_order_count_diff|\n",
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "|        101|           4.75|    442.5|            7.5|               0.75|                 0.0|\n",
            "|        103|           5.75|    517.5|            8.5|                0.5|                 0.0|\n",
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "\n",
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "|customer_id|avg_order_count|avg_spend|avg_site_visits|avg_support_tickets|avg_order_count_diff|\n",
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "|        102|            1.0|     92.5|            1.5|               1.75| -0.6666666666666666|\n",
            "|        104|            1.5|    162.5|            3.0|                2.0|                -1.0|\n",
            "+-----------+---------------+---------+---------------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Spending decline rate\n",
        "# Customer engagement metrics\n",
        "# Support ticket frequency\n",
        "# Early warning indicators"
      ],
      "metadata": {
        "id": "XTzLce4dkPR9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZH9RDdykPLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8: Anomaly Detection in Time Series Data\n",
        "Develop an anomaly detection system that identifies unusual patterns in time series data using multiple statistical methods: Z-score, modified Z-score, and IQR (Interquartile Range).\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, stddev, abs, median, percentile_approx, lit, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample sensor data with anomalies\n",
        "sensor_data = spark.createDataFrame([\n",
        "    (\"sensor1\", \"2024-04-01 00:00:00\", 25.2),\n",
        "    (\"sensor1\", \"2024-04-01 01:00:00\", 24.8),\n",
        "    (\"sensor1\", \"2024-04-01 02:00:00\", 24.5),\n",
        "    (\"sensor1\", \"2024-04-01 03:00:00\", 24.3),\n",
        "    (\"sensor1\", \"2024-04-01 04:00:00\", 24.1),\n",
        "    (\"sensor1\", \"2024-04-01 05:00:00\", 24.0),\n",
        "    (\"sensor1\", \"2024-04-01 06:00:00\", 24.2),\n",
        "    (\"sensor1\", \"2024-04-01 07:00:00\", 24.6),\n",
        "    (\"sensor1\", \"2024-04-01 08:00:00\", 25.0),\n",
        "    (\"sensor1\", \"2024-04-01 09:00:00\", 25.5),\n",
        "    (\"sensor1\", \"2024-04-01 10:00:00\", 35.8),  # Anomaly\n",
        "    (\"sensor1\", \"2024-04-01 11:00:00\", 26.5),\n",
        "    (\"sensor1\", \"2024-04-01 12:00:00\", 26.8),\n",
        "    (\"sensor1\", \"2024-04-01 13:00:00\", 27.1),\n",
        "    (\"sensor1\", \"2024-04-01 14:00:00\", 27.3),\n",
        "    (\"sensor1\", \"2024-04-01 15:00:00\", 17.2),  # Anomaly\n",
        "    (\"sensor1\", \"2024-04-01 16:00:00\", 27.5),\n",
        "    (\"sensor1\", \"2024-04-01 17:00:00\", 27.2),\n",
        "    (\"sensor1\", \"2024-04-01 18:00:00\", 26.8),\n",
        "    (\"sensor1\", \"2024-04-01 19:00:00\", 26.5),\n",
        "    (\"sensor1\", \"2024-04-01 20:00:00\", 26.2),\n",
        "    (\"sensor1\", \"2024-04-01 21:00:00\", 25.8),\n",
        "    (\"sensor1\", \"2024-04-01 22:00:00\", 25.5),\n",
        "    (\"sensor1\", \"2024-04-01 23:00:00\", 25.2),\n",
        "    \n",
        "    (\"sensor2\", \"2024-04-01 00:00:00\", 80.5),\n",
        "    (\"sensor2\", \"2024-04-01 01:00:00\", 81.2),\n",
        "    (\"sensor2\", \"2024-04-01 02:00:00\", 80.8),\n",
        "    (\"sensor2\", \"2024-04-01 03:00:00\", 81.0),\n",
        "    (\"sensor2\", \"2024-04-01 04:00:00\", 80.7),\n",
        "    (\"sensor2\", \"2024-04-01 05:00:00\", 80.9),\n",
        "    (\"sensor2\", \"2024-04-01 06:00:00\", 81.5),\n",
        "    (\"sensor2\", \"2024-04-01 07:00:00\", 82.0),\n",
        "    (\"sensor2\", \"2024-04-01 08:00:00\", 82.5),\n",
        "    (\"sensor2\", \"2024-04-01 09:00:00\", 50.2),  # Anomaly\n",
        "    (\"sensor2\", \"2024-04-01 10:00:00\", 83.5),\n",
        "    (\"sensor2\", \"2024-04-01 11:00:00\", 84.0),\n",
        "    (\"sensor2\", \"2024-04-01 12:00:00\", 120.8), # Anomaly\n",
        "    (\"sensor2\", \"2024-04-01 13:00:00\", 84.5),\n",
        "    (\"sensor2\", \"2024-04-01 14:00:00\", 85.0),\n",
        "    (\"sensor2\", \"2024-04-01 15:00:00\", 84.8),\n",
        "    (\"sensor2\", \"2024-04-01 16:00:00\", 84.5),\n",
        "    (\"sensor2\", \"2024-04-01 17:00:00\", 84.2),\n",
        "    (\"sensor2\", \"2024-04-01 18:00:00\", 83.8),\n",
        "    (\"sensor2\", \"2024-04-01 19:00:00\", 83.5),\n",
        "    (\"sensor2\", \"2024-04-01 20:00:00\", 83.0),\n",
        "    (\"sensor2\", \"2024-04-01 21:00:00\", 82.5),\n",
        "    (\"sensor2\", \"2024-04-01 22:00:00\", 82.0),\n",
        "    (\"sensor2\", \"2024-04-01 23:00:00\", 81.5)\n",
        "], [\"sensor_id\", \"timestamp\", \"reading\"])\n",
        "```\n",
        "\n",
        "Your solution should:\n",
        "1. Implement three anomaly detection methods: Z-score, modified Z-score, and IQR\n",
        "2. Compare the methods and identify which anomalies are detected by each method\n",
        "3. Calculate an anomaly score for each data point\n",
        "4. Visualize the anomalies in a meaningful way\n"
      ],
      "metadata": {
        "id": "pybYoHoPPV8a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF6URvVsPXkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}