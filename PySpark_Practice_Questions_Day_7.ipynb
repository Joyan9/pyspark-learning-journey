{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzDVcdML0qAcgA74uZf3d6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "c75bca66-6e35-4954-b2b0-c2fa44673390"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"daily_practice\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,642 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,712 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,901 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Fetched 24.1 MB in 9s (2,727 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "1eddd325-d09c-4c3b-efdc-fae9c919cbb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 7 - 2025/05/02**"
      ],
      "metadata": {
        "id": "8H3qhSSGk4Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Window Function for Moving Calculations\n",
        "Calculate a moving average of stock prices for multiple companies. For each stock, show the current day's closing price along with the 3-day and 7-day moving averages.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample stock price data\n",
        "stock_data = spark.createDataFrame([\n",
        "    (\"AAPL\", \"2024-04-01\", 175.20),\n",
        "    (\"AAPL\", \"2024-04-02\", 177.50),\n",
        "    (\"AAPL\", \"2024-04-03\", 176.80),\n",
        "    (\"AAPL\", \"2024-04-04\", 179.30),\n",
        "    (\"AAPL\", \"2024-04-05\", 180.25),\n",
        "    (\"AAPL\", \"2024-04-08\", 182.15),\n",
        "    (\"AAPL\", \"2024-04-09\", 181.45),\n",
        "    (\"AAPL\", \"2024-04-10\", 183.75),\n",
        "    (\"MSFT\", \"2024-04-01\", 410.50),\n",
        "    (\"MSFT\", \"2024-04-02\", 412.80),\n",
        "    (\"MSFT\", \"2024-04-03\", 408.25),\n",
        "    (\"MSFT\", \"2024-04-04\", 415.75),\n",
        "    (\"MSFT\", \"2024-04-05\", 417.90),\n",
        "    (\"MSFT\", \"2024-04-08\", 420.10),\n",
        "    (\"MSFT\", \"2024-04-09\", 419.25),\n",
        "    (\"MSFT\", \"2024-04-10\", 422.50)\n",
        "], [\"ticker\", \"date\", \"close_price\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "|ticker |date      |close_price|moving_avg_3day   |moving_avg_7day   |\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "|AAPL   |2024-04-10|183.75     |182.45            |181.02            |\n",
        "|AAPL   |2024-04-09|181.45     |181.28            |179.61            |\n",
        "|AAPL   |2024-04-08|182.15     |180.88            |178.74            |\n",
        "|...    |...       |...        |...               |...               |\n",
        "|MSFT   |2024-04-10|422.50     |420.62            |416.68            |\n",
        "|...    |...       |...        |...               |...               |\n",
        "+-------+----------+-----------+------------------+------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "3lGF2ODJlbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# current day's closing price along with the 3-day and 7-day moving averages.\n",
        "from pyspark.sql.functions import col, avg, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample stock price data\n",
        "stock_data = spark.createDataFrame([\n",
        "    (\"AAPL\", \"2024-04-01\", 175.20),\n",
        "    (\"AAPL\", \"2024-04-02\", 177.50),\n",
        "    (\"AAPL\", \"2024-04-03\", 176.80),\n",
        "    (\"AAPL\", \"2024-04-04\", 179.30),\n",
        "    (\"AAPL\", \"2024-04-05\", 180.25),\n",
        "    (\"AAPL\", \"2024-04-08\", 182.15),\n",
        "    (\"AAPL\", \"2024-04-09\", 181.45),\n",
        "    (\"AAPL\", \"2024-04-10\", 183.75),\n",
        "    (\"MSFT\", \"2024-04-01\", 410.50),\n",
        "    (\"MSFT\", \"2024-04-02\", 412.80),\n",
        "    (\"MSFT\", \"2024-04-03\", 408.25),\n",
        "    (\"MSFT\", \"2024-04-04\", 415.75),\n",
        "    (\"MSFT\", \"2024-04-05\", 417.90),\n",
        "    (\"MSFT\", \"2024-04-08\", 420.10),\n",
        "    (\"MSFT\", \"2024-04-09\", 419.25),\n",
        "    (\"MSFT\", \"2024-04-10\", 422.50)\n",
        "], [\"ticker\", \"date\", \"close_price\"])\n",
        "\n",
        "# last two prices and current price\n",
        "window_3_day = Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(Window.currentRow - 2, Window.currentRow)\n",
        "\n",
        "# last six prices and current price\n",
        "window_7_day = Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(Window.currentRow - 6, Window.currentRow)\n",
        "\n",
        "\n",
        "stock_data = stock_data.withColumn(\"date\", F.to_date(\"date\"))\n",
        "\n",
        "moving_averages = stock_data.withColumn(\"moving_avg_3day\", F.round(F.avg(\"close_price\").over(window_3_day), 2))\\\n",
        "                            .withColumn(\"moving_avg_7day\", F.round(F.avg(\"close_price\").over(window_7_day), 2))\\\n",
        "                            .select(\"ticker\", \"date\", \"close_price\", \"moving_avg_3day\", \"moving_avg_7day\").orderBy(F.col(\"ticker\").asc(), F.col(\"date\").desc())\n",
        "\n",
        "moving_averages.show()\n"
      ],
      "metadata": {
        "id": "sNiQEYIPk88-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13f1882-0cd6-49bd-8d57-83d47ba30bc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-----------+---------------+---------------+\n",
            "|ticker|      date|close_price|moving_avg_3day|moving_avg_7day|\n",
            "+------+----------+-----------+---------------+---------------+\n",
            "|  AAPL|2024-04-10|     183.75|         182.45|         180.17|\n",
            "|  AAPL|2024-04-09|     181.45|         181.28|         178.95|\n",
            "|  AAPL|2024-04-08|     182.15|         180.57|         178.53|\n",
            "|  AAPL|2024-04-05|     180.25|         178.78|         177.81|\n",
            "|  AAPL|2024-04-04|      179.3|         177.87|          177.2|\n",
            "|  AAPL|2024-04-03|      176.8|          176.5|          176.5|\n",
            "|  AAPL|2024-04-02|      177.5|         176.35|         176.35|\n",
            "|  AAPL|2024-04-01|      175.2|          175.2|          175.2|\n",
            "|  MSFT|2024-04-10|      422.5|         420.62|         416.65|\n",
            "|  MSFT|2024-04-09|     419.25|         419.08|         414.94|\n",
            "|  MSFT|2024-04-08|      420.1|         417.92|         414.22|\n",
            "|  MSFT|2024-04-05|      417.9|         413.97|         413.04|\n",
            "|  MSFT|2024-04-04|     415.75|         412.27|         411.83|\n",
            "|  MSFT|2024-04-03|     408.25|         410.52|         410.52|\n",
            "|  MSFT|2024-04-02|      412.8|         411.65|         411.65|\n",
            "|  MSFT|2024-04-01|      410.5|          410.5|          410.5|\n",
            "+------+----------+-----------+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: Data Deduplication with Business Rules\n",
        "Remove duplicate customer records based on complex business rules: if multiple records exist for the same customer, keep the most complete record with preference to more recent updates.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, coalesce, sum, max, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample customer data with duplicates and varying completeness\n",
        "customer_data = spark.createDataFrame([\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", \"123-456-7890\", \"123 Main St\", \"New York\", \"NY\", \"2024-01-15\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", None, \"123 Main St\", \"New York\", \"NY\", \"2024-02-20\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.s@email.com\", \"123-456-7890\", \"123 Main St\", None, \"NY\", \"2024-03-05\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", None, \"Los Angeles\", \"CA\", \"2024-01-10\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", \"456 Oak Ave\", \"Los Angeles\", \"CA\", \"2024-02-25\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.j@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-01-20\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.johnson@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-02-10\")\n",
        "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"last_updated\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "|customer_id |first_name|last_name|email                 |phone        |address      |city         |state|last_updated|\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "|1001        |John      |Smith    |john.smith@email.com  |123-456-7890 |123 Main St  |New York     |NY   |2024-03-05  |\n",
        "|1002        |Jane      |Doe      |jane.doe@email.com    |987-654-3210 |456 Oak Ave  |Los Angeles  |CA   |2024-02-25  |\n",
        "|1003        |Michael   |Johnson   |michael.johnson@email.com|555-123-4567|789 Pine St|Chicago      |IL   |2024-02-10  |\n",
        "+------------+----------+---------+----------------------+-------------+-------------+-------------+-----+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vjOJRiaulmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when, coalesce, sum, max, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample customer data with duplicates and varying completeness\n",
        "customer_data = spark.createDataFrame([\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", \"123-456-7890\", \"123 Main St\", \"New York\", \"NY\", \"2024-01-15\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.smith@email.com\", None, \"123 Main St\", \"New York\", \"NY\", \"2024-02-20\"),\n",
        "    (1001, \"John\", \"Smith\", \"john.s@email.com\", \"123-456-7890\", \"123 Main St\", None, \"NY\", \"2024-03-05\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", None, \"Los Angeles\", \"CA\", \"2024-01-10\"),\n",
        "    (1002, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"987-654-3210\", \"456 Oak Ave\", \"Los Angeles\", \"CA\", \"2024-02-25\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.j@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-01-20\"),\n",
        "    (1003, \"Michael\", \"Johnson\", \"michael.johnson@email.com\", \"555-123-4567\", \"789 Pine St\", \"Chicago\", \"IL\", \"2024-02-10\")\n",
        "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"last_updated\"])\n",
        "\n",
        "customer_data = customer_data.withColumn(\"last_updated\", F.to_date(\"last_updated\"))\n",
        "\n",
        "customer_data.select(\"*\").orderBy(F.col(\"customer_id\").asc(), F.col(\"last_updated\").desc()).show()\n"
      ],
      "metadata": {
        "id": "OTm-Krpq6X8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6993263b-0abb-449b-bd4a-138b2aaed72b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|customer_id|first_name|last_name|               email|       phone|    address|       city|state|last_updated|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|       1001|      John|    Smith|    john.s@email.com|123-456-7890|123 Main St|       NULL|   NY|  2024-03-05|\n",
            "|       1001|      John|    Smith|john.smith@email.com|        NULL|123 Main St|   New York|   NY|  2024-02-20|\n",
            "|       1001|      John|    Smith|john.smith@email.com|123-456-7890|123 Main St|   New York|   NY|  2024-01-15|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|456 Oak Ave|Los Angeles|   CA|  2024-02-25|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|       NULL|Los Angeles|   CA|  2024-01-10|\n",
            "|       1003|   Michael|  Johnson|michael.johnson@e...|555-123-4567|789 Pine St|    Chicago|   IL|  2024-02-10|\n",
            "|       1003|   Michael|  Johnson| michael.j@email.com|555-123-4567|789 Pine St|    Chicago|   IL|  2024-01-20|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Step 1: Calculate completeness score\n",
        "df_with_score = customer_data.withColumn(\"completeness_score\",\n",
        "    (F.col(\"customer_id\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"first_name\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"last_name\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"email\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"phone\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"address\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"city\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"state\").isNotNull().cast(\"int\") +\n",
        "     F.col(\"last_updated\").isNotNull().cast(\"int\"))\n",
        ")\n",
        "\n",
        "# Step 2: Define window spec - ensure last_updated is correctly ordered\n",
        "window_spec = Window.partitionBy(\"customer_id\")\\\n",
        "                   .orderBy(F.col(\"completeness_score\").desc(),\n",
        "                            F.to_date(F.col(\"last_updated\"), \"yyyy-MM-dd\").desc())\n",
        "\n",
        "# Step 3: Rank the records\n",
        "df_ranked = df_with_score.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "# Step 4: Filter to keep only the top-ranked record\n",
        "df_deduplicated = df_ranked.filter(F.col(\"rank\") == 1).drop(\"completeness_score\", \"rank\")"
      ],
      "metadata": {
        "id": "Ig1gEBmIFSr7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduplicated.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLk2DN1GI2FK",
        "outputId": "3e53c3bd-d3d7-4a0c-8e13-329576d9b3d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|customer_id|first_name|last_name|               email|       phone|    address|       city|state|last_updated|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "|       1001|      John|    Smith|john.smith@email.com|123-456-7890|123 Main St|   New York|   NY|  2024-01-15|\n",
            "|       1002|      Jane|      Doe|  jane.doe@email.com|987-654-3210|456 Oak Ave|Los Angeles|   CA|  2024-02-25|\n",
            "|       1003|   Michael|  Johnson|michael.johnson@e...|555-123-4567|789 Pine St|    Chicago|   IL|  2024-02-10|\n",
            "+-----------+----------+---------+--------------------+------------+-----------+-----------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Dynamic Pivot and Unpivot\n",
        "Create a solution that can dynamically pivot and unpivot data without hardcoding column names. Transform sales data from a normalized format to a crosstab view by product category and quarter, then transform it back.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr, lit, concat, collect_list, struct, first, avg\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample sales data in normalized format\n",
        "sales_data = spark.createDataFrame([\n",
        "    (\"Electronics\", \"Q1\", 1250000),\n",
        "    (\"Electronics\", \"Q2\", 1380000),\n",
        "    (\"Electronics\", \"Q3\", 1120000),\n",
        "    (\"Electronics\", \"Q4\", 1680000),\n",
        "    (\"Clothing\", \"Q1\", 850000),\n",
        "    (\"Clothing\", \"Q2\", 940000),\n",
        "    (\"Clothing\", \"Q3\", 1020000),\n",
        "    (\"Clothing\", \"Q4\", 1350000),\n",
        "    (\"Home Goods\", \"Q1\", 720000),\n",
        "    (\"Home Goods\", \"Q2\", 680000),\n",
        "    (\"Home Goods\", \"Q3\", 790000),\n",
        "    (\"Home Goods\", \"Q4\", 950000),\n",
        "    (\"Sports\", \"Q1\", 450000),\n",
        "    (\"Sports\", \"Q2\", 520000),\n",
        "    (\"Sports\", \"Q3\", 670000),\n",
        "    (\"Sports\", \"Q4\", 540000)\n",
        "], [\"category\", \"quarter\", \"sales\"])\n",
        "\n",
        "# You need to create two transformations:\n",
        "# 1. Pivoted data - categories as rows, quarters as columns\n",
        "# 2. Unpivoted data - return to original format from the pivoted data\n",
        "```\n",
        "\n",
        "Expected output for pivoted data:\n",
        "```\n",
        "+-------------+--------+--------+--------+--------+\n",
        "|category     |Q1_sales|Q2_sales|Q3_sales|Q4_sales|\n",
        "+-------------+--------+--------+--------+--------+\n",
        "|Electronics  |1250000 |1380000 |1120000 |1680000 |\n",
        "|Clothing     |850000  |940000  |1020000 |1350000 |\n",
        "|Home Goods   |720000  |680000  |790000  |950000  |\n",
        "|Sports       |450000  |520000  |670000  |540000  |\n",
        "+-------------+--------+--------+--------+--------+\n",
        "```"
      ],
      "metadata": {
        "id": "C3QAR2d1lqi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr, lit, concat, collect_list, struct, first, avg\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Sample sales data in normalized format\n",
        "sales_data = spark.createDataFrame([\n",
        "    (\"Electronics\", \"Q1\", 1250000),\n",
        "    (\"Electronics\", \"Q2\", 1380000),\n",
        "    (\"Electronics\", \"Q3\", 1120000),\n",
        "    (\"Electronics\", \"Q4\", 1680000),\n",
        "    (\"Clothing\", \"Q1\", 850000),\n",
        "    (\"Clothing\", \"Q2\", 940000),\n",
        "    (\"Clothing\", \"Q3\", 1020000),\n",
        "    (\"Clothing\", \"Q4\", 1350000),\n",
        "    (\"Home Goods\", \"Q1\", 720000),\n",
        "    (\"Home Goods\", \"Q2\", 680000),\n",
        "    (\"Home Goods\", \"Q3\", 790000),\n",
        "    (\"Home Goods\", \"Q4\", 950000),\n",
        "    (\"Sports\", \"Q1\", 450000),\n",
        "    (\"Sports\", \"Q2\", 520000),\n",
        "    (\"Sports\", \"Q3\", 670000),\n",
        "    (\"Sports\", \"Q4\", 540000)\n",
        "], [\"category\", \"quarter\", \"sales\"])\n",
        "\n",
        "sales_data.show()\n",
        "\n",
        "# 1. Pivoted data - categories as rows, quarters as columns\n",
        "sales_data_pivoted = sales_data.groupBy(\"category\").pivot(\"quarter\").agg(F.sum(\"sales\"))\n",
        "sales_data_pivoted.show()"
      ],
      "metadata": {
        "id": "GNhXJFlnOD2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9ec6d6-a6cc-413d-f8cd-671acdb9faa2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------+\n",
            "|   category|quarter|  sales|\n",
            "+-----------+-------+-------+\n",
            "|Electronics|     Q1|1250000|\n",
            "|Electronics|     Q2|1380000|\n",
            "|Electronics|     Q3|1120000|\n",
            "|Electronics|     Q4|1680000|\n",
            "|   Clothing|     Q1| 850000|\n",
            "|   Clothing|     Q2| 940000|\n",
            "|   Clothing|     Q3|1020000|\n",
            "|   Clothing|     Q4|1350000|\n",
            "| Home Goods|     Q1| 720000|\n",
            "| Home Goods|     Q2| 680000|\n",
            "| Home Goods|     Q3| 790000|\n",
            "| Home Goods|     Q4| 950000|\n",
            "|     Sports|     Q1| 450000|\n",
            "|     Sports|     Q2| 520000|\n",
            "|     Sports|     Q3| 670000|\n",
            "|     Sports|     Q4| 540000|\n",
            "+-----------+-------+-------+\n",
            "\n",
            "+-----------+-------+-------+-------+-------+\n",
            "|   category|     Q1|     Q2|     Q3|     Q4|\n",
            "+-----------+-------+-------+-------+-------+\n",
            "|     Sports| 450000| 520000| 670000| 540000|\n",
            "|Electronics|1250000|1380000|1120000|1680000|\n",
            "|   Clothing| 850000| 940000|1020000|1350000|\n",
            "| Home Goods| 720000| 680000| 790000| 950000|\n",
            "+-----------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Unpivoted data - return to original format from the pivoted data\n",
        "sales_data_pivoted.unpivot(\"category\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\"quarter\", \"sales\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho3PG_1cPEAj",
        "outputId": "b93059bd-5d3e-4ed3-9bc8-97f6cbe94bfa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------+\n",
            "|   category|quarter|  sales|\n",
            "+-----------+-------+-------+\n",
            "|     Sports|     Q1| 450000|\n",
            "|     Sports|     Q2| 520000|\n",
            "|     Sports|     Q3| 670000|\n",
            "|     Sports|     Q4| 540000|\n",
            "|Electronics|     Q1|1250000|\n",
            "|Electronics|     Q2|1380000|\n",
            "|Electronics|     Q3|1120000|\n",
            "|Electronics|     Q4|1680000|\n",
            "|   Clothing|     Q1| 850000|\n",
            "|   Clothing|     Q2| 940000|\n",
            "|   Clothing|     Q3|1020000|\n",
            "|   Clothing|     Q4|1350000|\n",
            "| Home Goods|     Q1| 720000|\n",
            "| Home Goods|     Q2| 680000|\n",
            "| Home Goods|     Q3| 790000|\n",
            "| Home Goods|     Q4| 950000|\n",
            "+-----------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Sessionization with Complex Rules\n",
        "Implement a sessionization algorithm that groups user activity into sessions. A new session starts after 30 minutes of inactivity OR when a user switches device types, regardless of time.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, when, sum, lit, coalesce, unix_timestamp, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample user activity data\n",
        "user_logs = spark.createDataFrame([\n",
        "    (\"user1\", \"2024-04-01 10:15:20\", \"mobile\", \"/home\"),\n",
        "    (\"user1\", \"2024-04-01 10:18:35\", \"mobile\", \"/products\"),\n",
        "    (\"user1\", \"2024-04-01 10:25:42\", \"mobile\", \"/product/123\"),\n",
        "    (\"user1\", \"2024-04-01 10:30:21\", \"desktop\", \"/product/123\"),  # Device change\n",
        "    (\"user1\", \"2024-04-01 10:32:15\", \"desktop\", \"/cart\"),\n",
        "    (\"user1\", \"2024-04-01 10:35:10\", \"desktop\", \"/checkout\"),\n",
        "    (\"user1\", \"2024-04-01 11:20:30\", \"desktop\", \"/home\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:05:12\", \"tablet\", \"/home\"),\n",
        "    (\"user2\", \"2024-04-01 14:08:25\", \"tablet\", \"/search\"),\n",
        "    (\"user2\", \"2024-04-01 14:12:30\", \"tablet\", \"/product/456\"),\n",
        "    (\"user2\", \"2024-04-01 14:45:12\", \"tablet\", \"/product/789\"),  # Time gap\n",
        "    (\"user2\", \"2024-04-01 14:50:30\", \"mobile\", \"/product/789\"),  # Device change\n",
        "    (\"user2\", \"2024-04-01 14:52:45\", \"mobile\", \"/cart\")\n",
        "], [\"user_id\", \"timestamp\", \"device\", \"page\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "|user_id|timestamp          |device |page         |session_id|session_duration_min|\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "|user1  |2024-04-01 10:15:20|mobile |/home        |1         |10.37               |\n",
        "|user1  |2024-04-01 10:18:35|mobile |/products    |1         |10.37               |\n",
        "|user1  |2024-04-01 10:25:42|mobile |/product/123 |1         |10.37               |\n",
        "|user1  |2024-04-01 10:30:21|desktop|/product/123 |2         |4.82                |\n",
        "|user1  |2024-04-01 10:32:15|desktop|/cart        |2         |4.82                |\n",
        "|user1  |2024-04-01 10:35:10|desktop|/checkout    |2         |4.82                |\n",
        "|user1  |2024-04-01 11:20:30|desktop|/home        |3         |0.00                |\n",
        "|user2  |2024-04-01 14:05:12|tablet |/home        |1         |7.30                |\n",
        "|user2  |2024-04-01 14:08:25|tablet |/search      |1         |7.30                |\n",
        "|user2  |2024-04-01 14:12:30|tablet |/product/456 |1         |7.30                |\n",
        "|user2  |2024-04-01 14:45:12|tablet |/product/789 |2         |0.00                |\n",
        "|user2  |2024-04-01 14:50:30|mobile |/product/789 |3         |2.25                |\n",
        "|user2  |2024-04-01 14:52:45|mobile |/cart        |3         |2.25                |\n",
        "+-------+-------------------+-------+-------------+----------+--------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "aj46DjzZluiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lQRgsLU04FZ"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}