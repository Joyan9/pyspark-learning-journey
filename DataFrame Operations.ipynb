{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a9f21-c6f8-48aa-b0e7-1ef1962e70a5",
   "metadata": {},
   "source": [
    "# DataFrame Operations\n",
    "Goal is to learn -> Basic Operations, Aggregations, Functions\t\n",
    "\n",
    "- Creating DataFrames\n",
    "- select, filter, where, orderBy\n",
    "- groupBy, agg, count, sum, avg\n",
    "- Window functions\n",
    "- Joins (inner, outer, cross)\n",
    "- UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf0de62-4ca1-4cad-b56b-e98c8362f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Spark Session is ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"dataframe_operations\").getOrCreate()\n",
    "print(\"You Spark Session is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a62c6d-4bb7-4b4f-ba39-51aa67ffaac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guns are loaded\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Guns are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d55b24-f88e-484b-bae2-34938e4cd60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows are loaded\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "print(\"Windows are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6a3260-511a-449f-83f4-451ca482e944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Type', 'Union', 'UserDefinedFunction', 'UserDefinedTableFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_create_py_udtf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aes_decrypt', 'aes_encrypt', 'aggregate', 'any_value', 'approxCountDistinct', 'approx_count_distinct', 'approx_percentile', 'array', 'array_agg', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_prepend', 'array_remove', 'array_repeat', 'array_size', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_and', 'bit_count', 'bit_get', 'bit_length', 'bit_or', 'bit_xor', 'bitmap_bit_position', 'bitmap_bucket_number', 'bitmap_construct_agg', 'bitmap_count', 'bitmap_or_agg', 'bitwiseNOT', 'bitwise_not', 'bool_and', 'bool_or', 'broadcast', 'bround', 'btrim', 'bucket', 'call_function', 'call_udf', 'cardinality', 'cast', 'cbrt', 'ceil', 'ceiling', 'char', 'char_length', 'character_length', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'contains', 'conv', 'convert_timezone', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'count_if', 'count_min_sketch', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'curdate', 'current_catalog', 'current_database', 'current_date', 'current_schema', 'current_timestamp', 'current_timezone', 'current_user', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'day', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decimal', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'e', 'element_at', 'elt', 'encode', 'endswith', 'equal_null', 'every', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'extract', 'factorial', 'filter', 'find_in_set', 'first', 'first_value', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'getbit', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'histogram_numeric', 'hll_sketch_agg', 'hll_sketch_estimate', 'hll_union', 'hll_union_agg', 'hour', 'hours', 'hypot', 'ifnull', 'ilike', 'initcap', 'inline', 'inline_outer', 'input_file_block_length', 'input_file_block_start', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnotnull', 'isnull', 'java_method', 'json_array_length', 'json_object_keys', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'last_value', 'lcase', 'lead', 'least', 'left', 'length', 'levenshtein', 'like', 'lit', 'ln', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'make_dt_interval', 'make_interval', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'make_ym_interval', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'mask', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'named_struct', 'nanvl', 'negate', 'negative', 'next_day', 'now', 'np', 'nth_value', 'ntile', 'nullif', 'nvl', 'nvl2', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'parse_url', 'percent_rank', 'percentile', 'percentile_approx', 'pi', 'pmod', 'posexplode', 'posexplode_outer', 'position', 'positive', 'pow', 'power', 'printf', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'reduce', 'reflect', 'regexp', 'regexp_count', 'regexp_extract', 'regexp_extract_all', 'regexp_instr', 'regexp_like', 'regexp_replace', 'regexp_substr', 'regr_avgx', 'regr_avgy', 'regr_count', 'regr_intercept', 'regr_r2', 'regr_slope', 'regr_sxx', 'regr_sxy', 'regr_syy', 'repeat', 'replace', 'reverse', 'right', 'rint', 'rlike', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'sign', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'some', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'split_part', 'sqrt', 'stack', 'startswith', 'std', 'stddev', 'stddev_pop', 'stddev_samp', 'str_to_map', 'struct', 'substr', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_binary', 'to_char', 'to_csv', 'to_date', 'to_json', 'to_number', 'to_str', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'to_varchar', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_add', 'try_aes_decrypt', 'try_avg', 'try_divide', 'try_element_at', 'try_multiply', 'try_remote_functions', 'try_subtract', 'try_sum', 'try_to_binary', 'try_to_number', 'try_to_timestamp', 'typeof', 'ucase', 'udf', 'udtf', 'unbase64', 'unhex', 'unix_date', 'unix_micros', 'unix_millis', 'unix_seconds', 'unix_timestamp', 'unwrap_udt', 'upper', 'url_decode', 'url_encode', 'user', 'var_pop', 'var_samp', 'variance', 'version', 'warnings', 'weekday', 'weekofyear', 'when', 'width_bucket', 'window', 'window_time', 'xpath', 'xpath_boolean', 'xpath_double', 'xpath_float', 'xpath_int', 'xpath_long', 'xpath_number', 'xpath_short', 'xpath_string', 'xxhash64', 'year', 'years', 'zip_with']\n"
     ]
    }
   ],
   "source": [
    "# List all functions in pyspark.sql.functions\n",
    "print(dir(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb7aa75-bd81-40e8-a4dd-3af1598b5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_file = \"transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd3d859-1f1e-48cc-b6ac-548beb8cef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_file = \"customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)\n",
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dfcaa-0522-44d3-ac07-cf8b029c4c15",
   "metadata": {},
   "source": [
    "## 1.1. Basic Operations (Easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2723ac-c9cb-4293-a390-fa92892bdc70",
   "metadata": {},
   "source": [
    "1. Load both parquet files and display the first 5 rows of each DataFrame.\n",
    "2. Count the total number of customers in the customers DataFrame.\n",
    "3. Display all unique expense types from the transactions DataFrame.\n",
    "4. Find all transactions with amounts greater than 1000.\n",
    "5. List all unique cities from both datasets.\n",
    "6. Count how many male and female customers are in the database.\n",
    "7. Find all transactions that occurred in 2017.\n",
    "8. Display the names of customers who live in \"New York\" city.\n",
    "9. Count the number of transactions for each expense type.\n",
    "10. Find the oldest customer in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52bd7280-2731-4047-8d94-282874f8e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|   cust_id|         name|age|gender|  birthday|  zip|       city|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9| Aaron Abbott| 34|Female| 7/13/1991|97823|     boston|\n",
      "|C00B971T1J| Aaron Austin| 37|Female|12/16/2004|30332|    chicago|\n",
      "|C00WRSJF1Q| Aaron Barnes| 29|Female| 3/11/1977|23451|     denver|\n",
      "|C01AZWQMF3|Aaron Barrett| 31|  Male|  7/9/1998|46613|los_angeles|\n",
      "|C01BKUFRHA| Aaron Becker| 54|  Male|11/24/1979|40284|  san_diego|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_file = \"transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transactions.show(5)\n",
    "\n",
    "customers_file = \"customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)\n",
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8c0ab0-5190-4f84-848c-a8aff9598665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the total number of customers in the customers DataFrame.\n",
    "df_customers.filter(F.col('cust_id')!='').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51d32b9-02d2-41f0-85bf-5e2aa37da67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       expense_type|\n",
      "+-------------------+\n",
      "|              Fines|\n",
      "|          Education|\n",
      "|      Entertainment|\n",
      "|            Housing|\n",
      "|            Savings|\n",
      "|          Groceries|\n",
      "|                Tax|\n",
      "|             Health|\n",
      "|           Clothing|\n",
      "|           Gambling|\n",
      "|       Motor/Travel|\n",
      "|Bills and Utilities|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all unique expense types from the transactions DataFrame.\n",
    "df_transactions.select(\"expense_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6452e59-cd2e-48c8-9da9-e4fc28470b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+------------+-------+-------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|expense_type|    amt|         city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+------------+-------+-------------+\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|TM1A3KGAH6MZM2G|2017-10-08|2017|   10|  8|     Housing|2663.91|    san_diego|\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|TYD3HUFENAI2QR5|2017-11-02|2017|   11|  2|   Education|1251.05|      seattle|\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|TNCQ08OEZKED3GV|2018-12-02|2018|   12|  2|   Education|1278.18|  los_angeles|\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|TNBWA6ZYJ07AD1G|2019-07-08|2019|    7|  8|     Housing|2901.35|      chicago|\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|THS7NFU1HW4LGQ9|2011-07-02|2011|    7|  2|   Education|1103.59|san_francisco|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all transactions with amounts greater than 1000.\n",
    "df_transactions.filter(F.col(\"amt\")>1000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed53a0e-b1bb-4560-adfb-3e0559e9289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     txn_city|\n",
      "+-------------+\n",
      "|    san_diego|\n",
      "|      chicago|\n",
      "|       denver|\n",
      "|       boston|\n",
      "|      seattle|\n",
      "|  los_angeles|\n",
      "|     new_york|\n",
      "|san_francisco|\n",
      "| philadelphia|\n",
      "|     portland|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all unique cities from both datasets.\n",
    "\n",
    "# Rename the 'city' column in both DataFrames before joining\n",
    "transactions_renamed = df_transactions.withColumnRenamed(\"city\", \"txn_city\")\n",
    "customers_renamed = df_customers.withColumnRenamed(\"city\", \"cust_city\")\n",
    "\n",
    "# Perform the join\n",
    "joined_df = transactions_renamed.join(customers_renamed, on=\"cust_id\", how=\"left\")\n",
    "joined_df.select(\"txn_city\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b41ea0b8-cc95-42ae-80c5-d81ca99de336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female| 2502|\n",
      "|  Male| 2498|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count how many male and female customers are in the database.\n",
    "df_customers.groupby(F.col(\"gender\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0043479d-49c3-493b-a4fc-292a88777c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+---------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|     city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+---------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T1FSOGKASVCV7FM|2017-07-14|2017|    7| 14|Entertainment|  4.01|   denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TODEIOF2REW2GKB|2017-03-25|2017|    3| 25|     Gambling| 164.5| new_york|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TU285PGHVE16XEX|2017-07-24|2017|    7| 24|Entertainment|  4.05|san_diego|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TEY9ROTMZD6AHVI|2017-04-04|2017|    4|  4|    Groceries|129.98| portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TXU0JZKYENLM0P5|2017-08-08|2017|    8|  8| Motor/Travel|  43.3|san_diego|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all transactions that occurred in 2017\n",
    "# Cast 'year' column to an integer and filter transactions from 2017\n",
    "df_transactions.filter(F.col(\"year\").cast(\"int\") == 2017).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34156641-64bf-4306-9a9b-6cdd7aa21b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+----------+-----+--------+\n",
      "|   cust_id|          name|age|gender|  birthday|  zip|    city|\n",
      "+----------+--------------+---+------+----------+-----+--------+\n",
      "|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|new_york|\n",
      "|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|new_york|\n",
      "|C04S4IDJV4| Aaron Gilbert| 24|Female|  7/2/2002|13359|new_york|\n",
      "|C06KZJ6B2B|    Aaron Hall| 36|  Male| 9/13/1977|14297|new_york|\n",
      "|C08MWU4U7A|  Aaron Jacobs| 38|  Male|12/21/1970|20955|new_york|\n",
      "|C0BBHD2UQE| Aaron Patrick| 21|  Male| 6/16/1989|05992|new_york|\n",
      "+----------+--------------+---+------+----------+-----+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the names of customers who live in \"New York\" city.\n",
    "# where is an alias of filter function\n",
    "df_customers.where(F.col(\"city\") == \"new_york\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f3007b1-2e2f-4c30-b324-77a7ed12dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|       expense_type|txn_count|\n",
      "+-------------------+---------+\n",
      "|              Fines|     7467|\n",
      "|          Education|   559518|\n",
      "|      Entertainment| 22417986|\n",
      "|            Housing|   261668|\n",
      "|            Savings|   357141|\n",
      "|          Groceries|  6473528|\n",
      "|                Tax|   453669|\n",
      "|             Health|  1136161|\n",
      "|           Clothing|  1165579|\n",
      "|           Gambling|   958807|\n",
      "|       Motor/Travel|  4738090|\n",
      "|Bills and Utilities|  1260478|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of transactions for each expense type.\n",
    "df_transactions.groupby(\"expense_type\").agg(F.count(\"txn_id\").alias(\"txn_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c45931a-e404-46a4-b279-0bec8ea610e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---+------+---------+-----+-------------+\n",
      "|   cust_id|             name|age|gender| birthday|  zip|         city|\n",
      "+----------+-----------------+---+------+---------+-----+-------------+\n",
      "|C07OSZNHI9|     Aaron Harper| 65|Female|3/29/2004|27672|      seattle|\n",
      "|C0FDN25R0K|     Abbie Abbott| 65|  Male|1/11/1970|53095|      seattle|\n",
      "|C0FQB13ESC|    Abbie Burgess| 65|  Male|9/15/1965|79294|    san_diego|\n",
      "|C153A5ZXKO|      Adam Atkins| 65|  Male| 9/4/1971|97631|  los_angeles|\n",
      "|C1AXBX5002|       Adam Hicks| 65|  Male|1/24/2003|39189|     portland|\n",
      "|C1JGG6YJM4|    Addie Bridges| 65|  Male|3/13/1995|79628| philadelphia|\n",
      "|C1RJXLKOVJ|    Addie Shelton| 65|Female|11/4/1999|10451|san_francisco|\n",
      "|C1Y80F4B9K|  Adelaide Harvey| 65|Female|5/22/1979|54573|      chicago|\n",
      "|C3B55OYDXS|    Agnes Flowers| 65|  Male| 8/8/1962|41089|     new_york|\n",
      "|C3LVXNC8ID|    Agnes Stanley| 65|  Male| 3/3/1977|66589|       denver|\n",
      "|C3ONI1HDNM|      Agnes Wells| 65|Female|7/22/2005|23392|san_francisco|\n",
      "|C3RKD368SW|      Aiden Clark| 65|Female|9/27/1988|87219|     portland|\n",
      "|C3V81RT0VS|      Aiden Jones| 65|  Male| 2/2/1958|80788|      seattle|\n",
      "|C44SSD1WWE|        Alan Boyd| 65|Female|7/14/1974|06467|  los_angeles|\n",
      "|C4MN7YS1B8| Albert Gutierrez| 65|  Male|11/5/1972|48882|  los_angeles|\n",
      "|C4P7AEI6CC|  Albert Jennings| 65|  Male|6/20/1994|51650|       denver|\n",
      "|C4ZV0R3K6C| Alberta Griffith| 65|Female|9/16/1969|25884| philadelphia|\n",
      "|C5KEL6OLHA|   Alejandro Rios| 65|  Male| 2/8/2002|03506|     new_york|\n",
      "|C5WQ32OGWO|      Alex Vaughn| 65|  Male|1/10/1996|67927|     portland|\n",
      "|C6AK22G1OF|Alexander Roberts| 65|  Male| 9/9/1969|77175|       boston|\n",
      "+----------+-----------------+---+------+---------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the oldest customer in the database.\n",
    "highest_age = df_customers.agg(F.max(\"age\").alias(\"highest_age\")).collect()[0][\"highest_age\"]\n",
    "\n",
    "df_customers.filter(F.col(\"age\")== highest_age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea753f35-ecb9-427d-ab34-6e6708606126",
   "metadata": {},
   "source": [
    "##### End of basic operations (easy)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46302894-937e-4c40-a0d0-6344c315800a",
   "metadata": {},
   "source": [
    "#\n",
    "#1.2. **Easy Questio**\r\n",
    "1. **Create a DataFrame from `transactions.parquet` and display its schema.**  \r\n",
    "   *Hint: Use `spark.read.parquet` and `printSchema`.*\r\n",
    "\r\n",
    "2. **Display the first 10 rows of the `transactions` DataFrame.**  \r\n",
    "   *Hint: Use `show` with a limit.*\r\n",
    "\r\n",
    "3. **Select the `cust_id`, `txn_id`, and `amt` columns from the `transactions` DataFrame.**  \r\n",
    "   *Hint: Use `select`.*\r\n",
    "\r\n",
    "4. **Filter all transactions where the `amt` is greater than 1000.**  \r\n",
    "   *Hint: Use `filter` or `where`.*\r\n",
    "\r\n",
    "5. **Sort the `transactions` DataFrame by the `amt` column in descending order.**  \r\n",
    "   *Hint: Use `orderBy`.*\r\n",
    "\r\n",
    "6. **Count the total number of transactions in the `transactions` DataFrame.**  \r\n",
    "   *Hint: Use `count`.*\r\n",
    "\r\n",
    "7. **Group the `transactions` DataFrame by `expense_type` and count the number of transactions for each type.**  \r\n",
    "   *Hint: Use `groupBy` and `count`.*\r\n",
    "\r\n",
    "8. **Read the `customers.parquet` file and display the distinct cities in the DataFrame.**  \r\n",
    "   *Hint: Use `distinct`.*\r\n",
    "\r\n",
    "9. **Find the total amount (`amt`) spent by all customers in the `transactions` DataFrame.**  \r\n",
    "   *Hint: Use `agg` with `sum`.*\r\n",
    "\r\n",
    "10. **Rename the `amt` column in the `transctiFrion on any of these questions! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9127b41-9ff7-4084-acfd-444f64f73742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|       expense_type|   amt|         city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+\n",
      "|C0YDPQWPBJ|2012-11-01|2020-11-01|TQS5CGEC5EI915T|2020-10-10|2020|   10| 10|       Motor/Travel|999.99|    san_diego|\n",
      "|CQN2R97ZSG|2011-11-01|2020-05-01|T2OM31ABM16KCHC|2019-01-04|2019|    1|  4|       Motor/Travel|999.99|san_francisco|\n",
      "|C3R72MS04F|2011-08-01|      NULL|TACKNPEPG630N0H|2020-03-07|2020|    3|  7|             Health|999.99|       boston|\n",
      "|C8I6XM3Y2A|2011-12-01|2020-09-01|TLGZFM6PSF4TJ4D|2013-11-05|2013|   11|  5|Bills and Utilities|999.99|     new_york|\n",
      "|C0YDPQWPBJ|2011-01-01|2020-03-01|TSYJ1TQBYYJAUVS|2011-07-06|2011|    7|  6|Bills and Utilities|999.99|       denver|\n",
      "|CLE3P7DBPR|2013-04-01|      NULL|TOAZ5LRVHSP991T|2018-10-07|2018|   10|  7|Bills and Utilities|999.98|     new_york|\n",
      "|C0YDPQWPBJ|2011-12-01|      NULL|TSYMBO1ZOBW179B|2012-09-10|2012|    9| 10|Bills and Utilities|999.98|       denver|\n",
      "|CVVC8ELOXF|2011-11-01|2019-11-01|TP7W3GX0RRECOWG|2014-08-03|2014|    8|  3|Bills and Utilities|999.98|san_francisco|\n",
      "|CCFR9ND2HX|2010-06-01|2019-01-01|TPW9C68DUSAJG5P|2014-03-08|2014|    3|  8|             Health|999.98|      chicago|\n",
      "|C0YDPQWPBJ|2010-12-01|2020-10-01|TCHWVJXII10R1CD|2018-07-02|2018|    7|  2|             Health|999.98| philadelphia|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sort the transactions DataFrame by the amt column in descending order.\n",
    "df_transactions.orderBy([\"amt\"],ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58acad3c-b9f2-4dd2-8a2d-911c5a0d9949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39790092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the total number of transactions in the transactions DataFrame.\n",
    "df_transactions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df32bc1-7a15-41e7-98cd-b9be63c23d87",
   "metadata": {},
   "source": [
    "### 2.1. Medium Questions\n",
    "11. **Join the `transactions` and `customers` DataFrames on `cust_id` to get a single DataFrame with customer details and their transactions.**  \n",
    "    *Hint: Use `join`.*\n",
    "\n",
    "12. **Find the average transaction amount (`amt`) for each `expense_type`.**  \n",
    "    *Hint: Use `groupBy` with `agg` and `avg`.*\n",
    "\n",
    "13. **Filter the `customers` DataFrame to only include customers who are 25 years old or older.**  \n",
    "    *Hint: Use `filter` with a condition on the `age` column.*\n",
    "\n",
    "14. **Add a new column to the `transactions` DataFrame called `txn_year` that extracts the year from the `date` column.**  \n",
    "    *Hint: Use `withColumn` and PySpark functions like `year`.*\n",
    "\n",
    "15. **Create a DataFrame with all unique pairs of `cust_id` and `expense_type` from the `transactions` DataFrame.**  \n",
    "    *Hint: Use `select` and `distinct`.*\n",
    "\n",
    "16. **Find the maximum transaction amount (`amt`) for each customer.**  \n",
    "    *Hint: Use `groupBy` and `agg` with `max`.*\n",
    "\n",
    "17. **Find the top 3 customers who have spent the highest total amount in transactions.**  \n",
    "    *Hint: Use `groupBy`, `agg`, and `orderBy` with `limit`.*\n",
    "\n",
    "18. **For each customer, find the total number of transactions they made and their average transaction amount.**  \n",
    "    *Hint: Use `groupBy` with `count` and `avg`.*\n",
    "\n",
    "19. **Filter all customers from the `customers` DataFrame who share the same city as their transactions.**  \n",
    "    *Hint: Use `join` on the `city` column with a filter.*\n",
    "\n",
    "20. **Find all transactions made in December (month = \"12\").**  \n",
    "    *Hint: Use `filter` with a condition on the `month` column.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "937454da-d20b-4f78-9931-d76fdd5bfd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|       expense_type|avg_amt|\n",
      "+-------------------+-------+\n",
      "|              Fines| 166.27|\n",
      "|          Education| 282.11|\n",
      "|      Entertainment|  24.38|\n",
      "|            Housing|1591.13|\n",
      "|            Savings| 230.73|\n",
      "|          Groceries|  82.71|\n",
      "|                Tax| 427.62|\n",
      "|             Health| 162.91|\n",
      "|           Clothing| 178.02|\n",
      "|           Gambling| 109.74|\n",
      "|       Motor/Travel| 124.99|\n",
      "|Bills and Utilities|  220.4|\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average transaction amount (amt) for each expense_type\n",
    "df_transactions.groupby(F.col(\"expense_type\")).agg(F.round(F.avg(\"amt\"),2).alias(\"avg_amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1615278c-d193-43b1-b78c-1db4b713e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|txn_start_year|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|          2010|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|          2010|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|          2010|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|          2010|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|          2010|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to the transactions DataFrame called txn_year that extracts the year from the date column.\n",
    "df_transactions.withColumn(\"txn_start_year\", F.year(F.col(\"start_date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a16978f-a2a3-4ba7-a225-39635d205195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|   cust_id|       expense_type|\n",
      "+----------+-------------------+\n",
      "|C1S3GH1FKR|              Fines|\n",
      "|COTEH9WRII|Bills and Utilities|\n",
      "|C0HCX8JU7B|          Education|\n",
      "|C0W0YS75TQ|            Savings|\n",
      "|CP665RP38K|          Groceries|\n",
      "|CM8GEPBJW0|           Clothing|\n",
      "|CHA4JTIZ9R|       Motor/Travel|\n",
      "|CMY3LTLU4P|            Savings|\n",
      "|CLTCHYIE8F|            Savings|\n",
      "|CAJ9AVEA8J|      Entertainment|\n",
      "|C44F5Y5V9W|           Clothing|\n",
      "|CGADRJ9CMF|            Housing|\n",
      "|CEUYCUI1GE|                Tax|\n",
      "|CJ359NPTTF|           Clothing|\n",
      "|CE9AFTP6UO|Bills and Utilities|\n",
      "|CRB5VB5VGF|      Entertainment|\n",
      "|CJIY3R0IJ2|      Entertainment|\n",
      "|C0YDPQWPBJ|            Housing|\n",
      "|CYV0UQGKF1|                Tax|\n",
      "|C8EKBL7G9T|          Groceries|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with all unique pairs of cust_id and expense_type from the transactions DataFrame\n",
    "df_transactions.select([\"cust_id\",\"expense_type\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a07f6848-5a0f-4313-9f62-672c5e52c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|   cust_id|highest_purchase_amt|\n",
      "+----------+--------------------+\n",
      "|C0YDPQWPBJ|              999.99|\n",
      "|C3R72MS04F|              999.99|\n",
      "|C8I6XM3Y2A|              999.99|\n",
      "|CQN2R97ZSG|              999.99|\n",
      "|CCFR9ND2HX|              999.98|\n",
      "|CLE3P7DBPR|              999.98|\n",
      "|CVVC8ELOXF|              999.98|\n",
      "|C0I1IG7GEX|              999.97|\n",
      "|C2CCLKG3EX|              999.97|\n",
      "|C226O53ID1|              999.97|\n",
      "|CP73FT1JD5|              999.97|\n",
      "|C8OQLZ2AVM|              999.96|\n",
      "|CNXGF8RRAM|              999.96|\n",
      "|CWX3K09BDQ|              999.96|\n",
      "|CZ9L6WLYUT|              999.96|\n",
      "|CLV775F7OR|              999.95|\n",
      "|CQKVJ5BB83|              999.95|\n",
      "|CXRL8VPR54|              999.95|\n",
      "|COGQ9IM7L1|              999.94|\n",
      "|C2ORXL6547|              999.94|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum transaction amount (amt) for each customer.\n",
    "df_transactions.groupby(\"cust_id\").agg(F.max(\"amt\").alias(\"highest_purchase_amt\"))\\\n",
    "                .orderBy(\"highest_purchase_amt\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3564d9b-e859-4cf6-9add-252d88d07c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|   cust_id|          total_amt|\n",
      "+----------+-------------------+\n",
      "|C0YDPQWPBJ|1.491545975629971E9|\n",
      "|CA9UYOQ5DA| 2028950.4899999967|\n",
      "|C1YG5D12KV| 2014760.8700000087|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 3 customers who have spent the highest total amount in transactions.\n",
    "df_transactions.groupby(\"cust_id\").agg(F.sum(\"amt\").alias(\"total_amt\"))\\\n",
    "                .orderBy(\"total_amt\", ascending=False)\\\n",
    "                .limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198997ef-9147-4811-9000-e3bbaa6a5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+\n",
      "|   cust_id|    txns|           avg_amt|\n",
      "+----------+--------+------------------+\n",
      "|C0YDPQWPBJ|17539732| 85.03812804152145|\n",
      "|C3KUDEN3KO|    7999|192.02384048005962|\n",
      "|CBW3FMEAU7|    7999| 65.58727340917598|\n",
      "|C89FCEGPJP|    7999| 49.99358669833731|\n",
      "|CHNFNR89ZV|    7998|20.588202050512546|\n",
      "|CFTWKHAVFW|    7997|10.566552457171412|\n",
      "|CZAZIXSCKP|    7995| 50.86983614759223|\n",
      "|CAKBTN7JJQ|    7995|140.51184615384645|\n",
      "|C8GCIG2WOM|    7995| 7.448561601000619|\n",
      "|CXRV8HXIQD|    7994|15.313982987240395|\n",
      "|CLZND1XQQZ|    7994| 104.4256067050287|\n",
      "|C9X9HA3C9I|    7994|34.243212409307034|\n",
      "|CZWMRCSUXO|    7994|30.166269702276665|\n",
      "|C57PDEGOUJ|    7994| 18.62143857893418|\n",
      "|CR1DHMX30F|    7994|57.927873405053965|\n",
      "|CID7KEOKJV|    7993|202.23004253721962|\n",
      "|CAIJYX09LT|    7993|58.399869886149965|\n",
      "|CHSQEB7XLF|    7993|213.13004378831454|\n",
      "|CP6ZEG31D9|    7993| 10.55756411860379|\n",
      "|CLT59OG1NE|    7992|124.48662912912876|\n",
      "+----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total number of transactions they made and their average transaction amount.\n",
    "# Group by 'cust_id' to find the total number of transactions and the average transaction amount\n",
    "df_transactions.groupby(\"cust_id\") \\\n",
    "    .agg(\n",
    "        F.count(\"txn_id\").alias(\"txns\"),  # Count total transactions\n",
    "        F.avg(\"amt\").alias(\"avg_amt\")    # Average transaction amount\n",
    "    ) \\\n",
    "    .orderBy(\"txns\", ascending=False) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125a4de8-b785-407d-ad80-59327528033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+----------+---------------+----------+----+-----+---+-------------+------+--------+---+------+---------+-----+\n",
      "|   cust_id|  city|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|    name|age|gender| birthday|  zip|\n",
      "+----------+------+----------+----------+---------------+----------+----+-----+---+-------------+------+--------+---+------+---------+-----+\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|T16BPI2Q485F7XF|2015-08-19|2015|    8| 19|     Gambling| 59.75|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|T1FSOGKASVCV7FM|2017-07-14|2017|    7| 14|Entertainment|  4.01|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TGO6A5NNOMI6IT5|2016-04-21|2016|    4| 21|Entertainment|  2.51|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TDEWL11NTJCN8CE|2016-06-02|2016|    6|  2|     Gambling| 59.88|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|T3M1E64TGFGV828|2018-10-09|2018|   10|  9|      Savings|182.71|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TLK5NNMW8JY2XFE|2010-07-23|2010|    7| 23| Motor/Travel| 17.48|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TJQUDUUP7MPJCRV|2011-07-08|2011|    7|  8|Entertainment|  2.26|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TVPQ0KD1Z8UQPW3|2016-04-19|2016|    4| 19|Entertainment|  1.89|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TZ70ARK9M27C7L1|2015-05-08|2015|    5|  8|    Groceries|  67.2|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TIBA1ZBLUPN1Z4P|2015-03-08|2015|    3|  8|     Gambling|102.21|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|T1DJAUGWBDSBI6Z|2012-12-08|2012|   12|  8|       Health| 21.77|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TCXEO4PRJNX87YR|2011-11-16|2011|   11| 16|    Groceries| 92.36|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TGRXTRH84ILDCS5|2013-04-17|2013|    4| 17|Entertainment|  2.65|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TZXP0SLWQLXG8LC|2011-10-19|2011|   10| 19|Entertainment| 12.29|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TJX0W78U0PTZP0C|2010-11-04|2010|   11|  4|Entertainment|  3.34|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TJDO9JLU1H8S68N|2014-06-09|2014|    6|  9|      Savings|182.71|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TZQC2HF8032Z0PD|2017-06-21|2017|    6| 21| Motor/Travel| 71.88|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|T2EALOM5QXE97CI|2015-10-20|2015|   10| 20| Motor/Travel| 46.52|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TY9SE8HZJQKUYLL|2015-12-09|2015|   12|  9|    Groceries| 97.32|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "|C0YDPQWPBJ|denver|2010-07-01|2018-12-01|TJ70XJFZAATOP4M|2013-04-20|2013|    4| 20| Motor/Travel| 61.65|Ada Lamb| 32|Female|9/29/2005|22457|\n",
      "+----------+------+----------+----------+---------------+----------+----+-----+---+-------------+------+--------+---+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter all customers from the customers DataFrame who share the same city as their transactions.\n",
    "df_transactions.join(F.broadcast(df_customers), on=[\"cust_id\",\"city\"] ,how = \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9e58e-4733-46d8-9ce6-f0513085469d",
   "metadata": {},
   "source": [
    "\r\n",
    "#2.2. # Intermediate Operations (Medium)\r\n",
    "\r\n",
    "11. Calculate the average transaction amount for each expense type, sorted by average a mount in descending order.\r\n",
    "\r\n",
    "12. Find customers who have made transactions in cities different from their residence city.\r\n",
    "\r\n",
    "13. Calculate the total amount spent by each customer, showing their name and total amount.\r\n",
    "\r\n",
    "14. Find the month with the highest number of transactions in each year.\r\n",
    "\r\n",
    "15. Create a summary showing for each customer:\r\n",
    "    - Total number of transactions\r\n",
    "    - Average transaction amount\r\n",
    "    - First transaction date\r\n",
    "    - Last transaction date\r\n",
    "\r\n",
    "16. Find customers who have made transactions in all expense types.\r\n",
    "\r\n",
    "17. Calculate the age distribution of customers (count of customers in different age groups: 18-25, 26-35, 36-45, 46-55, 56+).\r\n",
    "\r\n",
    "18. Find the top 3 customers who spent the most in each city.\r\n",
    "\r\n",
    "19. Calculate thectrunning total oustomer concepts in more detail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25479c2f-6e43-40ef-a9cf-9c838a72b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "|   cust_id|       city|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|\n",
      "+----------+-----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "|C0YDPQWPBJ|     boston|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|\n",
      "|C0YDPQWPBJ|   portland|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|\n",
      "|C0YDPQWPBJ|    chicago|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|\n",
      "|C0YDPQWPBJ|los_angeles|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|\n",
      "|C0YDPQWPBJ|    chicago|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|\n",
      "+----------+-----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find customers who have made transactions in cities different from their residence city.\n",
    "\n",
    "# left ant-join means that it will return only those rows from left DF that do not have matching data records in right DF\n",
    "df_transactions.join(F.broadcast(df_customers), on = [\"cust_id\", \"city\"], how = \"left_anti\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0af5d5-c722-4631-a672-da7abda8ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|   cust_id|           name|          sum(amt)|\n",
      "+----------+---------------+------------------+\n",
      "|CG1NEJSAXO|  Bernard Mason|1620561.4300000053|\n",
      "|CGTWUY5AGZ|  Bertha Sparks|283178.89000000054|\n",
      "|CV3G07K3WC|  Chris Osborne| 780173.4400000022|\n",
      "|CA59VTI86N|       Amy Wise| 693919.1100000032|\n",
      "|CL3B876N0W| Blanche Foster|411599.46000000095|\n",
      "|CKGIIMAL4T|   Blake Cortez| 191719.0899999989|\n",
      "|C6G96DT69P|Alfred Buchanan|1641742.7300000028|\n",
      "|CUWOXYDTSP|  Chris Chapman| 903847.9500000024|\n",
      "|CW1X61XN86|Christine Clark|254222.44000000038|\n",
      "|C0EFPK9NVV| Aaron Thornton| 893915.4100000007|\n",
      "|C34CEP15CV|   Adrian Stone| 660656.7900000014|\n",
      "|CYEKKU5GV5|Clifford Fuller| 348114.5799999992|\n",
      "|CM8GEPBJW0|   Brent Conner| 936285.4199999992|\n",
      "|CMS8N5LMMI|  Brett Lindsey|  590509.289999999|\n",
      "|CGQEQIV6B1|  Bertha Cooper| 431130.0600000015|\n",
      "|CGITEKW8I0|   Bernice Rowe| 230427.7499999999|\n",
      "|CYV0UQGKF1|    Clyde Tyler| 660873.9500000012|\n",
      "|CAKRBAKXAI|Andrew Schwartz| 1322510.729999998|\n",
      "|CXW17SCIUR| Claudia Torres|  373770.329999999|\n",
      "|CV7GLS4GOV| Christian Cain| 908197.9200000024|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total amount spent by each customer, showing their name and total amount.\n",
    "\n",
    "df_transactions_join_cols = df_transactions.select(F.col(\"cust_id\").alias(\"txn_cust_id\"), \"amt\")\n",
    "\n",
    "df_customers_amt = df_transactions_join_cols.join(df_customers, \n",
    "                                                  df_transactions_join_cols.txn_cust_id == df_customers.cust_id, \n",
    "                                                how = \"inner\")\n",
    "\n",
    "df_customers_amt.groupby(\"cust_id\",\"name\").agg(F.sum(\"amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60928d61-8228-4584-af52-dcba78233f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the month with the highest number of transactions in each year.\n",
    "df_txn_year_month =  df_transactions.groupby(\"year\",\"month\").agg(F.count(\"txn_id\").alias(\"transactions\"))\n",
    "\n",
    "# specify the window\n",
    "window_spec_txn_yr = Window.partitionBy(\"year\").orderBy(F.col(\"transactions\").desc())\n",
    "\n",
    "# rank the months using the window and filter for top mnths\n",
    "df_top_mnths_per_year = df_txn_year_month.withColumn(\"rank\", F.row_number().over(window_spec_txn_yr))\\\n",
    "                                        .filter(F.col(\"rank\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7217d8fb-fc2a-467c-9495-1ddd36d07dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----+\n",
      "|year|month|transactions|rank|\n",
      "+----+-----+------------+----+\n",
      "|2010|   12|      121503|   1|\n",
      "|2011|   12|      230898|   1|\n",
      "|2012|   12|      341426|   1|\n",
      "|2013|   12|      384566|   1|\n",
      "|2014|    5|      389520|   1|\n",
      "+----+-----+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top_mnths_per_year.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1065f407-9eee-41f3-a251-d527623d2f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+------------------+--------------+-------------+\n",
      "|   cust_id|         name|transactions|       avg_txn_amt|first_txn_date|last_txn_date|\n",
      "+----------+-------------+------------+------------------+--------------+-------------+\n",
      "|C007YEYTX9| Aaron Abbott|        7445|200.59494425789075|    2012-02-01|   2020-09-27|\n",
      "|C00B971T1J| Aaron Austin|        7532|133.01896840148697|    2012-10-01|   2020-12-27|\n",
      "|C00WRSJF1Q| Aaron Barnes|        7777|105.57785007072154|    2012-11-01|   2020-12-27|\n",
      "|C01AZWQMF3|Aaron Barrett|        7548|17.418098834128244|    2010-10-01|   2019-03-27|\n",
      "|C01BKUFRHA| Aaron Becker|        7401| 82.77615457370636|    2012-04-01|   2020-09-27|\n",
      "+----------+-------------+------------+------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a summary showing for each customer:\n",
    "\n",
    "Total number of transactions\n",
    "Average transaction amount\n",
    "First transaction date\n",
    "Last transaction date\n",
    "'''\n",
    "df_joined = df_transactions.join(df_customers, on = \"cust_id\", how=\"inner\")\n",
    "\n",
    "df_customer_summary = df_joined.groupby(\"cust_id\",\"name\").agg(F.count(\"txn_id\").alias(\"transactions\"), \n",
    "                                        F.avg(\"amt\").alias(\"avg_txn_amt\"),\n",
    "                                       F.min(\"date\").alias(\"first_txn_date\"),\n",
    "                                       F.max(\"date\").alias(\"last_txn_date\")\n",
    "                                       )\n",
    "df_customer_summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee81b160-5a32-4196-a0e2-0dcd07e4ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|   cust_id|distinct_expense_types|\n",
      "+----------+----------------------+\n",
      "|C1AAN0AHMK|                    12|\n",
      "|C0YDPQWPBJ|                    12|\n",
      "|CO13ZCQKBA|                    12|\n",
      "|C33RJK3Z4T|                    12|\n",
      "|CIJ881FF3R|                    12|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total number of distinct expense types\n",
    "distinct_expense_types = df_transactions.select(\"expense_type\").distinct().count()\n",
    "\n",
    "# Step 2: Group by 'cust_id' and count distinct expense types for each customer\n",
    "result = df_transactions.groupby(\"cust_id\") \\\n",
    "    .agg(F.countDistinct(\"expense_type\").alias(\"distinct_expense_types\")) \\\n",
    "    .filter(F.col(\"distinct_expense_types\") == distinct_expense_types)\n",
    "\n",
    "# Step 3: Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2058ca1-87c2-43a3-90be-103c9e2fa3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|age_group|customers|\n",
      "+---------+---------+\n",
      "|    18-25|      828|\n",
      "|    26-35|     1049|\n",
      "|      56+|     1038|\n",
      "|    46-55|     1019|\n",
      "|    36-45|     1066|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the age distribution of customers (count of customers in different age groups: 18-25, 26-35, 36-45, 46-55, 56+).\n",
    "\n",
    "#IMP: Wrap each condition in parentheses\n",
    "df_customers =  df_customers.withColumn(\n",
    "                                        \"age_group\",\n",
    "                                        F.when((F.col(\"age\") >= 18) & (F.col(\"age\") <= 25), \"18-25\")\n",
    "                                         .when((F.col(\"age\") >= 26) & (F.col(\"age\") <= 35), \"26-35\")\n",
    "                                         .when((F.col(\"age\") >= 36) & (F.col(\"age\") <= 45), \"36-45\")\n",
    "                                         .when((F.col(\"age\") >= 46) & (F.col(\"age\") <= 55), \"46-55\")\n",
    "                                         .otherwise(\"56+\")\n",
    "                                    )\n",
    "\n",
    "df_customers.groupby(\"age_group\").agg(F.countDistinct(\"cust_id\").alias(\"customers\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "795d32f1-e3d9-4b95-bf75-94535828d2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+----+\n",
      "|   cust_id|        city|         total_spent|rank|\n",
      "+----------+------------+--------------------+----+\n",
      "|C0YDPQWPBJ|      boston|1.4896172264999977E8|   1|\n",
      "|CA9UYOQ5DA|      boston|  217740.75000000006|   2|\n",
      "|CV441771EN|      boston|  214680.87999999995|   3|\n",
      "|C0YDPQWPBJ|     chicago|1.4916500168000048E8|   1|\n",
      "|CA8M8U39J2|     chicago|  212630.29000000053|   2|\n",
      "|C7REP7GP36|     chicago|   206931.9399999998|   3|\n",
      "|C0YDPQWPBJ|      denver|1.4894455308000013E8|   1|\n",
      "|CGN9VRRD9S|      denver|  225581.85000000015|   2|\n",
      "|CAAOZWFK5A|      denver|  215381.55000000005|   3|\n",
      "|C0YDPQWPBJ| los_angeles| 1.492145839800003E8|   1|\n",
      "|CA9UYOQ5DA| los_angeles|  235140.72999999975|   2|\n",
      "|CP1LHO57T6| los_angeles|  207523.43999999994|   3|\n",
      "|C0YDPQWPBJ|    new_york|1.4870774328000045E8|   1|\n",
      "|CGN9VRRD9S|    new_york|  218306.62999999986|   2|\n",
      "|C0DNME1XBG|    new_york|  211895.57999999993|   3|\n",
      "|C0YDPQWPBJ|philadelphia|1.4962093732000005E8|   1|\n",
      "|CHSW3FQ8WY|philadelphia|  226043.21999999988|   2|\n",
      "|CQYO6YFE5T|philadelphia|  219616.04999999987|   3|\n",
      "|C0YDPQWPBJ|    portland|1.4967602739000043E8|   1|\n",
      "|CA9UYOQ5DA|    portland|  242449.49000000037|   2|\n",
      "+----------+------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 3 customers who spent the most in each city.\n",
    "df_spend_per_city = df_transactions.groupby(\"cust_id\", \"city\").agg(F.sum(\"amt\").alias(\"total_spent\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(F.col(\"total_spent\").desc())\n",
    "\n",
    "df_spend_per_city.withColumn(\"rank\", F.row_number().over(window_spec))\\\n",
    "                 .filter(F.col(\"rank\").isin(1,2,3))\\\n",
    "                 .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b65b2c6f-dec4-46b0-80d9-664d7208c6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------------+\n",
      "|   cust_id|      date|    amt|     running_total|\n",
      "+----------+----------+-------+------------------+\n",
      "|C007YEYTX9|2012-02-01|  45.73|             45.73|\n",
      "|C007YEYTX9|2012-02-01|  28.89|             74.62|\n",
      "|C007YEYTX9|2012-02-02| 252.29|326.90999999999997|\n",
      "|C007YEYTX9|2012-02-02|  40.82|367.72999999999996|\n",
      "|C007YEYTX9|2012-02-03|  146.7|            514.43|\n",
      "|C007YEYTX9|2012-02-04| 3647.9|           4162.33|\n",
      "|C007YEYTX9|2012-02-05| 147.76|           4310.09|\n",
      "|C007YEYTX9|2012-02-05|  28.34|           4338.43|\n",
      "|C007YEYTX9|2012-02-05|  37.53|           4375.96|\n",
      "|C007YEYTX9|2012-02-05|  47.83|           4423.79|\n",
      "|C007YEYTX9|2012-02-06| 458.48|           4882.27|\n",
      "|C007YEYTX9|2012-02-06| 147.24|           5029.51|\n",
      "|C007YEYTX9|2012-02-06|  42.54|           5072.05|\n",
      "|C007YEYTX9|2012-02-06|  37.64|5109.6900000000005|\n",
      "|C007YEYTX9|2012-02-06|  33.77| 5143.460000000001|\n",
      "|C007YEYTX9|2012-02-07|1075.03| 6218.490000000001|\n",
      "|C007YEYTX9|2012-02-08|  573.1| 6791.590000000001|\n",
      "|C007YEYTX9|2012-02-08|  37.68| 6829.270000000001|\n",
      "|C007YEYTX9|2012-02-08|  33.56| 6862.830000000002|\n",
      "|C007YEYTX9|2012-02-08| 156.75| 7019.580000000002|\n",
      "+----------+----------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the running total of transactions for each customer ordered by date.\n",
    "# we will create a window over which the amount would be summed\n",
    "# partitionBy(\"cust_id\") -> means for each customers\n",
    "# the rowsBetween specifies the window lenght - here it is all the rows before the current row, till the current row\n",
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(F.col(\"date\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"running_total\", F.sum(\"amt\").over(window_spec))\n",
    "\n",
    "df_transactions.select(\"cust_id\",\"date\",\"amt\",\"running_total\").orderBy(\"cust_id\",\"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2d392-b45e-44d7-abe6-c56b07db6cca",
   "metadata": {},
   "source": [
    "## 3.1. Difficult Questions\n",
    "21. **Write a PySpark SQL query to find the top 5 cities with the highest total transaction amounts.**  \n",
    "    *Hint: Use `createOrReplaceTempView` to register a temporary view and write a SQL query.*\n",
    "\n",
    "22. **Create a new column `is_weekend` in the `transactions` DataFrame that indicates whether the `date` falls on a Saturday or Sunday.**  \n",
    "    *Hint: Use PySpark functions like `date_format` or `dayofweek`.*\n",
    "\n",
    "23. **Using a window function, calculate the cumulative transaction amount (`amt`) for each customer, ordered by transaction date.**  \n",
    "    *Hint: Use `Window` with `partitionBy` and `orderBy`.*\n",
    "\n",
    "24. **Write a User-Defined Function (UDF) to categorize transactions as `High` (amt > 1000), `Medium` (500 ≤ amt ≤ 1000), or `Low` (amt < 500). Add this categorization as a new column.**  \n",
    "    *Hint: Use `udf` and `withColumn`.*\n",
    "\n",
    "25. **For each `expense_type`, calculate the difference between the maximum and minimum transaction amounts.**  \n",
    "    *Hint: Use `groupBy` with `agg` and `max`/`min`.*\n",
    "\n",
    "26. **Find customers who have made transactions in more than one city.**  \n",
    "    *Hint: Use `groupBy` on `cust_id` with `countDistinct` on `city`.*\n",
    "\n",
    "27. **Using a window function, find the top transaction for each customer based on the amount (`amt`).**  \n",
    "    *Hint: Use `Window` with `rank` or `dense_rank`.*\n",
    "\n",
    "28. **Create a DataFrame showing, for each customer, the first and last transaction dates.**  \n",
    "    *Hint: Use `groupBy` with `agg` and `min`/`max`.*\n",
    "\n",
    "29. **Join `customers` and `transactions` DataFrames and find the average transaction amount for male and female customers.**  \n",
    "    *Hint: Use `join` and `groupBy` on `gender`.*\n",
    "\n",
    "30. **Using a window function, calculate the rolling average of transaction amounts for each customer over their last 3 transactions.**  \n",
    "    *Hint: Use `Window` with `rowsBetween`.*\n",
    "\n",
    "## 3.2. Advanced Operations (Difficult)\n",
    "\n",
    "21. Calculate the month-over-month percentage change in total transaction amounts for each expense type.\n",
    "\n",
    "22. Create a cohort analysis showing customer retention based on their first transaction month:\n",
    "    - Group customers by their first transaction month\n",
    "    - Show how many were still active in subsequent months\n",
    "\n",
    "23. Implement a fraud detection feature that flags suspicious transactions based on:\n",
    "    - More than 3 transactions in a single day\n",
    "    - Transactions amount > 5x the customer's average transaction amount\n",
    "    - Transactions in different cities within 24 hours\n",
    "\n",
    "24. Calculate the customer lifetime value (CLV) for each customer:\n",
    "    - Total amount spent\n",
    "    - Average transaction frequency\n",
    "    - Customer age in the system\n",
    "    - Trend of spending (increasing/decreasing)\n",
    "\n",
    "25. Create a product affinity analysis:\n",
    "    - Find which expense types are commonly seen together in the same month\n",
    "    - Calculate the correlation between different expense types\n",
    "\n",
    "26. Implement a dynamic window calculation that shows:\n",
    "    - Moving average of transaction amounts (7-day window)\n",
    "    - Transaction amount percentile within their expense type\n",
    "    - Rank of transaction amount within customer's history\n",
    "\n",
    "27. Build a customer segmentation analysis using:\n",
    "    - Recency (days since last transaction)\n",
    "    - Frequency (number of transactions)\n",
    "    - Monetary value (total amount spent)\n",
    "    - Create segments like \"High Value\", \"Medium Value\", \"Low Value\"\n",
    "\n",
    "28. Create a geographical analysis:\n",
    "    - Transaction density by city\n",
    "    - Average transaction amount by city\n",
    "    - Customer movement patterns between cities\n",
    "    - City-wise customer demographics\n",
    "\n",
    "29. Implement a recommendation system:\n",
    "    - Based on similar customers (age, gender, city)\n",
    "    - Based on transaction patterns\n",
    "    - Calculate similarity scores between customers\n",
    "\n",
    "30. Build a churn prediction feature:\n",
    "    - Define churn (e.g., no transactions in last 3 months)\n",
    "    - Calculate churn probability based on:\n",
    "        - Transaction frequency changes\n",
    "        - Amount changes\n",
    "        - Last transaction recency\n",
    "        - Customer demographics\n",
    "\n",
    "## 4.1. Bonus Challenge Questions\n",
    "\n",
    "31. Create a customer journey analysis:\n",
    "    - Map the progression of expense types over time\n",
    "    - Identify common paths/patterns in customer spending\n",
    "    - Calculate the probability of next expense type\n",
    "\n",
    "32. Implement a seasonal analysis:\n",
    "    - Detect seasonal patterns in different expense types\n",
    "    - Account for city-specific seasonal variations\n",
    "    - Create year-over-year comparison accounting for seasonality\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e71f01c-42f6-469d-a743-2cffb8742150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will work with a smaller dataset hereon out\n",
    "df_transactions = df_transactions.limit(100000)\n",
    "df_transactions = df_transactions.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3c19fb-7db5-4bc4-a61b-1f27bc66d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|        city|       total_spent|\n",
      "+------------+------------------+\n",
      "|    portland|1103259.6099999961|\n",
      "|     chicago|1059268.4400000009|\n",
      "|     seattle|1046470.0400000042|\n",
      "| los_angeles| 1039331.759999998|\n",
      "|philadelphia|1038915.2799999986|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a PySpark SQL query to find the top 5 cities with the highest total transaction amounts.\n",
    "\n",
    "# two types of views\n",
    "# 1.createOrReplaceTempView - the created view is only available to the current spark session and not accessible by other spark sessions\n",
    "# 2.createOrReplaceGlobalTempView - a global view that can be accessed by other spark applications through the global_temp database\n",
    "\n",
    "df_transactions.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "query = '''\n",
    "        SELECT \n",
    "        city,\n",
    "        SUM(amt) as total_spent\n",
    "        FROM transactions\n",
    "        GROUP BY city\n",
    "        ORDER BY total_spent DESC\n",
    "        LIMIT 5\n",
    "        '''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c6f647-d0a2-4f14-b4c8-35eaf128e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+-------+-------------+----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|       expense_type|    amt|         city|is_weekend|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+-------+-------------+----------+\n",
      "|C9S3C56LPV|2012-11-01|      NULL|TNI2YXRCY8FLQEU|2019-06-15|2019|    6| 15|Bills and Utilities|  31.56|       denver|       yes|\n",
      "|C4P7AEI6CC|2013-09-01|      NULL|TO4ARSBVH7FJ3G6|2019-06-15|2019|    6| 15|      Entertainment|   3.34|       denver|       yes|\n",
      "|C0YDPQWPBJ|2012-07-01|      NULL|TTHKIN5IJJB9EEW|2015-07-19|2015|    7| 19|      Entertainment|  12.91|       denver|       yes|\n",
      "|C4P7AEI6CC|2013-09-01|      NULL|T84K5F5Y77E9GRC|2020-08-07|2020|    8|  7|      Entertainment|   3.35|san_francisco|        no|\n",
      "|CXMWPDLHKQ|2013-09-01|      NULL|TNBK45PI6OWCYEB|2020-10-07|2020|   10|  7|      Entertainment|    4.2|     portland|        no|\n",
      "|C0YDPQWPBJ|2012-01-01|      NULL|TWHI9G401M73TVA|2016-01-11|2016|    1| 11|          Groceries| 147.09|       boston|        no|\n",
      "|C0YDPQWPBJ|2011-09-01|      NULL|T432RPFHFX93DYX|2012-01-23|2012|    1| 23|      Entertainment|    2.9|      chicago|        no|\n",
      "|C9S3C56LPV|2012-11-01|      NULL|TJHF6MZCDY1UWC4|2018-04-12|2018|    4| 12|Bills and Utilities|  55.33|  los_angeles|        no|\n",
      "|C0YDPQWPBJ|2011-03-01|2019-12-01|TYC7ZXGITUWEKI5|2011-06-10|2011|    6| 10|          Education|  52.92|  los_angeles|        no|\n",
      "|CL3B876N0W|2010-09-01|2018-06-01|T0UR5AKXCPNRZCD|2015-11-21|2015|   11| 21|      Entertainment|   3.26|       denver|       yes|\n",
      "|C0YDPQWPBJ|2012-01-01|      NULL|T4MOLT2085Z7CH7|2015-02-24|2015|    2| 24|      Entertainment|   7.88|     new_york|        no|\n",
      "|C0YDPQWPBJ|2013-02-01|      NULL|TBY499JXP6H4NHC|2020-12-13|2020|   12| 13|      Entertainment|    3.0|      chicago|       yes|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TSAOSEF02TSSJ06|2011-03-23|2011|    3| 23|      Entertainment|   24.5|  los_angeles|        no|\n",
      "|C0YDPQWPBJ|2013-02-01|      NULL|TKELR348K1GG7YI|2018-12-18|2018|   12| 18|      Entertainment|   3.57|     portland|        no|\n",
      "|C0YDPQWPBJ|2012-07-01|      NULL|T4ODONZPRGIRWFU|2020-04-11|2020|    4| 11|      Entertainment|  19.33|       denver|       yes|\n",
      "|C0YDPQWPBJ|2011-09-01|      NULL|T4EQ0WW41ALAO0O|2012-08-01|2012|    8|  1|      Entertainment|    4.2|     new_york|        no|\n",
      "|C0YDPQWPBJ|2011-09-01|      NULL|TTT029SVG5P7IG1|2020-10-09|2020|   10|  9|            Housing|3114.38|     new_york|        no|\n",
      "|CY3U68IY1F|2010-12-01|2020-01-01|TR5OJZXP3FO2OWT|2014-10-08|2014|   10|  8|       Motor/Travel|  33.19|     new_york|        no|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TKWTO1H6Y7MG84A|2013-12-02|2013|   12|  2|      Entertainment|   5.85| philadelphia|        no|\n",
      "|C0YDPQWPBJ|2011-09-01|      NULL|TVYLML36Z1ZGJKM|2020-02-24|2020|    2| 24|      Entertainment|  13.89|  los_angeles|        no|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+-------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column is_weekend in the transactions DataFrame that indicates whether the date falls on a Saturday or Sunday.\n",
    "# Lets work with date \n",
    "df_transactions.withColumn(\"is_weekend\", \n",
    "                            F.when(F.dayofweek(\"date\").isin(1, 7), \"yes\").otherwise(\"no\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2117feb1-47ce-478c-af82-579adc18e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+------+-------------+--------+\n",
      "|   cust_id|start_date|end_date|         txn_id|      date|year|month|day| expense_type|   amt|         city|cum_txns|\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+------+-------------+--------+\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TLBGW4LK9IRX4G3|2012-11-01|2012|   11|  1|Entertainment| 23.56|      chicago|       1|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T7RBVNNRJH33E96|2012-11-01|2012|   11|  1|Entertainment|337.55|san_francisco|       2|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T5K2L4VNTHJAJVL|2012-11-01|2012|   11|  1|    Education|337.55|      seattle|       3|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T00H6MUJOW1LAW6|2012-11-01|2012|   11|  1| Motor/Travel|713.27|    san_diego|       4|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TTGC9U9CUNQG0VB|2012-11-02|2012|   11|  2|Entertainment| 23.57|  los_angeles|       5|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TCZ47E6G2090WJD|2012-11-02|2012|   11|  2|Entertainment| 21.89|  los_angeles|       6|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TCWAKBEQOKCS4ON|2012-11-03|2012|   11|  3|Entertainment| 25.26| philadelphia|       7|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TOLDWBITGSXV9ZZ|2012-11-03|2012|   11|  3|Entertainment| 28.78|       denver|       8|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T2BPB3MLR7X4U8B|2012-11-03|2012|   11|  3|Entertainment| 21.65|     new_york|       9|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TOE6NBKPDEE7D7Q|2012-11-03|2012|   11|  3|    Groceries| 73.02|       boston|      10|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TPGWDYQZP3QQ61G|2012-11-03|2012|   11|  3|Entertainment| 18.94|  los_angeles|      11|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TAXNH0HHL8WGHVJ|2012-11-03|2012|   11|  3|Entertainment| 30.21|       boston|      12|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TPC55BUEQWVDW55|2012-11-03|2012|   11|  3|    Groceries| 71.24| philadelphia|      13|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TUARM90S3DSA3NI|2012-11-04|2012|   11|  4|    Groceries|105.84|       boston|      14|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T9ZJIQSBV9153G8|2012-11-04|2012|   11|  4|Entertainment| 22.39|    san_diego|      15|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TRH2D1AAXCLHPW2|2012-11-05|2012|   11|  5| Motor/Travel| 38.08|       boston|      16|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T1F43WKSG57EPCE|2012-11-05|2012|   11|  5|Entertainment| 20.76|    san_diego|      17|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TM19S33QFZ473VK|2012-11-05|2012|   11|  5|    Groceries| 72.66|  los_angeles|      18|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|T9NX1TBV4LCT0WQ|2012-11-06|2012|   11|  6|Entertainment| 34.83|      seattle|      19|\n",
      "|C00WRSJF1Q|2012-11-01|    NULL|TPO2IMQH60XN72U|2012-11-06|2012|   11|  6| Motor/Travel| 41.19|       boston|      20|\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a window function, calculate the cumulative no. of transactions for each customer, ordered by transaction date.\n",
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_transactions.withColumn(\"cum_txns\", F.count(\"txn_id\").over(window_spec)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da84b122-d2bb-466a-b78f-ddf8d7c0330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|transaction_category|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------------------+\n",
      "|CHKQQWH8EO|2012-12-01|2020-08-01|TPP59D3CVSNS0M2|2018-11-12|2018|   11| 12| Motor/Travel| 75.08| los_angeles|                 Low|\n",
      "|CJS78KKO2R|2014-03-01|      NULL|T8WENNS0QMI3WBD|2020-04-23|2020|    4| 23|    Groceries|119.66|philadelphia|                 Low|\n",
      "|CM8GEPBJW0|2010-04-01|2018-08-01|TE7Q1TL44L963Z8|2011-05-01|2011|    5|  1|          Tax|775.53|   san_diego|              Medium|\n",
      "|C0YDPQWPBJ|2010-01-01|2019-01-01|THFZS1YHQ77LARE|2016-11-27|2016|   11| 27|Entertainment|  9.41|   san_diego|                 Low|\n",
      "|C1YTOSZPBB|2013-01-01|      NULL|T0SMOQFPHBZAYBF|2013-10-03|2013|   10|  3|Entertainment| 22.46| los_angeles|                 Low|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a User-Defined Function (UDF) to categorize transactions as High (amt > 1000), Medium (500 ≤ amt ≤ 1000), or Low (amt < 500). Add this categorization as a new column.\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def categorize_transaction(amount):\n",
    "    if amount > 1000:\n",
    "        return \"High\"\n",
    "    elif amount >= 500 and amount <=1000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "categorize_transaction_udf = F.udf(categorize_transaction, T.StringType())\n",
    "\n",
    "df_transactions.withColumn(\"transaction_category\", categorize_transaction_udf(F.col(\"amt\").cast(\"int\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f17bbf92-da05-41b5-b713-a134d3c1b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|gender|avg_txn_amt|\n",
      "+------+-----------+\n",
      "|Female|     61.605|\n",
      "|  Male|    111.945|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join customers and transactions DataFrames and find the average transaction amount for male and female customers.\n",
    "df_joined = df_transactions.join(df_customers, on = \"cust_id\", how = \"inner\")\n",
    "df_joined.groupby(\"gender\").agg(F.avg(\"amt\").alias(\"avg_txn_amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9416e251-6281-465f-a503-2d094a6712ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-----------+------------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|       expense_type|   amt|       city|cum_sum_last_3_txn|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-----------+------------------+\n",
      "|C0YDPQWPBJ|2012-04-01|      NULL|TVC719DW1VM9RDJ|2016-06-26|2016|    6| 26|      Entertainment|  8.73|    chicago|              8.73|\n",
      "|C0YDPQWPBJ|2012-04-01|      NULL|TAKG4AC0L2QXEPB|2020-08-26|2020|    8| 26|          Groceries| 65.16|   new_york|             73.89|\n",
      "|C8TZQZJBU9|2011-12-01|2020-06-01|TZR6IQS57CJVWSO|2017-10-13|2017|   10| 13|          Groceries| 17.14|los_angeles|             17.14|\n",
      "|CMAF9IOXI3|2013-10-01|2020-11-01|TLZ4IY4GKQOAOUY|2016-05-13|2016|    5| 13|          Education| 26.32|    seattle|             26.32|\n",
      "|CMX7KOJDQN|2013-01-01|      NULL|TGFRORVKGUZ65GI|2016-09-20|2016|    9| 20|       Motor/Travel|197.57|     boston|            197.57|\n",
      "|CYP9JBS8CC|2012-09-01|      NULL|TMWRS6BJMBI8W8T|2014-05-08|2014|    5|  8|Bills and Utilities|155.39|los_angeles|            155.39|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a window function, calculate the rolling average of transaction amounts for each customer over their last 3 transactions.\n",
    "# The range -2 specifies the last two rows before the current row (inclusive), making the total window size three (last 3 transactions, including the current one).\n",
    "\n",
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(F.col(\"date\")).rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "df_transactions.withColumn(\"cum_sum_last_3_txn\", F.sum(\"amt\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ccc78f0-9144-4fc8-8b31-31fcd5ace3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+------------------+------------------+-------------------+\n",
      "|year|month|       expense_type|         total_amt|      previous_amt|  percentage_change|\n",
      "+----+-----+-------------------+------------------+------------------+-------------------+\n",
      "|2010|    7|Bills and Utilities|            467.38|              NULL|               NULL|\n",
      "|2010|    8|Bills and Utilities|             463.9|            467.38|-0.7445761478882319|\n",
      "|2010|    9|Bills and Utilities|             468.2|             463.9| 0.9269239060142298|\n",
      "|2010|   10|Bills and Utilities|458.91999999999996|             468.2| -1.982058949167029|\n",
      "|2010|   11|Bills and Utilities|            466.44|458.91999999999996| 1.6386298265492982|\n",
      "|2010|   12|Bills and Utilities|            464.06|            466.44|-0.5102478346625494|\n",
      "|2011|    1|Bills and Utilities|            994.41|            464.06| 114.28479075981552|\n",
      "|2011|    2|Bills and Utilities| 988.8199999999999|            994.41| -0.562142375881179|\n",
      "|2011|    3|Bills and Utilities|            466.24| 988.8199999999999| -52.84885014461681|\n",
      "|2011|    4|Bills and Utilities| 512.9300000000001|            466.24| 10.014155799588206|\n",
      "|2011|    5|Bills and Utilities| 952.4300000000001| 512.9300000000001|   85.6842064219289|\n",
      "|2011|    6|Bills and Utilities| 997.3899999999999| 952.4300000000001|  4.720556891320077|\n",
      "|2011|    7|Bills and Utilities|            463.74| 997.3899999999999|  -53.5046471290067|\n",
      "|2011|    8|Bills and Utilities|            468.59|            463.74| 1.0458446543321616|\n",
      "|2011|    9|Bills and Utilities|            475.27|            468.59|  1.425553255511216|\n",
      "|2011|   10|Bills and Utilities|466.71000000000004|            475.27|-1.8010814905211658|\n",
      "|2011|   11|Bills and Utilities|            533.25|466.71000000000004| 14.257247541299728|\n",
      "|2011|   12|Bills and Utilities| 529.6600000000001|            533.25|-0.6732301922175187|\n",
      "|2012|    1|Bills and Utilities| 492.7800000000001| 529.6600000000001| -6.962957368878146|\n",
      "|2012|    2|Bills and Utilities|480.78000000000003| 492.7800000000001|-2.4351637647631916|\n",
      "+----+-----+-------------------+------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the month-over-month percentage change in total transaction amounts for each expense type.\n",
    "# Group by year, month, and expense_type\n",
    "df_grouped = df_transactions \\\n",
    "    .withColumn(\"year\", F.year(F.col(\"date\"))) \\\n",
    "    .withColumn(\"month\", F.month(F.col(\"date\"))) \\\n",
    "    .groupby(\"year\", \"month\", \"expense_type\") \\\n",
    "    .agg(F.sum(\"amt\").alias(\"total_amt\"))\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"expense_type\").orderBy(\"year\", \"month\")\n",
    "\n",
    "df_grouped = df_grouped.withColumn(\"previous_amt\", F.lag(\"total_amt\", 1).over(window_spec))\n",
    "\n",
    "df_grouped.withColumn(\"percentage_change\", ((F.col(\"total_amt\") - F.col(\"previous_amt\"))/F.col(\"previous_amt\"))*100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d24f1a7-ccc8-4d56-92e9-5b50affd1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------------+\n",
      "|   cust_id|first_txn_date|first_txn_month|\n",
      "+----------+--------------+---------------+\n",
      "|C0YDPQWPBJ|    2010-07-01|        2010-07|\n",
      "|C4P7AEI6CC|    2013-09-01|        2013-09|\n",
      "|C9S3C56LPV|    2012-11-01|        2012-11|\n",
      "|CL3B876N0W|    2010-09-01|        2010-09|\n",
      "|CXMWPDLHKQ|    2013-09-01|        2013-09|\n",
      "|CY3U68IY1F|    2010-12-01|        2010-12|\n",
      "+----------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a cohort analysis showing customer retention based on their first transaction month:\n",
    "# Group customers by their first transaction month\n",
    "# Show how many were still active in subsequent months\n",
    "df_first_txn = df_transactions.groupby(\"cust_id\")\\\n",
    "                .agg(F.min(\"date\").alias(\"first_txn_date\"))\\\n",
    "                .withColumn(\"first_txn_month\", F.date_format(F.col(\"first_txn_date\"), \"yyyy-MM\"))\n",
    "\n",
    "df_first_txn.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48a59e9f-df71-47b4-85c0-e63534c3d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+--------------------------+\n",
      "|   cust_id|      date|first_txn_date|months_btwn_first_last_txn|\n",
      "+----------+----------+--------------+--------------------------+\n",
      "|C0YDPQWPBJ|2017-05-24|    2010-07-01|                        83|\n",
      "|C0YDPQWPBJ|2017-05-24|    2010-07-01|                        83|\n",
      "|C0YDPQWPBJ|2017-05-24|    2010-07-01|                        83|\n",
      "|C0YDPQWPBJ|2017-05-24|    2010-07-01|                        83|\n",
      "|C0YDPQWPBJ|2017-05-24|    2010-07-01|                        83|\n",
      "+----------+----------+--------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_transactions.join(df_first_txn, on = \"cust_id\", how = \"inner\")\n",
    "df_joined = df_joined.withColumn(\"txn_month\", F.date_format(F.col(\"date\"), \"yyyy-MM\"))\\\n",
    "                    .withColumn(\"months_btwn_first_last_txn\", \n",
    "                                F.round(F.months_between(F.col(\"date\"), F.col(\"first_txn_date\")),0).cast(\"int\")\n",
    "                               )\n",
    "df_joined.select(\"cust_id\",\"date\",\"first_txn_date\",\"months_btwn_first_last_txn\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13f098e7-7faa-4b28-969f-6f16e8ec5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+----------------+\n",
      "|first_txn_month|months_btwn_first_last_txn|active_customers|\n",
      "+---------------+--------------------------+----------------+\n",
      "|        2010-07|                         0|               1|\n",
      "|        2010-07|                         1|               1|\n",
      "|        2010-07|                         2|               1|\n",
      "|        2010-07|                         3|               1|\n",
      "|        2010-07|                         4|               1|\n",
      "|        2010-07|                         5|               1|\n",
      "|        2010-07|                         6|               1|\n",
      "|        2010-07|                         7|               1|\n",
      "|        2010-07|                         8|               1|\n",
      "|        2010-07|                         9|               1|\n",
      "|        2010-07|                        10|               1|\n",
      "|        2010-07|                        11|               1|\n",
      "|        2010-07|                        12|               1|\n",
      "|        2010-07|                        13|               1|\n",
      "|        2010-07|                        14|               1|\n",
      "|        2010-07|                        15|               1|\n",
      "|        2010-07|                        16|               1|\n",
      "|        2010-07|                        17|               1|\n",
      "|        2010-07|                        18|               1|\n",
      "|        2010-07|                        19|               1|\n",
      "+---------------+--------------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+---+---+---+---+\n",
      "|first_txn_month|  0|  1|  2|  3|\n",
      "+---------------+---+---+---+---+\n",
      "|        2013-09|  2|  2|  2|  2|\n",
      "|        2010-09|  1|  1|  1|  1|\n",
      "|        2010-07|  1|  1|  1|  1|\n",
      "|        2012-11|  1|  1|  1|  1|\n",
      "|        2010-12|  1|  1|  1|  1|\n",
      "+---------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_retention = df_joined.groupby(\"first_txn_month\", \"months_btwn_first_last_txn\")\\\n",
    "                        .agg(F.countDistinct(\"cust_id\").alias(\"active_customers\"))\n",
    "\n",
    "df_retention.orderBy(\"first_txn_month\",\"months_btwn_first_last_txn\").show()\n",
    "# Show the cohort analysis\n",
    "df_retention_pivot = df_retention.groupby(\"first_txn_month\").pivot(\"months_btwn_first_last_txn\").agg(F.sum(\"active_customers\"))\n",
    "df_retention_pivot.select(\"first_txn_month\", \"0\", \"1\", \"2\", \"3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eaa1932-88df-44a0-a1bd-31fb755749a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a fraud detection feature that flags suspicious transactions based on:\n",
    "    # More than 3 transactions in a single day\n",
    "    # Transactions amount > 5x the customer's average transaction amount\n",
    "    # Transactions in different cities within 24 hours\n",
    "\n",
    "#let's tackle this one first: Transactions amount > 5x the customer's average transaction amount\n",
    "df_cust_avg_amt = df_transactions.groupby(\"cust_id\").agg(F.avg(\"amt\").alias(\"avg_txn_amt\"))\n",
    "df_transactions = df_transactions.join(F.broadcast(df_cust_avg_amt), on=\"cust_id\", how=\"inner\")\n",
    "df_transactions = df_transactions.withColumn(\"is_5x_avg_amt\", F.when(F.col(\"amt\") > F.col(\"avg_txn_amt\")*5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b01d65c-c796-4c83-a82a-6ffb84511336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: More than 3 transactions in a single day\n",
    "# group by cust_id and date to see no. of transactions per day\n",
    "df_transactions = df_transactions.groupby(\"cust_id\",\"date\").agg(F.countDistinct(\"txn_id\").alias(\"txns\"))\\\n",
    "                .withColumn(\"is_more_than_3_txn_per_day\", F.when(F.col(\"txns\")>3, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f73efeb-7324-4e5a-866d-09d5e7bdd804",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `avg_txn_amt` is ambiguous, could be: [`avg_txn_amt`, `avg_txn_amt`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df_cust_avg_amt \u001b[38;5;241m=\u001b[39m df_transactions\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcust_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mavg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_txn_amt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m df_transactions \u001b[38;5;241m=\u001b[39m df_transactions\u001b[38;5;241m.\u001b[39mjoin(F\u001b[38;5;241m.\u001b[39mbroadcast(df_cust_avg_amt), on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcust_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m df_transactions \u001b[38;5;241m=\u001b[39m \u001b[43mdf_transactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_5x_avg_amt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_txn_amt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 3. Transactions in different cities within 24 hours\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Window specification to partition by cust_id and order by date\u001b[39;00m\n\u001b[1;32m     12\u001b[0m window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcust_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `avg_txn_amt` is ambiguous, could be: [`avg_txn_amt`, `avg_txn_amt`]."
     ]
    }
   ],
   "source": [
    "# 1. More than 3 transactions in a single day (you already have this one)\n",
    "df_transactions_per_day = df_transactions.groupby(\"cust_id\", \"date\").agg(F.countDistinct(\"txn_id\").alias(\"txns\")) \\\n",
    "    .withColumn(\"is_more_than_3_txn_per_day\", F.when(F.col(\"txns\") > 3, 1).otherwise(0))\n",
    "\n",
    "# 2. Transactions amount > 5x the customer's average transaction amount (you already have this one)\n",
    "df_cust_avg_amt = df_transactions.groupby(\"cust_id\").agg(F.avg(\"amt\").alias(\"avg_txn_amt\"))\n",
    "df_transactions = df_transactions.join(F.broadcast(df_cust_avg_amt), on=\"cust_id\", how=\"inner\")\n",
    "df_transactions = df_transactions.withColumn(\"is_5x_avg_amt\", F.when(F.col(\"amt\") > F.col(\"avg_txn_amt\") * 5, 1).otherwise(0))\n",
    "\n",
    "# 3. Transactions in different cities within 24 hours\n",
    "# Window specification to partition by cust_id and order by date\n",
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the time difference between the current transaction and the next transaction\n",
    "df_transactions_with_lag = df_transactions.withColumn(\"next_city\", F.lead(\"city\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"next_date\", F.lead(\"date\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"time_diff_seconds\", \n",
    "                F.unix_timestamp(F.col(\"next_date\")) - F.unix_timestamp(F.col(\"date\"))) \\\n",
    "    .withColumn(\"is_diff_city_within_24hr\", \n",
    "                F.when((F.col(\"time_diff_seconds\") <= 86400) & (F.col(\"city\") != F.col(\"next_city\")), 1).otherwise(0))\n",
    "\n",
    "# Final result combining all flags\n",
    "df_result = df_transactions_with_lag.join(df_transactions_per_day, on=\"cust_id\", how=\"left\") \\\n",
    "    .withColumn(\"fraud_flag\", \n",
    "                F.when((F.col(\"is_5x_avg_amt\") == 1) | (F.col(\"is_more_than_3_txn_per_day\") == 1) | \n",
    "                       (F.col(\"is_diff_city_within_24hr\") == 1), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c15bca9f-7fee-40f5-83fa-ee874d3100a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-------------+--------------------------+------------------------+----------+\n",
      "|   cust_id|  amt|  city|is_5x_avg_amt|is_more_than_3_txn_per_day|is_diff_city_within_24hr|fraud_flag|\n",
      "+----------+-----+------+-------------+--------------------------+------------------------+----------+\n",
      "|C0YDPQWPBJ|15.84|denver|            0|                         1|                       0|         1|\n",
      "|C0YDPQWPBJ|15.84|denver|            0|                         0|                       0|         0|\n",
      "|C0YDPQWPBJ|15.84|denver|            0|                         0|                       0|         0|\n",
      "|C0YDPQWPBJ|15.84|denver|            0|                         1|                       0|         1|\n",
      "|C0YDPQWPBJ|15.84|denver|            0|                         1|                       0|         1|\n",
      "+----------+-----+------+-------------+--------------------------+------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.select(\"cust_id\", \"amt\", \"city\", \"is_5x_avg_amt\", \n",
    "                 \"is_more_than_3_txn_per_day\", \"is_diff_city_within_24hr\", \"fraud_flag\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a976d43-de0b-437c-b969-a6d1e3e5ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+------------------+-------------+------------------+-----------------+--------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day|       expense_type|   amt|         city|       avg_txn_amt|is_5x_avg_amt|       avg_txn_amt| cumulative_spend|spending_trend|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+------------------+-------------+------------------+-----------------+--------------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TTKJ71UL9MC2FIA|2010-07-01|2010|    7|  1|      Entertainment| 17.96|       denver|122.20164913079681|            0|122.20164913079681|           849.06|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T3QJN1UXI21LUMC|2010-07-01|2010|    7|  1|      Entertainment| 15.84|       denver|122.20164913079681|            0|122.20164913079681|           849.06|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TMFZTGDFPD0MVVN|2010-07-01|2010|    7|  1|       Motor/Travel| 17.23|    san_diego|122.20164913079681|            0|122.20164913079681|           849.06|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TFKMYCIEKH6VDKY|2010-07-01|2010|    7|  1|       Motor/Travel| 19.53|     portland|122.20164913079681|            0|122.20164913079681|           849.06|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TCCEYC0LOOYFC46|2010-07-01|2010|    7|  1|          Education| 778.5| philadelphia|122.20164913079681|            1|122.20164913079681|           849.06|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TEX2I1AW51029I6|2010-07-02|2010|    7|  2|       Motor/Travel| 21.18|       boston|122.20164913079681|            0|122.20164913079681|902.7699999999999|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TP1MUZ1P7B60SSD|2010-07-02|2010|    7|  2|           Clothing| 32.53|     new_york|122.20164913079681|            0|122.20164913079681|902.7699999999999|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TBO4EJ6HDLGSLNX|2010-07-03|2010|    7|  3|          Groceries| 55.06|     portland|122.20164913079681|            0|122.20164913079681|          1241.56|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TWJ8IZU7CGDHU34|2010-07-03|2010|    7|  3|          Groceries| 55.34|    san_diego|122.20164913079681|            0|122.20164913079681|          1241.56|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TOQ9CAG3Y0B7LKH|2010-07-03|2010|    7|  3|                Tax|228.39|    san_diego|122.20164913079681|            0|122.20164913079681|          1241.56|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T77P5KOH15A5RV3|2010-07-04|2010|    7|  4|           Clothing| 32.38|       denver|122.20164913079681|            0|122.20164913079681|          1273.94|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T67Q2REI30200D2|2010-07-05|2010|    7|  5|            Savings| 26.29|       denver|122.20164913079681|            0|122.20164913079681|          1300.23|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TX2UG5L68B5642X|2010-07-06|2010|    7|  6|          Groceries| 56.48|san_francisco|122.20164913079681|            0|122.20164913079681|          1771.91|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T79V268J85VGI2I|2010-07-06|2010|    7|  6|Bills and Utilities| 415.2|  los_angeles|122.20164913079681|            0|122.20164913079681|          1771.91|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T1UI49HAYT87EXO|2010-07-07|2010|    7|  7|Bills and Utilities| 27.09|       boston|122.20164913079681|            0|122.20164913079681|           1799.0|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T9PW6AK3ZYEQM5P|2010-07-08|2010|    7|  8|       Motor/Travel| 17.63|    san_diego|122.20164913079681|            0|122.20164913079681|          1816.63|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TAL1E0DAUIQSBHE|2010-07-09|2010|    7|  9|             Health|374.45|      seattle|122.20164913079681|            0|122.20164913079681|          2465.94|    increasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|THILZ79F6DDTJZA|2010-07-09|2010|    7|  9|            Savings|182.71|       boston|122.20164913079681|            0|122.20164913079681|          2465.94|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T2IWI9IH0X44GXN|2010-07-09|2010|    7|  9|Bills and Utilities| 25.09| philadelphia|122.20164913079681|            0|122.20164913079681|          2465.94|    decreasing|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TDTTQ4QNN6PFC6A|2010-07-09|2010|    7|  9|       Motor/Travel| 20.93|     portland|122.20164913079681|            0|122.20164913079681|          2465.94|    decreasing|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------------+------+-------------+------------------+-------------+------------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculate the customer lifetime value (CLV) for each customer:\n",
    "\n",
    "Total amount spent\n",
    "Average transaction frequency\n",
    "Customer age in the system\n",
    "Trend of spending (increasing/decreasing)\n",
    "'''\n",
    "\n",
    "df_transactions_grouped_cust_id = df_transactions.groupby(\"cust_id\").agg(F.sum(\"amt\").alias(\"total_amt_spent\"),\n",
    "                                                                           F.countDistinct(\"txn_id\").alias(\"txns\"),\n",
    "                                                                           F.min(\"date\").alias(\"first_txn_date\"),\n",
    "                                                                           F.max(\"date\").alias(\"last_txn_date\")\n",
    "                                                                          )\n",
    "df_joined = df_transactions_grouped_cust_id.join(df_customers, on=\"cust_id\", how=\"inner\")\n",
    "df_joined = df_joined.withColumn(\"days_between_first_last_txn\", \n",
    "                     F.date_diff(F.col(\"last_txn_date\"),F.col(\"first_txn_date\"))\n",
    "                    )\\\n",
    "            .withColumn(\"transaction_frequency\", \n",
    "                        F.when(F.col(\"days_between_first_last_txn\") > 0, F.col(\"txns\") / F.col(\"days_between_first_last_txn\"))\n",
    "                         .otherwise(0)\n",
    "                       )\n",
    "\n",
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(\"date\")\n",
    "df_transactions.withColumn(\"cumulative_spend\", \n",
    "                          F.sum(\"amt\").over(window_spec)\n",
    "                          )\\\n",
    "                .withColumn(\"spending_trend\",\n",
    "                            F.when(F.col(\"cumulative_spend\")>F.lag(\"cumulative_spend\",1).over(window_spec),\"increasing\").otherwise(\"decreasing\")                          \n",
    "                           ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcf00d04-1d50-45d4-981d-b05e24d3af47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>Bills and Utilities</th>\n",
       "      <th>Clothing</th>\n",
       "      <th>Education</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>Fines</th>\n",
       "      <th>Gambling</th>\n",
       "      <th>Groceries</th>\n",
       "      <th>Health</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Motor/Travel</th>\n",
       "      <th>Savings</th>\n",
       "      <th>Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2013-04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2012-03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2018-07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2015-03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       month  Bills and Utilities  Clothing  Education  Entertainment  Fines  \\\n",
       "0    2020-06                  3.0       3.0        2.0           57.0    0.0   \n",
       "1    2013-05                  3.0       3.0        2.0           50.0    0.0   \n",
       "2    2019-10                  3.0       3.0        2.0           56.0    0.0   \n",
       "3    2020-12                  3.0       3.0        2.0           55.0    NaN   \n",
       "4    2018-10                  3.0       2.0        3.0           54.0    NaN   \n",
       "..       ...                  ...       ...        ...            ...    ...   \n",
       "121  2013-04                  4.0       1.0        2.0           51.0    NaN   \n",
       "122  2017-01                  3.0       4.0        2.0           56.0    0.0   \n",
       "123  2012-03                  4.0       3.0        1.0           56.0    0.0   \n",
       "124  2018-07                  4.0       3.0        3.0           47.0    NaN   \n",
       "125  2015-03                  3.0       1.0        2.0           62.0    NaN   \n",
       "\n",
       "     Gambling  Groceries  Health  Housing  Motor/Travel  Savings  Tax  \n",
       "0         4.0       12.0     4.0      1.0          12.0      1.0  1.0  \n",
       "1         5.0       18.0     2.0      1.0          11.0      2.0  2.0  \n",
       "2         5.0       13.0     1.0      1.0          12.0      1.0  1.0  \n",
       "3         5.0       14.0     3.0      1.0          12.0      1.0  1.0  \n",
       "4         4.0       18.0     3.0      1.0           9.0      1.0  1.0  \n",
       "..        ...        ...     ...      ...           ...      ...  ...  \n",
       "121       5.0       16.0     3.0      1.0          14.0      1.0  1.0  \n",
       "122       4.0       16.0     2.0      0.0          10.0      2.0  1.0  \n",
       "123       1.0       15.0     3.0      0.0          13.0      2.0  1.0  \n",
       "124       6.0       15.0     3.0      1.0          17.0      1.0  1.0  \n",
       "125       5.0       14.0     3.0      0.0           7.0      1.0  1.0  \n",
       "\n",
       "[126 rows x 13 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a product affinity analysis:\n",
    "Find which expense types are commonly seen together in the same month\n",
    "Calculate the correlation between different expense types\n",
    "'''\n",
    "\n",
    "# total amount spent for each expense_type in each month\n",
    "df_aggregated = df_transactions.withColumn(\"month\", F.date_format(F.col(\"date\"), \"yyyy-MM\"))\\\n",
    "                                .groupBy(\"expense_type\", \"month\")\\\n",
    "                                .agg(F.sum(\"amt\").alias(\"total_amt\"),\n",
    "                                     F.countDistinct(\"txn_id\").alias(\"txns\")\n",
    "                                    )\n",
    "\n",
    "df_monthly_totals = df_aggregated.groupby(\"month\") \\\n",
    "                              .agg(F.sum(\"total_amt\").alias(\"monthly_total_amt\"),\n",
    "                                   F.sum(\"txns\").alias(\"monthly_total_txns\")\n",
    "                                  )\n",
    "\n",
    "df_joined = df_aggregated.join(df_monthly_totals, on=\"month\", how=\"inner\")\n",
    "\n",
    "\n",
    "df_percentages = df_joined.withColumn(\"percentage\",\n",
    "                                      F.round((F.col(\"txns\") / F.col(\"monthly_total_txns\")) * 100, 0)\n",
    "                                     )\n",
    "\n",
    "\n",
    "# Once we have the aggregated data, we need to pivot it so that each expense_type becomes a column.\n",
    "# df.pivot(pivot_col, [list_of_values])\n",
    "\n",
    "df_pivot = df_percentages.groupBy(\"month\").pivot(\"expense_type\").agg(F.first(\"percentage\"))\n",
    "df_pivot.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae24076c-a5c9-4a46-9277-24958194e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement a dynamic window calculation that shows:\n",
    "\n",
    "Moving average of transaction amounts (7-day window)\n",
    "Transaction amount percentile within their expense type\n",
    "Rank of transaction amount within customer's history\n",
    "'''\n",
    "\n",
    "df_transactions_selected = df_transactions.select(\"cust_id\",\"txn_id\",\"date\",\"amt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b63f6bba-e800-4f4d-b196-ac91c5bc9d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+------+------------------+\n",
      "|   cust_id|         txn_id|      date|   amt|  7_day_moving_avg|\n",
      "+----------+---------------+----------+------+------------------+\n",
      "|C0YDPQWPBJ|TFKMYCIEKH6VDKY|2010-07-01| 19.53|102.10035290000053|\n",
      "|C0YDPQWPBJ|TMFZTGDFPD0MVVN|2010-07-01| 17.23|102.10035290000053|\n",
      "|C0YDPQWPBJ|TTKJ71UL9MC2FIA|2010-07-01| 17.96|102.10035290000053|\n",
      "|C0YDPQWPBJ|T3QJN1UXI21LUMC|2010-07-01| 15.84|102.10035290000053|\n",
      "|C0YDPQWPBJ|TCCEYC0LOOYFC46|2010-07-01| 778.5|102.10035290000053|\n",
      "|C0YDPQWPBJ|TP1MUZ1P7B60SSD|2010-07-02| 32.53|102.10035290000053|\n",
      "|C0YDPQWPBJ|TEX2I1AW51029I6|2010-07-02| 21.18|102.10035290000053|\n",
      "|C0YDPQWPBJ|TBO4EJ6HDLGSLNX|2010-07-03| 55.06|102.10035290000053|\n",
      "|C0YDPQWPBJ|TWJ8IZU7CGDHU34|2010-07-03| 55.34|102.10035290000053|\n",
      "|C0YDPQWPBJ|TOQ9CAG3Y0B7LKH|2010-07-03|228.39|102.10035290000053|\n",
      "|C0YDPQWPBJ|T77P5KOH15A5RV3|2010-07-04| 32.38|102.10035290000053|\n",
      "|C0YDPQWPBJ|T67Q2REI30200D2|2010-07-05| 26.29|102.10035290000053|\n",
      "|C0YDPQWPBJ|T79V268J85VGI2I|2010-07-06| 415.2|102.10035290000053|\n",
      "|C0YDPQWPBJ|TX2UG5L68B5642X|2010-07-06| 56.48|102.10035290000053|\n",
      "|C0YDPQWPBJ|T1UI49HAYT87EXO|2010-07-07| 27.09|102.10035290000053|\n",
      "|C0YDPQWPBJ|T9PW6AK3ZYEQM5P|2010-07-08| 17.63|102.10035290000053|\n",
      "|C0YDPQWPBJ|THILZ79F6DDTJZA|2010-07-09|182.71|102.10035290000053|\n",
      "|C0YDPQWPBJ|TDTTQ4QNN6PFC6A|2010-07-09| 20.93|102.10035290000053|\n",
      "|C0YDPQWPBJ|TRY80JWK0AYENM5|2010-07-09| 46.13|102.10035290000053|\n",
      "|C0YDPQWPBJ|T2IWI9IH0X44GXN|2010-07-09| 25.09|102.10035290000053|\n",
      "+----------+---------------+----------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving average of transaction amounts (7-day window)\n",
    "# Convert dates to timestamps for window calculation\n",
    "window_spec_7_avg = (Window\n",
    "    .orderBy(F.unix_timestamp(\"date\"))\n",
    "    .rangeBetween(-7 * 24 * 60 * 60, 0)  # 7 days in seconds\n",
    ")\n",
    "\n",
    "# Calculate 7-day moving average\n",
    "df_with_moving_avg = (df_transactions_selected.withColumn(\"7_day_moving_avg\", \n",
    "                                                F.avg(\"amt\").over(window_spec_7_avg)\n",
    "                                                )\n",
    "                                        )\n",
    "df_with_moving_avg.orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "208174cc-95fd-4768-aba7-347c7b706a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+--------------------+\n",
      "| expense_type| amt| percentile_rank_amt|\n",
      "+-------------+----+--------------------+\n",
      "|Entertainment|1.69|                 0.0|\n",
      "|Entertainment| 1.7|1.853430700226118...|\n",
      "|Entertainment|1.71|3.706861400452237E-5|\n",
      "|Entertainment|1.71|3.706861400452237E-5|\n",
      "|Entertainment|1.72|7.413722800904474E-5|\n",
      "|Entertainment|1.72|7.413722800904474E-5|\n",
      "|Entertainment|1.72|7.413722800904474E-5|\n",
      "|Entertainment|1.73|1.297401490158283E-4|\n",
      "|Entertainment|1.73|1.297401490158283E-4|\n",
      "|Entertainment|1.73|1.297401490158283E-4|\n",
      "|Entertainment|1.73|1.297401490158283E-4|\n",
      "|Entertainment|1.75|2.038773770248730...|\n",
      "|Entertainment|1.75|2.038773770248730...|\n",
      "|Entertainment|1.77|2.409459910293954E-4|\n",
      "|Entertainment|1.78|2.594802980316566E-4|\n",
      "|Entertainment|1.78|2.594802980316566E-4|\n",
      "|Entertainment|1.78|2.594802980316566E-4|\n",
      "|Entertainment|1.79|3.150832190384401...|\n",
      "|Entertainment|1.79|3.150832190384401...|\n",
      "|Entertainment|1.79|3.150832190384401...|\n",
      "+-------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transaction amount percentile within their expense type\n",
    "\n",
    "# order By amount as we are ranking based on transaction size\n",
    "window_spec = Window.partitionBy(\"expense_type\").orderBy(\"amt\")\n",
    "\n",
    "# The percent_rank func will calculate the relative position of each transaction amount (0 to 1) within its expense type\n",
    "# 0 means lowest amount in that expense type\n",
    "# 1 means highest amount in that expense type\n",
    "\n",
    "df_transactions_wt_rank = df_transactions.withColumn(\"percentile_rank_amt\",\n",
    "                          F.percent_rank().over(window_spec)\n",
    "                          )\n",
    "\n",
    "df_transactions_wt_rank.filter(F.col(\"expense_type\")=='Entertainment')\\\n",
    "                        .select(\"expense_type\", \"amt\", \"percentile_rank_amt\")\\\n",
    "                        .orderBy(\"amt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c61f34a-b22c-4487-b678-ff78f6044ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+------+-----------------+\n",
      "|   cust_id|         txn_id|      date|   amt|txn_rank_customer|\n",
      "+----------+---------------+----------+------+-----------------+\n",
      "|C0YDPQWPBJ|T60WDKTEXEWVU9F|2014-04-10|998.87|                1|\n",
      "|C0YDPQWPBJ|TPBGFXSHKAFSQEK|2014-03-10|997.22|                2|\n",
      "|C0YDPQWPBJ|TE0VSZ1C4CJUHE4|2014-02-10|995.58|                3|\n",
      "|C0YDPQWPBJ|T3R5NS38YAMUCA4|2020-12-05|995.17|                4|\n",
      "|C0YDPQWPBJ|T8EUZWQK3AB1A0U|2014-01-10|993.94|                5|\n",
      "|C0YDPQWPBJ|TBP6330UW8QDLI2|2020-11-05|993.53|                6|\n",
      "|C0YDPQWPBJ|TL6S9N4XSXJ52DX|2013-12-10| 992.3|                7|\n",
      "|C0YDPQWPBJ|T8YU3FTQUXYEN4Z|2020-10-05|991.89|                8|\n",
      "|C0YDPQWPBJ|TJ2RD2RQNNLFIA4|2013-11-10|990.66|                9|\n",
      "|C0YDPQWPBJ|TEBW5PBVDIVMD2P|2020-09-05|990.26|               10|\n",
      "|C0YDPQWPBJ|TQ3C5WZVRIC3HLZ|2016-10-27| 99.99|               11|\n",
      "|C0YDPQWPBJ|T73JKOC2MA79RM1|2020-10-25| 99.97|               12|\n",
      "|C0YDPQWPBJ|TRGL8DST6VWDMQR|2016-05-03| 99.96|               13|\n",
      "|C0YDPQWPBJ|TY1UPNTJQGY6OMP|2012-08-27| 99.95|               14|\n",
      "|C0YDPQWPBJ|TXJ8ICI8JQQQEJD|2016-11-06| 99.95|               14|\n",
      "|C0YDPQWPBJ|TUC937QBLP6A2FO|2015-01-05| 99.95|               14|\n",
      "|C0YDPQWPBJ|TR2DE3SU65ZUC59|2020-08-18| 99.94|               15|\n",
      "|C0YDPQWPBJ|TGQ242AIU7HRWZE|2019-04-27| 99.94|               15|\n",
      "|C0YDPQWPBJ|TBYCO94ES9TRZ18|2012-09-16| 99.93|               16|\n",
      "|C0YDPQWPBJ|TIES2FYQM2ILOB7|2016-11-17| 99.93|               16|\n",
      "+----------+---------------+----------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank of transaction amount within customer's history\n",
    "'''\n",
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
    "sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
    "and had three people tie for second place, you would say that all three were in second\n",
    "place and that the next person came in third. Rank would give me sequential numbers, making\n",
    "the person that came in third place (after the ties) would register as coming in fifth.\n",
    "'''\n",
    "window_spec_cust = Window.partitionBy(\"cust_id\").orderBy(F.col(\"amt\").desc())\n",
    "\n",
    "df_transactions_selected = df_transactions_selected.withColumn(\"txn_rank_customer\",\n",
    "                                   F.dense_rank().over(window_spec_cust)\n",
    "                                   )\n",
    "\n",
    "df_transactions_selected.filter(F.col(\"cust_id\")=='C0YDPQWPBJ').orderBy(\"amt\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f64112d-6b49-4609-9231-ae39a17bb194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------------+------------+\n",
      "|   cust_id|         txn_id|last_transaction_date|recency_days|\n",
      "+----------+---------------+---------------------+------------+\n",
      "|C0YDPQWPBJ|TFKMYCIEKH6VDKY|                 NULL|        NULL|\n",
      "|C0YDPQWPBJ|TMFZTGDFPD0MVVN|           2010-07-01|        3832|\n",
      "|C0YDPQWPBJ|TTKJ71UL9MC2FIA|           2010-07-01|        3832|\n",
      "|C0YDPQWPBJ|T3QJN1UXI21LUMC|           2010-07-01|        3832|\n",
      "|C0YDPQWPBJ|TCCEYC0LOOYFC46|           2010-07-01|        3832|\n",
      "|C0YDPQWPBJ|TP1MUZ1P7B60SSD|           2010-07-01|        3832|\n",
      "|C0YDPQWPBJ|TEX2I1AW51029I6|           2010-07-02|        3831|\n",
      "|C0YDPQWPBJ|TOQ9CAG3Y0B7LKH|           2010-07-02|        3831|\n",
      "|C0YDPQWPBJ|TWJ8IZU7CGDHU34|           2010-07-03|        3830|\n",
      "|C0YDPQWPBJ|TBO4EJ6HDLGSLNX|           2010-07-03|        3830|\n",
      "|C0YDPQWPBJ|T77P5KOH15A5RV3|           2010-07-03|        3830|\n",
      "|C0YDPQWPBJ|T67Q2REI30200D2|           2010-07-04|        3829|\n",
      "|C0YDPQWPBJ|T79V268J85VGI2I|           2010-07-05|        3828|\n",
      "|C0YDPQWPBJ|TX2UG5L68B5642X|           2010-07-06|        3827|\n",
      "|C0YDPQWPBJ|T1UI49HAYT87EXO|           2010-07-06|        3827|\n",
      "|C0YDPQWPBJ|T9PW6AK3ZYEQM5P|           2010-07-07|        3826|\n",
      "|C0YDPQWPBJ|T2IWI9IH0X44GXN|           2010-07-08|        3825|\n",
      "|C0YDPQWPBJ|THILZ79F6DDTJZA|           2010-07-09|        3824|\n",
      "|C0YDPQWPBJ|TAL1E0DAUIQSBHE|           2010-07-09|        3824|\n",
      "|C0YDPQWPBJ|TDTTQ4QNN6PFC6A|           2010-07-09|        3824|\n",
      "+----------+---------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Build a customer segmentation analysis using:\n",
    "\n",
    "Recency (days since last transaction)\n",
    "Frequency (number of transactions)\n",
    "Monetary value (total amount spent)\n",
    "Create segments like \"High Value\", \"Medium Value\", \"Low Value\"\n",
    "'''\n",
    "df_transactions_grouped =  df_transactions.groupby(\"cust_id\")\\\n",
    "                                            .agg(F.countDistinct(\"txn_id\").alias(\"txns\"),\n",
    "                                                 F.round(F.sum(\"amt\"),0).alias(\"total_amt_spent\")\n",
    "                                                )\n",
    "df_transactions_joined = df_transactions.join(df_transactions_grouped, on=\"cust_id\", how=\"inner\")\n",
    "\n",
    "window_spec_cust = Window.partitionBy(\"cust_id\").orderBy(\"date\")\n",
    "\n",
    "df_transactions_joined = df_transactions_joined.withColumn(\"last_transaction_date\", \n",
    "                                                           F.lag(\"date\").over(window_spec_cust))\n",
    "\n",
    "# Get the latest transaction date in the dataset (reference date)\n",
    "max_date = df_transactions.agg(F.max(\"date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# Calculate Recency\n",
    "df_transactions_joined = df_transactions_joined.withColumn(\n",
    "    \"recency_days\",\n",
    "    F.when(F.col(\"last_transaction_date\").isNotNull(), \n",
    "           F.datediff(F.lit(max_date), F.col(\"last_transaction_date\"))\n",
    "    ).otherwise(None)  # Handle nulls for first transaction\n",
    ")\n",
    "\n",
    "df_transactions_joined.select(\"cust_id\", \"txn_id\", \"last_transaction_date\", \"recency_days\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f415f353-faf4-45d5-8852-55351143978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+--------------+---------+------------+\n",
      "|   cust_id|recency_score|frequency_score|monetary_score|rfm_score|     segment|\n",
      "+----------+-------------+---------------+--------------+---------+------------+\n",
      "|C4P7AEI6CC|            1|              5|             5|       11|Medium Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "|C4P7AEI6CC|            5|              5|             5|       15|  High Value|\n",
      "+----------+-------------+---------------+--------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Step 1: Calculate RFM Metrics\n",
    "rfm_df = df_transactions_grouped.select(\"cust_id\", \"total_amt_spent\", \"txns\").join(\n",
    "    df_transactions_joined.select(\"cust_id\", \"recency_days\").distinct(),\n",
    "    on=\"cust_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 2: Assign Scores for Each Metric\n",
    "# Recency: Lower days = Higher score\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"recency_score\",\n",
    "    when(F.col(\"recency_days\") <= 30, 5)\n",
    "    .when((F.col(\"recency_days\") > 30) & (F.col(\"recency_days\") <= 60), 4)\n",
    "    .when((F.col(\"recency_days\") > 60) & (F.col(\"recency_days\") <= 90), 3)\n",
    "    .when((F.col(\"recency_days\") > 90) & (F.col(\"recency_days\") <= 120), 2)\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "# Frequency: Higher txns = Higher score\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"frequency_score\",\n",
    "    when(F.col(\"txns\") >= 20, 5)\n",
    "    .when((F.col(\"txns\") >= 15) & (F.col(\"txns\") < 20), 4)\n",
    "    .when((F.col(\"txns\") >= 10) & (F.col(\"txns\") < 15), 3)\n",
    "    .when((F.col(\"txns\") >= 5) & (F.col(\"txns\") < 10), 2)\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "# Monetary Value: Higher total amount spent = Higher score\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"monetary_score\",\n",
    "    when(F.col(\"total_amt_spent\") >= 1000, 5)\n",
    "    .when((F.col(\"total_amt_spent\") >= 750) & (F.col(\"total_amt_spent\") < 1000), 4)\n",
    "    .when((F.col(\"total_amt_spent\") >= 500) & (F.col(\"total_amt_spent\") < 750), 3)\n",
    "    .when((F.col(\"total_amt_spent\") >= 250) & (F.col(\"total_amt_spent\") < 500), 2)\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "# Step 3: Create a Combined RFM Score\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"rfm_score\",\n",
    "    F.col(\"recency_score\") + F.col(\"frequency_score\") + F.col(\"monetary_score\")\n",
    ")\n",
    "\n",
    "# Step 4: Segment Customers\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"segment\",\n",
    "    when(F.col(\"rfm_score\") >= 13, \"High Value\")\n",
    "    .when((F.col(\"rfm_score\") >= 9) & (F.col(\"rfm_score\") < 13), \"Medium Value\")\n",
    "    .otherwise(\"Low Value\")\n",
    ")\n",
    "\n",
    "# Show Final Segments\n",
    "rfm_df.groupby(\"cust_id\", \"recency_score\", \"frequency_score\", \"monetary_score\", \"rfm_score\", \"segment\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dbf9491-270d-411a-9061-e9035d5dfc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+-----+\n",
      "|         city|         total_amt| txns|\n",
      "+-------------+------------------+-----+\n",
      "|    san_diego| 968898.9500000017| 9864|\n",
      "|      chicago|        1059268.44|10141|\n",
      "|       denver|1023027.3099999998| 9924|\n",
      "|       boston| 984069.3899999992| 9800|\n",
      "|      seattle|1046470.0400000014|10010|\n",
      "|  los_angeles|1039331.7599999973|10061|\n",
      "|     new_york| 993799.5100000021| 9959|\n",
      "|san_francisco| 952994.9999999984|10135|\n",
      "| philadelphia|1038915.2800000006|10008|\n",
      "|     portland|        1103259.61|10098|\n",
      "+-------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a geographical analysis:\n",
    "\n",
    "Transaction density by city\n",
    "Average transaction amount by city\n",
    "Customer movement patterns between cities\n",
    "City-wise customer demographics\n",
    "'''\n",
    "# Transaction density by city\n",
    "# Average transaction amount by city\n",
    "\n",
    "df_transactions.groupby(\"city\").agg(F.sum(\"amt\").alias(\"total_amt\"),\n",
    "                                    F.count_distinct(\"txn_id\").alias(\"txns\")\n",
    "                                   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c55ee6c-67db-48e5-8752-e91b40af8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+----+\n",
      "|cust_city|     txn_city|      avg_txn_amt|txns|\n",
      "+---------+-------------+-----------------+----+\n",
      "| new_york|      seattle|75.23041966426867| 834|\n",
      "| new_york|       denver|84.91699266503655| 818|\n",
      "| new_york|  los_angeles|84.66374233128838| 815|\n",
      "| new_york|san_francisco|80.04725925925923| 810|\n",
      "| new_york|     portland|80.14797011207962| 803|\n",
      "| new_york| philadelphia|82.38393483709275| 798|\n",
      "| new_york|    san_diego|79.36619949494947| 792|\n",
      "| new_york|       boston|81.94199488491047| 782|\n",
      "| new_york|     new_york|83.34528074866307| 748|\n",
      "| new_york|      chicago|73.33599190283394| 741|\n",
      "+---------+-------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customer movement patterns between cities\n",
    "# What do you want to achieve with this\n",
    "# maybe answer questions like customers from so and so on avg make the highest txn in this city\n",
    "df_transactions_select = df_transactions.select(F.col(\"cust_id\"),F.col(\"city\").alias(\"txn_city\"),\"txn_id\",\"amt\",\"date\")\n",
    "df_customers_select = df_customers.select(\"cust_id\", F.col(\"city\").alias(\"cust_city\"))\n",
    "\n",
    "df_txn_cust = df_transactions_select.join(df_customers_select, on=\"cust_id\",how=\"inner\")\n",
    "df_grouped = df_txn_cust.groupby(\"cust_city\",\"txn_city\").agg(F.avg(\"amt\").alias(\"avg_txn_amt\"),\n",
    "                                               F.count_distinct(\"txn_id\").alias(\"txns\")\n",
    "                                               )\n",
    "df_grouped.filter(F.col(\"cust_city\")=='new_york').orderBy(\"txns\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d2705c-eea8-4273-98cc-e291b1b89501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+--------------+\n",
      "|    prev_city|         city|    next_city|movement_count|\n",
      "+-------------+-------------+-------------+--------------+\n",
      "|san_francisco|  los_angeles|       boston|           141|\n",
      "| philadelphia|      chicago|       denver|           134|\n",
      "|     new_york|      chicago|     portland|           130|\n",
      "|san_francisco|    san_diego|     portland|           129|\n",
      "|      chicago|       denver| philadelphia|           128|\n",
      "| philadelphia|      chicago|     portland|           127|\n",
      "|     new_york|san_francisco|     portland|           126|\n",
      "|       denver|       denver|     portland|           126|\n",
      "|       boston|       denver| philadelphia|           125|\n",
      "|       boston|      chicago|      chicago|           125|\n",
      "|     portland|     new_york|san_francisco|           125|\n",
      "|  los_angeles|      chicago|       denver|           124|\n",
      "|      chicago|     portland|       denver|           124|\n",
      "|     portland|    san_diego|     portland|           123|\n",
      "|    san_diego|     new_york|san_francisco|           122|\n",
      "|     new_york|san_francisco|     new_york|           122|\n",
      "|      chicago|     portland|    san_diego|           121|\n",
      "|      chicago|     new_york|san_francisco|           121|\n",
      "|     portland|     portland| philadelphia|           121|\n",
      "|       boston| philadelphia|      seattle|           121|\n",
      "+-------------+-------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way of thinking about it\n",
    "# Define a window to order transactions by date for each customer\n",
    "window_spec_movement = Window.partitionBy(\"cust_id\").orderBy(\"date\")\n",
    "\n",
    "# Add a lagged column to capture the previous city\n",
    "customer_movement = df_transactions \\\n",
    "    .withColumn(\"prev_city\", F.lag(\"city\").over(window_spec_movement)) \\\n",
    "    .withColumn(\"next_city\", F.lead(\"city\").over(window_spec_movement)) \\\n",
    "    .filter((F.col(\"city\") != F.col(\"prev_city\"))|(F.col(\"city\") != F.col(\"next_city\")))  \n",
    "    # Filter only if the customer moved to a different city\n",
    "\n",
    "# Count movements between city pairs\n",
    "movement_patterns = customer_movement \\\n",
    "    .groupBy(\"prev_city\", \"city\", \"next_city\") \\\n",
    "    .agg(F.count(\"txn_id\").alias(\"movement_count\")) \\\n",
    "    .orderBy(F.desc(\"movement_count\"))\n",
    "\n",
    "movement_patterns.orderBy(\"movement_count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
