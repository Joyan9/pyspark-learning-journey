{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp9HalcfubKERqQ2TjpZJj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "6d0df64a-7dd4-4993-e3f7-9aa9488738af"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 22.2 MB in 8s (2,950 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c91a164edd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://82bc90a4e2f1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "b42873e4-b78b-4be1-f4e0-c683d8bc1eb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# Reading Data\n",
        "\n",
        "For this example, I am going to use a data set from this [github repo](https://github.com/afaqueahmad7117/spark-experiments.git)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/afaqueahmad7117/spark-experiments.git\n",
        "\n",
        "# Load datasets from the cloned repo\n",
        "transactions_df = spark.read.parquet(\"spark-experiments/data/data_skew/transactions.parquet\")\n",
        "customers_df = spark.read.parquet(\"spark-experiments/data/data_skew/customers.parquet\")\n",
        "\n",
        "print(\"Transactions Dataset Schema:\")\n",
        "transactions_df.printSchema()\n",
        "print(\"Customers Dataset Schema:\")\n",
        "customers_df.printSchema()"
      ],
      "metadata": {
        "id": "NfXVooOy8kJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "3a63b99e-6b18-4311-d3f1-56e688ce2cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spark-experiments'...\n",
            "remote: Enumerating objects: 544, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 544 (delta 7), reused 0 (delta 0), pack-reused 536 (from 1)\u001b[K\n",
            "Receiving objects: 100% (544/544), 702.60 MiB | 16.88 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Updating files: 100% (351/351), done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6f1c4fd3dec6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load datasets from the cloned repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtransactions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark-experiments/data/data_skew/transactions.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcustomers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark-experiments/data/data_skew/customers.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 1 - 2025/04/14**"
      ],
      "metadata": {
        "id": "hwEA0-uvHAAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Schema Validation & Type Conversion (Easy)**\n",
        "\n",
        "Your ETL pipeline ingests raw data with all columns as strings. Convert the `amt` (transaction amount) to DoubleType and `age` to IntegerType. Validate by showing the schema post-conversion."
      ],
      "metadata": {
        "id": "L4TMklh0G1e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBNVmWuIGYm",
        "outputId": "2cba1d10-200c-4be4-adba-017698e302bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df = transactions_df.withColumn(\"amt\", F.col(\"amt\").cast(DoubleType()))\n",
        "transactions_df.printSchema()\n",
        "\n",
        "transactions_df.show(3) # this triggers the transformation"
      ],
      "metadata": {
        "id": "JgYb9Af68h3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ca042c-6c35-44ba-c98d-ae403681d362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- expense_type: string (nullable = true)\n",
            " |-- amt: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|    city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment|10.42|  boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel|44.34|portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment| 3.18| chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Time-Based Aggregations (Medium)**\n",
        "Scenario: The business wants monthly expense reports. Calculate total monthly expenses per customer, preserving the original schema's year and month columns. Handle potential nulls in amt.\n",
        "\n",
        "```\n",
        "# Expected output schema\n",
        "# |-- cust_id: string\n",
        "# |-- year: string\n",
        "# |-- month: string\n",
        "# |-- total_expense: double\n",
        "```"
      ],
      "metadata": {
        "id": "fMaPtAdxJbqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_transactions_per_customer = transactions_df.groupBy(\"cust_id\",\"year\",\"month\").agg(F.sum(\"amt\").alias(\"total_expense\"))\\\n",
        "                                                    .orderBy(\"year\",\"month\",\"total_expense\")\n",
        "monthly_transactions_per_customer.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTaX9tlJKae",
        "outputId": "c45924e9-5887-4d49-ed4a-8868bce60267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+------------------+\n",
            "|   cust_id|year|month|     total_expense|\n",
            "+----------+----+-----+------------------+\n",
            "|C42POJ8QKI|2010|    1|298.94000000000005|\n",
            "|CC5E1YOY7N|2010|    1|302.53000000000003|\n",
            "|CCQ557SM5V|2010|    1|332.96000000000004|\n",
            "|CLESRZVUWU|2010|    1|334.14000000000004|\n",
            "|COV6YRAYE9|2010|    1|362.21999999999997|\n",
            "+----------+----+-----+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Quality Check (Medium)**\n",
        "Scenario: Your team needs to validate customer data. Find:\n",
        "\n",
        "1. Customers with invalid ZIP codes (non-5-digit format)\n",
        "2. Transactions with future dates (dates beyond today's date)\n",
        "\n",
        "Return counts for both anomalies."
      ],
      "metadata": {
        "id": "-YbDsg9OK3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.select(\"cust_id\", \"zip\")\\\n",
        "            .filter(F.length(F.col(\"zip\")) != 5)\\\n",
        "            .show()"
      ],
      "metadata": {
        "id": "C6GtBQQlKJCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1552d4c1-4d77-4bc1-b0eb-6f3e2377dacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|cust_id|zip|\n",
            "+-------+---+\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*There are no records with zip codes with not equal to 5 digits*"
      ],
      "metadata": {
        "id": "-eJZ-9l2l2z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Customer-Transaction Enrichment (Hard)**\n",
        "\n",
        "Scenario: Create a master dataset showing each transaction enriched with customer demographics. Optimize for:\n",
        "\n",
        "Fast joins using broadcast join where appropriate\n",
        "\n",
        "Handling null values in customer data\n",
        "\n",
        "Preserving original transaction order*\n",
        "\n",
        "```\n",
        "# Target schema\n",
        "# |-- txn_id: string\n",
        "# |-- date: string\n",
        "# |-- expense_type: string\n",
        "# |-- amt: double\n",
        "# |-- city: string\n",
        "# |-- name: string\n",
        "# |-- age: int\n",
        "# |-- gender: string\n",
        "```"
      ],
      "metadata": {
        "id": "eUCBlAFRmFYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df_for_join = transactions_df.select(\"txn_id\", \"cust_id\", \"date\", \"expense_type\", \"amt\", F.col(\"city\").alias(\"txn_city\"))\n",
        "\n",
        "customers_df_for_join = customers_df.select(\"cust_id\", \"name\", \"age\", \"gender\")\n",
        "\n",
        "master_df = transactions_df_for_join.join(F.broadcast(customers_df_for_join),\n",
        "                              customers_df_for_join.cust_id == transactions_df_for_join.cust_id,\n",
        "                              how='inner')\n",
        "\n",
        "master_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGeEEjFrlm7P",
        "outputId": "a3ddaa88-b893-46c2-b1bd-4a8dec214fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|         txn_id|   cust_id|      date| expense_type|   amt|   txn_city|   cust_id|    name|age|gender|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|TZ5SMKZY9S03OQJ|C0YDPQWPBJ|2018-10-07|Entertainment| 10.42|     boston|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYIAPPNU066CJ5R|C0YDPQWPBJ|2016-03-27| Motor/Travel| 44.34|   portland|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TETSXIK4BLXHJ6W|C0YDPQWPBJ|2011-04-11|Entertainment|  3.18|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TQKL1QFJY3EM8LO|C0YDPQWPBJ|2018-02-22|    Groceries|268.97|los_angeles|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYL6DFP09PPXMVB|C0YDPQWPBJ|2010-10-16|Entertainment|  2.66|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Window Function Analysis (Hard)**\n",
        "\n",
        "Scenario: For customer retention analysis, calculate:\n",
        "\n",
        "a) Days since previous transaction per customer\n",
        "\n",
        "b) Rolling 30-day average spend per customer\n",
        "\n",
        "Use appropriate window functions and handle partition boundaries."
      ],
      "metadata": {
        "id": "c2zTCFvQnzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Days since previous transaction per customer\n",
        "# I'm assuming we are comparing it to today's date\n",
        "\n",
        "# group by cust_id, get last transaction date, check the difference between today's date and last transaction date\n",
        "transactions_df.groupBy(\"cust_id\").agg(F.max(\"date\").alias(\"last_transaction_date\"))\\\n",
        "                .withColumn(\"days_since_last_txn\", F.date_diff(F.current_date(), F.col(\"last_transaction_date\")))\\\n",
        "                .show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8UwdQykmWlA",
        "outputId": "24b30189-85be-4502-c765-87d27db871be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+-------------------+\n",
            "|   cust_id|last_transaction_date|days_since_last_txn|\n",
            "+----------+---------------------+-------------------+\n",
            "|C007YEYTX9|           2020-09-27|               1662|\n",
            "|C00B971T1J|           2020-12-27|               1571|\n",
            "|C00WRSJF1Q|           2020-12-27|               1571|\n",
            "|C01AZWQMF3|           2019-03-27|               2212|\n",
            "|C01BKUFRHA|           2020-09-27|               1662|\n",
            "+----------+---------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'date' is in timestamp format\n",
        "transactions_df = transactions_df.withColumn(\"date_ts\", F.col(\"date\").cast(\"timestamp\"))\n",
        "\n",
        "# Create a time-based window: last 30 days per customer\n",
        "window_30_days = Window.partitionBy(\"cust_id\")\\\n",
        "                       .orderBy(F.col(\"date_ts\").cast(\"long\"))\\\n",
        "                       .rangeBetween(-30 * 86400, 0)  # last 30 days in seconds\n",
        "\n",
        "# First aggregate daily total per customer\n",
        "daily_total_df = transactions_df.groupBy(\"cust_id\", \"date_ts\").agg(F.sum(\"amt\").alias(\"daily_total\"))\n",
        "\n",
        "# Apply rolling average over the 30-day window\n",
        "result_df = daily_total_df.withColumn(\"rolling_30_days_avg\", F.avg(\"daily_total\").over(window_30_days))\n",
        "\n",
        "result_df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_47U4Io6Ne",
        "outputId": "3cd50e4e-4ea3-4867-fb14-97022b949e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-----------+-------------------+\n",
            "|   cust_id|            date_ts|daily_total|rolling_30_days_avg|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "|C007YEYTX9|2012-02-01 00:00:00|      74.62|              74.62|\n",
            "|C007YEYTX9|2012-02-02 00:00:00|     293.11|            183.865|\n",
            "|C007YEYTX9|2012-02-03 00:00:00|      146.7|  171.4766666666667|\n",
            "|C007YEYTX9|2012-02-04 00:00:00|     3647.9|          1040.5825|\n",
            "|C007YEYTX9|2012-02-05 00:00:00|     261.46|            884.758|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 2 - 2025/04/16**"
      ],
      "metadata": {
        "id": "lK5enpK7rJAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Incremental Data Load Strategy (Medium)**\n",
        "\n",
        "Scenario: New transactions arrive daily. Write a PySpark job to:\n",
        "\n",
        "Load only new transactions (those with date > last_processed_date)\n",
        "\n",
        "Deduplicate using txn_id (keep latest record if duplicates exist)\n",
        "\n",
        "Update a master transactions table without reprocessing old data"
      ],
      "metadata": {
        "id": "cBpxXOAgukih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# latest transaction date in dataset is 2020-12-27\n",
        "\n",
        "# let's assume we only want to process records after 2020-09-01 and none before it\n",
        "\n",
        "# transactions_df is the raw data set that keeps populating everyday\n",
        "# master_transactions is the cleaned version of transactions_df with column last_processed_date\n",
        "# so we check what was the last_processed_date and filter using that value in the transactions_df\n",
        "\n",
        "\n",
        "# Cast date column to date type\n",
        "transactions_df = transactions_df.withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
        "\n",
        "# Define last processed date\n",
        "last_processed_date = \"2020-09-01\" # or max value from master_transactions\n",
        "\n",
        "# Filter only new transactions\n",
        "new_transactions = transactions_df.filter(F.col(\"date\") > F.lit(last_processed_date))\n",
        "\n",
        "# Deduplicate on txn_id by keeping the latest record per txn_id\n",
        "window_spec = Window.partitionBy(\"txn_id\").orderBy(F.col(\"date\").desc())\n",
        "\n",
        "deduped_new_txns = new_transactions.withColumn(\"rank\", F.row_number().over(window_spec))\\\n",
        "                                   .filter(F.col(\"rank\") == 1)\\\n",
        "                                   .drop(\"rank\")\n",
        "\n",
        "# Load master table as example\n",
        "master_transactions = transactions_df.filter(F.col(\"date\") <= F.lit(last_processed_date))\n",
        "\n",
        "# Append new deduped data to master\n",
        "updated_master = master_transactions.unionByName(deduped_new_txns)\n",
        "\n",
        "updated_master.orderBy(\"date\", ascending=False).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25srmM2fqnVA",
        "outputId": "924b80e4-2ef0-4205-c54d-5d8bd886446a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "|   cust_id|start_date|end_date|         txn_id|      date|year|month|day| expense_type|  amt|         city|            date_ts|\n",
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "|CO0JUS2B89|2013-03-01|    NULL|T09MG32WEEJYDTX|2020-12-27|2020|   12| 27|    Groceries|42.43|     portland|2020-12-27 00:00:00|\n",
            "|C4XSZX5AEQ|2012-11-01|    NULL|T093856A0IX0QKY|2020-12-27|2020|   12| 27|Entertainment|26.19|san_francisco|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-10-01|    NULL|T0I1IFIJMRCAPLO|2020-12-27|2020|   12| 27|Entertainment|25.56|    san_diego|2020-12-27 00:00:00|\n",
            "|CW57H4XRF4|2012-12-01|    NULL|T0INHCOKS6AYENZ|2020-12-27|2020|   12| 27|Entertainment|25.18|       boston|2020-12-27 00:00:00|\n",
            "|CRO4QOP409|2013-12-01|    NULL|T0A1STBKX7DM2GK|2020-12-27|2020|   12| 27|Entertainment|76.77|     new_york|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T09G6U9ZBQZVR99|2020-12-27|2020|   12| 27|Entertainment|27.16|       boston|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2012-12-01|    NULL|T00Z5JVTTS0F6TG|2020-12-27|2020|   12| 27|Entertainment|13.16|     new_york|2020-12-27 00:00:00|\n",
            "|CLXYTOOXJK|2013-04-01|    NULL|T00FL1TZRI4NWD7|2020-12-27|2020|   12| 27|Entertainment| 9.29|     new_york|2020-12-27 00:00:00|\n",
            "|CFUHOFZSHE|2013-02-01|    NULL|T0AB1WL6UO1KUK4|2020-12-27|2020|   12| 27|Entertainment|29.22| philadelphia|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-11-01|    NULL|T09O73JJBCDVV7W|2020-12-27|2020|   12| 27|Entertainment| 12.2|    san_diego|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-07-01|    NULL|T01Q2Y3BYA6AO3M|2020-12-27|2020|   12| 27| Motor/Travel|98.11|san_francisco|2020-12-27 00:00:00|\n",
            "|CQU9LG15ON|2013-05-01|    NULL|T01CDZ526V706VD|2020-12-27|2020|   12| 27|Entertainment| 8.73|     portland|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T0AME7FHS70MZFS|2020-12-27|2020|   12| 27|Entertainment|33.98|       boston|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-07-01|    NULL|T0ACN20D7KTUQ24|2020-12-27|2020|   12| 27|Entertainment|18.62|    san_diego|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T02JY52IQQ0UT68|2020-12-27|2020|   12| 27|Entertainment|36.99|  los_angeles|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2012-08-01|    NULL|T033GDLC4H7Z85Y|2020-12-27|2020|   12| 27|Entertainment|16.49|san_francisco|2020-12-27 00:00:00|\n",
            "|CHNFNR89ZV|2012-02-01|    NULL|T0ANZS80ILS4DQV|2020-12-27|2020|   12| 27|Entertainment|10.92|     portland|2020-12-27 00:00:00|\n",
            "|CFWOR4F86Q|2012-11-01|    NULL|T0AYLHF0P9EZEL2|2020-12-27|2020|   12| 27|Entertainment| 7.27|  los_angeles|2020-12-27 00:00:00|\n",
            "|CG8UBUD4V3|2013-07-01|    NULL|T04UOY0L3S9NM2E|2020-12-27|2020|   12| 27|    Groceries|193.2|       boston|2020-12-27 00:00:00|\n",
            "|CDZ683BJL4|2012-11-01|    NULL|T03AKK8R2MZ7Z8U|2020-12-27|2020|   12| 27|    Groceries|68.84|     new_york|2020-12-27 00:00:00|\n",
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Geospatial UDF Optimization (Hard)**\n",
        "Scenario: Calculate distances between customer ZIP codes and transaction cities.\n",
        "\n",
        "Create a UDF to convert ZIP codes to latitude/longitude (mock this)"
      ],
      "metadata": {
        "id": "iCz4uuoYxwwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lat_long(zip_code):\n",
        "  zip_int = int(str(zip_code).zfill(5))  # pad zeros if needed\n",
        "  latitude = 30.0 + (zip_int % 1000) * 0.01\n",
        "  longitude = -100.0 - (zip_int % 1000) * 0.01\n",
        "  return (latitude, longitude)\n",
        "\n",
        "udf_schema = StructType([StructField(\"latitude\", DoubleType()),\n",
        "                         StructField(\"longitude\", DoubleType())\n",
        "                         ])\n",
        "\n",
        "zip_to_lat_long_udf = F.udf(to_lat_long, udf_schema)\n",
        "\n",
        "# Apply the UDF\n",
        "df_with_coords = customers_df.withColumn(\"coords\", zip_to_lat_long_udf(F.col(\"zip\")))\n",
        "\n",
        "# Split into separate lat/lon columns\n",
        "df_with_coords = df_with_coords.withColumn(\"latitude\", F.col(\"coords.latitude\"))\\\n",
        "                               .withColumn(\"longitude\", F.col(\"coords.longitude\"))\\\n",
        "                               .drop(\"coords\")\n",
        "\n",
        "df_with_coords.show()"
      ],
      "metadata": {
        "id": "G_O_w12ku42w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab462e1-17dd-406a-b396-cf3b5a6c820b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "|   cust_id|          name|age|gender|  birthday|  zip|        city|          latitude|longitude|\n",
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "|C007YEYTX9|  Aaron Abbott| 34|Female| 7/13/1991|97823|      boston|38.230000000000004|  -108.23|\n",
            "|C00B971T1J|  Aaron Austin| 37|Female|12/16/2004|30332|     chicago|             33.32|  -103.32|\n",
            "|C00WRSJF1Q|  Aaron Barnes| 29|Female| 3/11/1977|23451|      denver|             34.51|  -104.51|\n",
            "|C01AZWQMF3| Aaron Barrett| 31|  Male|  7/9/1998|46613| los_angeles|             36.13|  -106.13|\n",
            "|C01BKUFRHA|  Aaron Becker| 54|  Male|11/24/1979|40284|   san_diego|             32.84|  -102.84|\n",
            "|C01RGUNJV9|    Aaron Bell| 24|Female| 8/16/1968|86331|      denver|             33.31|  -103.31|\n",
            "|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|    new_york|             30.78|  -100.78|\n",
            "|C01WMZQ7PN|   Aaron Brady| 51|Female| 8/20/1994|52204|philadelphia|             32.04|  -102.04|\n",
            "|C021567NJZ|  Aaron Briggs| 57|  Male| 3/10/1990|22008|philadelphia|             30.08|  -100.08|\n",
            "|C023M6MKR3|   Aaron Bryan| 29|  Male| 4/10/1976|05915|philadelphia|             39.15|  -109.15|\n",
            "|C0248N0EK3|  Aaron Burton| 26|Female| 8/27/1964|50477| los_angeles|             34.77|  -104.77|\n",
            "|C02C54RPNL|  Aaron Burton| 46|  Male| 5/29/1976|75857|     seattle|             38.57|  -108.57|\n",
            "|C02ERIY1O4|  Aaron Cannon| 50|  Male| 5/23/1965|70209|    portland|             32.09|  -102.09|\n",
            "|C02EVK2JWT|  Aaron Carter| 36|Female| 6/21/1993|89011|      denver|             30.11|  -100.11|\n",
            "|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|    new_york|             33.37|  -103.37|\n",
            "|C030A69V1L|  Aaron Clarke| 55|  Male| 4/28/1999|77176|philadelphia|             31.76|  -101.76|\n",
            "|C033JBNUYU|Aaron Ferguson| 27|  Male| 6/21/1959|73150|      denver|              31.5|   -101.5|\n",
            "|C034RB2MQ6|    Aaron Ford| 63|  Male|  7/8/1988|90592|     chicago|             35.92|  -105.92|\n",
            "|C036GAJ3BV|Aaron Franklin| 22|Female| 3/14/1961|01187|   san_diego|             31.87|  -101.87|\n",
            "|C03U340T3R| Aaron Gardner| 59|Female| 3/18/1975|31502|      denver|             35.02|  -105.02|\n",
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Skewed Join Mitigation (Hard)**\n",
        "\n",
        "Optimize this join between transactions_df and customers_df, where one of the customers has massive amounts of transactions as compared to others\n",
        "\n",
        "```\n",
        "+----------+--------+\n",
        "|   cust_id|   count|\n",
        "+----------+--------+\n",
        "|C0YDPQWPBJ|17539732|\n",
        "|C3KUDEN3KO|    7999|\n",
        "|CBW3FMEAU7|    7999|\n",
        "|C89FCEGPJP|    7999|\n",
        "|CHNFNR89ZV|    7998|\n",
        "+----------+--------+\n",
        "only showing top 5 rows\n",
        "```"
      ],
      "metadata": {
        "id": "RBZJaKx6oDD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first approach to rely on Spark's adaptive query execution to take the best route for the join\n",
        "joined_df_aqe = transactions_df.join(customers_df, customers_df.cust_id == transactions_df.cust_id, how='inner')\n",
        "\n",
        "joined_df_aqe.explain(mode=\"formatted\")\n",
        "joined_df_aqe.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-S28DZroHNr",
        "outputId": "871b0291-3ce9-48e3-a10d-e0e86fba9737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (7)\n",
            "+- BroadcastHashJoin Inner BuildRight (6)\n",
            "   :- Filter (2)\n",
            "   :  +- Scan parquet  (1)\n",
            "   +- BroadcastExchange (5)\n",
            "      +- Filter (4)\n",
            "         +- Scan parquet  (3)\n",
            "\n",
            "\n",
            "(1) Scan parquet \n",
            "Output [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/transactions.parquet]\n",
            "PushedFilters: [IsNotNull(cust_id)]\n",
            "ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,month:string,day:string,expense_type:string,amt:string,city:string>\n",
            "\n",
            "(2) Filter\n",
            "Input [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Condition : isnotnull(cust_id#0)\n",
            "\n",
            "(3) Scan parquet \n",
            "Output [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/customers.parquet]\n",
            "PushedFilters: [IsNotNull(cust_id)]\n",
            "ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
            "\n",
            "(4) Filter\n",
            "Input [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Condition : isnotnull(cust_id#22)\n",
            "\n",
            "(5) BroadcastExchange\n",
            "Input [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=103]\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [cust_id#0]\n",
            "Right keys [1]: [cust_id#22]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) AdaptiveSparkPlan\n",
            "Output [18]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|   cust_id|    name|age|gender| birthday|  zip|  city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second approach we can use salting\n",
        "\n",
        "# Add salt only for the skewed customer\n",
        "num_salts = 10  # number of salts for distributing skew\n",
        "\n",
        "transactions_df_salted = transactions_df.withColumn(\n",
        "    \"salt\",\n",
        "    F.when(F.col(\"cust_id\") == \"C0YDPQWPBJ\", F.floor(F.rand(seed=42) * num_salts).cast(\"int\"))\n",
        "    .otherwise(F.lit(None))\n",
        ")"
      ],
      "metadata": {
        "id": "bKdmCIj_oUnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Expand the skewed customer row\n",
        "skewed_rows = customers_df.filter(F.col(\"cust_id\") == \"C0YDPQWPBJ\").collect()\n",
        "salted_customers = []\n",
        "\n",
        "for row in skewed_rows:\n",
        "    for salt_val in range(num_salts):\n",
        "        row_dict = row.asDict()\n",
        "        row_dict[\"salt\"] = salt_val\n",
        "        salted_customers.append(Row(**row_dict))\n",
        "\n",
        "# Combine with rest of customers (unsalted)\n",
        "non_skewed_customers = customers_df.filter(F.col(\"cust_id\") != \"C0YDPQWPBJ\")\n",
        "salted_skewed_customers_df = spark.createDataFrame(salted_customers)\n",
        "\n",
        "# Add salt column with null to non-skewed customers\n",
        "non_skewed_customers = non_skewed_customers.withColumn(\"salt\", F.lit(None).cast(\"int\"))\n",
        "\n",
        "customers_df_salted = non_skewed_customers.unionByName(salted_skewed_customers_df)\n",
        "\n",
        "# Join using both cust_id and salt\n",
        "joined_df_salted = transactions_df_salted.join(\n",
        "    customers_df_salted,\n",
        "    on=[\"cust_id\", \"salt\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "joined_df_salted.explain(mode=\"formatted\")\n",
        "joined_df_salted.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSJlDiJ6qVoU",
        "outputId": "e54be566-a634-4c6a-9733-4a99df19ba8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (12)\n",
            "+- Project (11)\n",
            "   +- SortMergeJoin Inner (10)\n",
            "      :- Sort (5)\n",
            "      :  +- Exchange (4)\n",
            "      :     +- Filter (3)\n",
            "      :        +- Project (2)\n",
            "      :           +- Scan parquet  (1)\n",
            "      +- Sort (9)\n",
            "         +- Exchange (8)\n",
            "            +- Filter (7)\n",
            "               +- Scan ExistingRDD (6)\n",
            "\n",
            "\n",
            "(1) Scan parquet \n",
            "Output [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/transactions.parquet]\n",
            "ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,month:string,day:string,expense_type:string,amt:string,city:string>\n",
            "\n",
            "(2) Project\n",
            "Output [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, CASE WHEN (cust_id#0 = C0YDPQWPBJ) THEN cast(FLOOR((rand(42) * 10.0)) as int) END AS salt#325]\n",
            "Input [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "\n",
            "(3) Filter\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Condition : (isnotnull(cust_id#0) AND isnotnull(salt#325))\n",
            "\n",
            "(4) Exchange\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Arguments: hashpartitioning(cust_id#0, cast(salt#325 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=257]\n",
            "\n",
            "(5) Sort\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Arguments: [cust_id#0 ASC NULLS FIRST, cast(salt#325 as bigint) ASC NULLS FIRST], false, 0\n",
            "\n",
            "(6) Scan ExistingRDD\n",
            "Output [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L], MapPartitionsRDD[53] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
            "\n",
            "(7) Filter\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Condition : (isnotnull(cust_id#370) AND isnotnull(salt#377L))\n",
            "\n",
            "(8) Exchange\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: hashpartitioning(cust_id#370, salt#377L, 200), ENSURE_REQUIREMENTS, [plan_id=258]\n",
            "\n",
            "(9) Sort\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: [cust_id#370 ASC NULLS FIRST, salt#377L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(10) SortMergeJoin\n",
            "Left keys [2]: [cust_id#0, cast(salt#325 as bigint)]\n",
            "Right keys [2]: [cust_id#370, salt#377L]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(11) Project\n",
            "Output [18]: [cust_id#0, salt#325, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#371, age#372, gender#373, birthday#374, zip#375, city#376]\n",
            "Input [20]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325, cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "\n",
            "(12) AdaptiveSparkPlan\n",
            "Output [18]: [cust_id#0, salt#325, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#371, age#372, gender#373, birthday#374, zip#375, city#376]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "|   cust_id|salt|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|    name|age|gender| birthday|  zip|  city|\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97| los_angeles|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2011-07-01|2020-09-01|T4UTFAGFKIGZGKG|2019-02-10|2019|    2| 10|Entertainment|   6.8|philadelphia|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TSA7RJ3GSQSG4KX|2012-11-17|2012|   11| 17|Entertainment|   2.5|     chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2011-07-01|2020-09-01|TAQMZO2U0MBT9K9|2013-02-01|2013|    2|  1|    Groceries| 31.57|    new_york|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TODEIOF2REW2GKB|2017-03-25|2017|    3| 25|     Gambling| 164.5|    new_york|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "joined_df_aqe.cache().count()\n",
        "print(\"AQE Join Time:\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "joined_df_salted.cache().count()\n",
        "print(\"Salted Join Time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auRBHKGZquWJ",
        "outputId": "3ecfd636-489e-4649-b475-3c3b0fd56cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AQE Join Time: 428.58775758743286\n",
            "Salted Join Time: 256.59440326690674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 3 - 2025/04/18**"
      ],
      "metadata": {
        "id": "gRXFfmzdGEjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1: Basic Transformation**\n",
        "**User Full Name Creator**\n",
        "\n",
        "You are provided with a DataFrame of user information. Write a PySpark function to create a new column called `full_name`, which is a concatenation of the first name and last name, with a space in between.\n",
        "\n",
        "**Input DataFrame:**\n",
        "`users`\n",
        "```\n",
        "+----------+-----------+----------+\n",
        "| user_id  | first_name| last_name|\n",
        "+----------+-----------+----------+\n",
        "| 1        | Alice     | Cooper   |\n",
        "| 2        | Bob       | Marley   |\n",
        "+----------+-----------+----------+\n",
        "```\n",
        "\n",
        "**Output DataFrame:**\n",
        "```\n",
        "+----------+-----------+----------+-------------+\n",
        "| user_id  | first_name| last_name| full_name   |\n",
        "+----------+-----------+----------+-------------+\n",
        "| 1        | Alice     | Cooper   | Alice Cooper|\n",
        "| 2        | Bob       | Marley   | Bob Marley  |\n",
        "+----------+-----------+----------+-------------+\n",
        "```"
      ],
      "metadata": {
        "id": "WWpFApuLFzwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "         (1, \"Alice\", \"Cooper\"),\n",
        "         (2, \"Bob\", \"Marley\")\n",
        "         ]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"user_id\", \"first_name\", \"last_name\"])\n",
        "\n",
        "# full name column\n",
        "df_with_fullname = df.withColumn(\"full_name\", F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\")))\n",
        "\n",
        "df_with_fullname.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDkPAHIlGr8m",
        "outputId": "0190337a-945f-4864-c604-0d6dd61d46ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+------------+\n",
            "|user_id|first_name|last_name|   full_name|\n",
            "+-------+----------+---------+------------+\n",
            "|      1|     Alice|   Cooper|Alice Cooper|\n",
            "|      2|       Bob|   Marley|  Bob Marley|\n",
            "+-------+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2: Filter Orders by Value**\n",
        "\n",
        "**Scenario:**  \n",
        "You work at an e-commerce company and want to analyze high-value orders.\n",
        "\n",
        "**Task:**  \n",
        "Add a column `total_amount` (quantity Ã— unit_price) and return only the orders where `total_amount >= 100`.\n",
        "\n",
        "```python\n",
        "orders = spark.createDataFrame([\n",
        "    (1001, 1, 1, 50.0),\n",
        "    (1002, 2, 5, 20.0),\n",
        "    (1003, 3, 1, 200.0)\n",
        "], [\"order_id\", \"product_id\", \"quantity\", \"unit_price\"])\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "+--------+-----------+--------+-----------+-------------+\n",
        "|order_id|product_id |quantity|unit_price |total_amount |\n",
        "+--------+-----------+--------+-----------+-------------+\n",
        "|1002    |2          |5       |20.0       |100.0        |\n",
        "|1003    |3          |1       |200.0      |200.0        |\n",
        "+--------+-----------+--------+-----------+-------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "dSxk2QQpGZMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.createDataFrame([\n",
        "    (1001, 1, 1, 50.0),\n",
        "    (1002, 2, 5, 20.0),\n",
        "    (1003, 3, 1, 200.0)\n",
        "], [\"order_id\", \"product_id\", \"quantity\", \"unit_price\"])\n",
        "\n",
        "\n",
        "high_value_orders = orders.withColumn(\"total_amount\", F.col(\"quantity\") * F.col(\"unit_price\"))\\\n",
        "                            .filter(F.col(\"total_amount\") >=  100.0)\n",
        "\n",
        "high_value_orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVdS4n_DGtGE",
        "outputId": "446bdf06-c08a-4171-a469-f282c4677b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+----------+------------+\n",
            "|order_id|product_id|quantity|unit_price|total_amount|\n",
            "+--------+----------+--------+----------+------------+\n",
            "|    1002|         2|       5|      20.0|       100.0|\n",
            "|    1003|         3|       1|     200.0|       200.0|\n",
            "+--------+----------+--------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3: Join with Aggregation**\n",
        "\n",
        "**Scenario:**  \n",
        "You're helping HR compute the total hours each employee worked this month.\n",
        "\n",
        "**Task:**  \n",
        "Join `employees` and `timesheets`, and calculate the `total_hours` each employee has worked.\n",
        "\n",
        "```python\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\"),\n",
        "    (3, \"Charlie\")\n",
        "], [\"employee_id\", \"name\"])\n",
        "\n",
        "timesheets = spark.createDataFrame([\n",
        "    (1, \"2024-04-01\", 8),\n",
        "    (1, \"2024-04-02\", 7),\n",
        "    (2, \"2024-04-01\", 6),\n",
        "    (2, \"2024-04-02\", 5),\n",
        "    (3, \"2024-04-01\", 9)\n",
        "], [\"employee_id\", \"date\", \"hours\"])\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "+-----------+--------+\n",
        "|name       |total_hours|\n",
        "+-----------+------------+\n",
        "|Alice      |15          |\n",
        "|Bob        |11          |\n",
        "|Charlie    |9           |\n",
        "+-----------+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vGofdvHyGZGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\"),\n",
        "    (3, \"Charlie\")\n",
        "], [\"employee_id\", \"name\"])\n",
        "\n",
        "timesheets = spark.createDataFrame([\n",
        "    (1, \"2024-04-01\", 8),\n",
        "    (1, \"2024-04-02\", 7),\n",
        "    (2, \"2024-04-01\", 6),\n",
        "    (2, \"2024-04-02\", 5),\n",
        "    (3, \"2024-04-01\", 9)\n",
        "], [\"employee_id\", \"date\", \"hours\"])\n",
        "\n",
        "\n",
        "total_hours_per_employee = employees.join(timesheets, employees.employee_id==timesheets.employee_id, how = 'inner')\\\n",
        "                                    .groupBy(employees.employee_id, employees.name).agg(F.sum(F.col(\"hours\")).alias(\"total_hours\"))\\\n",
        "                                    .select(\"name\", \"total_hours\")\n",
        "\n",
        "total_hours_per_employee.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD4MiYfeKAjT",
        "outputId": "85379c9e-d62a-48d0-86b3-63d8dde2c9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   name|total_hours|\n",
            "+-------+-----------+\n",
            "|  Alice|         15|\n",
            "|    Bob|         11|\n",
            "|Charlie|          9|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQk2k5HBKAVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4: Window Functions**\n",
        "\n",
        "**Scenario:**  \n",
        "You are analyzing monthly sales and want to know the top-selling product each month.\n",
        "\n",
        "**Task:**  \n",
        "Use a window function to rank products by `units_sold` per month and return only those with rank = 1.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "sales = spark.createDataFrame([\n",
        "    (\"2024-01\", \"Product A\", 100),\n",
        "    (\"2024-01\", \"Product B\", 120),\n",
        "    (\"2024-01\", \"Product C\", 90),\n",
        "    (\"2024-02\", \"Product A\", 150),\n",
        "    (\"2024-02\", \"Product B\", 140),\n",
        "    (\"2024-02\", \"Product C\", 160)\n",
        "], [\"month\", \"product\", \"units_sold\"])\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "+--------+----------+-----------+\n",
        "|month   |product   |units_sold |\n",
        "+--------+----------+-----------+\n",
        "|2024-01 |Product B |120        |\n",
        "|2024-02 |Product C |160        |\n",
        "+--------+----------+-----------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "EvB6HbKUGY-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales = spark.createDataFrame([\n",
        "    (\"2024-01\", \"Product A\", 100),\n",
        "    (\"2024-01\", \"Product B\", 120),\n",
        "    (\"2024-01\", \"Product C\", 90),\n",
        "    (\"2024-02\", \"Product A\", 150),\n",
        "    (\"2024-02\", \"Product B\", 140),\n",
        "    (\"2024-02\", \"Product C\", 160)\n",
        "], [\"month\", \"product\", \"units_sold\"])\n",
        "\n",
        "window_spec = Window.partitionBy(\"month\").orderBy(F.col(\"units_sold\").desc())\n",
        "\n",
        "top_selling_product_per_month = sales.withColumn(\"rank\", F.row_number().over(window_spec))\\\n",
        "                                      .filter(F.col(\"rank\") == 1)\\\n",
        "                                      .select(\"month\", \"product\", \"units_sold\")\n",
        "top_selling_product_per_month.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK09lgFwLqEG",
        "outputId": "a36c609e-cd1b-4df4-e5ac-40ac6c9be175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+----------+\n",
            "|  month|  product|units_sold|\n",
            "+-------+---------+----------+\n",
            "|2024-01|Product B|       120|\n",
            "|2024-02|Product C|       160|\n",
            "+-------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q5: Multi-Join with Filtering and Aggregation**\n",
        "\n",
        "**Scenario:**  \n",
        "Youâ€™re working at a logistics company. You need to compute how many packages each customer received in February 2024 **only from active couriers**.\n",
        "\n",
        "**Task:**  \n",
        "Join `customers`, `shipments`, and `couriers` to return a DataFrame with `customer_name`, `courier_name`, and the number of shipments in Feb 2024 only from active couriers.\n",
        "\n",
        "```python\n",
        "customers = spark.createDataFrame([\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\"),\n",
        "    (3, \"Charlie\")\n",
        "], [\"customer_id\", \"customer_name\"])\n",
        "\n",
        "couriers = spark.createDataFrame([\n",
        "    (101, \"FastX\", True),\n",
        "    (102, \"GoExpress\", False),\n",
        "    (103, \"QuickShip\", True)\n",
        "], [\"courier_id\", \"courier_name\", \"is_active\"])\n",
        "\n",
        "shipments = spark.createDataFrame([\n",
        "    (1001, 1, 101, \"2024-02-01\"),\n",
        "    (1002, 1, 101, \"2024-02-15\"),\n",
        "    (1003, 2, 102, \"2024-01-10\"),\n",
        "    (1004, 3, 103, \"2024-01-30\"),\n",
        "    (1005, 3, 103, \"2024-02-12\")\n",
        "], [\"shipment_id\", \"customer_id\", \"courier_id\", \"shipment_date\"])\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "+-------------+-------------+-------------------+\n",
        "|customer_name|courier_name |num_shipments_feb  |\n",
        "+-------------+-------------+-------------------+\n",
        "|Alice        |FastX        |2                  |\n",
        "|Charlie      |QuickShip    |1                  |\n",
        "+-------------+-------------+-------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "eNyVZhzmGYz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = spark.createDataFrame([\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\"),\n",
        "    (3, \"Charlie\")\n",
        "], [\"customer_id\", \"customer_name\"])\n",
        "\n",
        "couriers = spark.createDataFrame([\n",
        "    (101, \"FastX\", True),\n",
        "    (102, \"GoExpress\", False),\n",
        "    (103, \"QuickShip\", True)\n",
        "], [\"courier_id\", \"courier_name\", \"is_active\"])\n",
        "\n",
        "shipments = spark.createDataFrame([\n",
        "    (1001, 1, 101, \"2024-02-01\"),\n",
        "    (1002, 1, 101, \"2024-02-15\"),\n",
        "    (1003, 2, 102, \"2024-01-10\"),\n",
        "    (1004, 3, 103, \"2024-01-30\"),\n",
        "    (1005, 3, 103, \"2024-02-12\")\n",
        "], [\"shipment_id\", \"customer_id\", \"courier_id\", \"shipment_date\"])\n",
        "\n",
        "customers.join(shipments, shipments.customer_id == customers.customer_id, how='inner')\\\n",
        "          .join(couriers, shipments.courier_id == couriers.courier_id, how = 'inner')\\\n",
        "          .filter(F.col(\"shipment_date\").between(\"2024-02-01\", \"2024-02-29\"))\\\n",
        "          .groupBy(customers.customer_name, couriers.courier_name).count()\\\n",
        "          .show()"
      ],
      "metadata": {
        "id": "OJBpes55GCyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402241dc-7860-442e-9385-015039f93aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+-----+\n",
            "|customer_name|courier_name|count|\n",
            "+-------------+------------+-----+\n",
            "|      Charlie|   QuickShip|    1|\n",
            "|        Alice|       FastX|    2|\n",
            "+-------------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 4 - 2025/04/19**"
      ],
      "metadata": {
        "id": "NpJdwdA42QyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Product Sales per Category and Price Tier\n",
        "\n",
        "You're analyzing product sales performance. Group products into price tiers:  \n",
        "- \"Low\" for price < 50  \n",
        "- \"Mid\" for 50 <= price < 150  \n",
        "- \"High\" for price >= 150  \n",
        "\n",
        "Then compute the total units sold per category and price tier.\n",
        "\n",
        "```python\n",
        "products = spark.createDataFrame([\n",
        "    (101, \"Gadget\", \"Electronics\", 49.99),\n",
        "    (102, \"Smartphone\", \"Electronics\", 199.99),\n",
        "    (103, \"Blender\", \"Home Appliances\", 89.99),\n",
        "    (104, \"Microwave\", \"Home Appliances\", 149.99),\n",
        "    (105, \"Notebook\", \"Stationery\", 5.99)\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\"])\n",
        "\n",
        "sales = spark.createDataFrame([\n",
        "    (101, 200),\n",
        "    (102, 150),\n",
        "    (103, 100),\n",
        "    (104, 120),\n",
        "    (105, 300)\n",
        "], [\"product_id\", \"units_sold\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "+------------------+----------+------------+\n",
        "|category          |price_tier|total_units |\n",
        "+------------------+----------+------------+\n",
        "|Electronics       |Low       |200         |\n",
        "|Electronics       |High      |150         |\n",
        "|Home Appliances   |Mid       |100         |\n",
        "|Home Appliances   |High      |120         |\n",
        "|Stationery        |Low       |300         |\n",
        "+------------------+----------+------------+\n",
        "```"
      ],
      "metadata": {
        "id": "qdkHPD5L2cFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = spark.createDataFrame([\n",
        "    (101, \"Gadget\", \"Electronics\", 49.99),\n",
        "    (102, \"Smartphone\", \"Electronics\", 199.99),\n",
        "    (103, \"Blender\", \"Home Appliances\", 89.99),\n",
        "    (104, \"Microwave\", \"Home Appliances\", 149.99),\n",
        "    (105, \"Notebook\", \"Stationery\", 5.99)\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\"])\n",
        "\n",
        "sales = spark.createDataFrame([\n",
        "    (101, 200),\n",
        "    (102, 150),\n",
        "    (103, 100),\n",
        "    (104, 120),\n",
        "    (105, 300)\n",
        "], [\"product_id\", \"units_sold\"])\n",
        "\n",
        "products_with_tier = products.withColumn(\"price_tier\",\n",
        "                                                      F.when(F.col(\"price\")<50, \"Low\")\\\n",
        "                                                      .when(F.col(\"price\").between(50, 150), \"Mid\")\\\n",
        "                                                      .otherwise(\"High\")\n",
        "                                                      )\n",
        "\n",
        "sales.join(products_with_tier, products_with_tier.product_id == sales.product_id, how='inner')\\\n",
        "      .groupBy(\"category\", \"price_tier\").agg(F.sum(F.col(\"units_sold\")).alias(\"total_units\"))\\\n",
        "      .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCB764iKzj6q",
        "outputId": "b09c51f2-863b-449a-9921-df264bbec2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-----------+\n",
            "|       category|price_tier|total_units|\n",
            "+---------------+----------+-----------+\n",
            "|    Electronics|      High|        150|\n",
            "|    Electronics|       Low|        200|\n",
            "|     Stationery|       Low|        300|\n",
            "|Home Appliances|       Mid|        220|\n",
            "+---------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7: Latest Login per User\n",
        "\n",
        "Youâ€™re maintaining a system that logs user logins. Return only the latest login for each user, along with their device.\n",
        "\n",
        "```python\n",
        "logins = spark.createDataFrame([\n",
        "    (1, \"2024-04-01 10:00:00\", \"Chrome\"),\n",
        "    (1, \"2024-04-01 18:00:00\", \"Firefox\"),\n",
        "    (2, \"2024-04-02 09:00:00\", \"Safari\"),\n",
        "    (3, \"2024-04-01 20:00:00\", \"Edge\"),\n",
        "    (3, \"2024-04-03 08:30:00\", \"Chrome\")\n",
        "], [\"user_id\", \"login_time\", \"device\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "+--------+-------------------+--------+\n",
        "|user_id |login_time         |device  |\n",
        "+--------+-------------------+--------+\n",
        "|1       |2024-04-01 18:00:00|Firefox |\n",
        "|2       |2024-04-02 09:00:00|Safari  |\n",
        "|3       |2024-04-03 08:30:00|Chrome  |\n",
        "+--------+-------------------+--------+\n",
        "```"
      ],
      "metadata": {
        "id": "9ZrGFrmxZBuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logins = spark.createDataFrame([\n",
        "    (1, \"2024-04-01 10:00:00\", \"Chrome\"),\n",
        "    (1, \"2024-04-01 18:00:00\", \"Firefox\"),\n",
        "    (2, \"2024-04-02 09:00:00\", \"Safari\"),\n",
        "    (3, \"2024-04-01 20:00:00\", \"Edge\"),\n",
        "    (3, \"2024-04-03 08:30:00\", \"Chrome\")\n",
        "], [\"user_id\", \"login_time\", \"device\"])\n",
        "\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(F.col(\"login_time\").desc())\n",
        "\n",
        "latest_logins = logins.withColumn(\"row_num\", F.row_number().over(window_spec))\\\n",
        "                      .filter(F.col(\"row_num\")==1)\\\n",
        "                      .select(\"user_id\", \"login_time\", \"device\")\n",
        "\n",
        "latest_logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErP9QHwyZbLF",
        "outputId": "e5763ad1-0bdd-4878-f118-b7d0c8cf5d19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+\n",
            "|user_id|         login_time| device|\n",
            "+-------+-------------------+-------+\n",
            "|      1|2024-04-01 18:00:00|Firefox|\n",
            "|      2|2024-04-02 09:00:00| Safari|\n",
            "|      3|2024-04-03 08:30:00| Chrome|\n",
            "+-------+-------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8: Customer Retention Calculation\n",
        "\n",
        "Youâ€™re analyzing whether customers placed orders in multiple months. For each customer, return the number of distinct months in which they placed orders.\n",
        "\n",
        "```python\n",
        "orders = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\"),\n",
        "    (1, \"2024-02-20\"),\n",
        "    (2, \"2024-01-10\"),\n",
        "    (3, \"2024-02-12\"),\n",
        "    (3, \"2024-02-28\"),\n",
        "    (3, \"2024-03-01\")\n",
        "], [\"customer_id\", \"order_date\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "+-------------+------------------+\n",
        "|customer_id  |months_active     |\n",
        "+-------------+------------------+\n",
        "|1            |2                 |\n",
        "|2            |1                 |\n",
        "|3            |2                 |\n",
        "+-------------+------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "JHpXh0soZDJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.createDataFrame([\n",
        "    (1, \"2024-01-15\"),\n",
        "    (1, \"2024-02-20\"),\n",
        "    (2, \"2024-01-10\"),\n",
        "    (3, \"2024-02-12\"),\n",
        "    (3, \"2024-02-28\"),\n",
        "    (3, \"2024-03-01\")\n",
        "], [\"customer_id\", \"order_date\"])\n",
        "\n",
        "orders_with_date_dims = orders.withColumn(\"month\", F.month(F.col(\"order_date\")))\\\n",
        "                              .withColumn(\"year\", F.year(F.col(\"order_date\")))\n",
        "\n",
        "months_active = orders_with_date_dims.groupBy(\"customer_id\").agg(F.count_distinct(\"month\").alias(\"months_active\"))\n",
        "\n",
        "months_active.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5e8MOuucKsk",
        "outputId": "8dabbae7-4b3c-48d5-c7d7-feb7498532c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|customer_id|months_active|\n",
            "+-----------+-------------+\n",
            "|          1|            2|\n",
            "|          3|            2|\n",
            "|          2|            1|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9: Top 2 Products by Revenue per Category\n",
        "\n",
        "You need to extract the top 2 products in each category based on total revenue (`price Ã— units_sold`).\n",
        "\n",
        "```python\n",
        "products = spark.createDataFrame([\n",
        "    (1, \"Product A\", \"Books\", 10),\n",
        "    (2, \"Product B\", \"Books\", 15),\n",
        "    (3, \"Product C\", \"Books\", 20),\n",
        "    (4, \"Product D\", \"Electronics\", 200),\n",
        "    (5, \"Product E\", \"Electronics\", 180)\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\"])\n",
        "\n",
        "sales = spark.createDataFrame([\n",
        "    (1, 100),\n",
        "    (2, 150),\n",
        "    (3, 50),\n",
        "    (4, 10),\n",
        "    (5, 15)\n",
        "], [\"product_id\", \"units_sold\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "+----------+------------+---------+\n",
        "|category  |product_name|revenue  |\n",
        "+----------+------------+---------+\n",
        "|Books     |Product B   |2250     |\n",
        "|Books     |Product A   |1000     |\n",
        "|Electronics|Product E  |2700     |\n",
        "|Electronics|Product D  |2000     |\n",
        "+----------+------------+---------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "tMMVeqs1ZHKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = spark.createDataFrame([\n",
        "    (1, \"Product A\", \"Books\", 10),\n",
        "    (2, \"Product B\", \"Books\", 15),\n",
        "    (3, \"Product C\", \"Books\", 20),\n",
        "    (4, \"Product D\", \"Electronics\", 200),\n",
        "    (5, \"Product E\", \"Electronics\", 180)\n",
        "], [\"product_id\", \"product_name\", \"category\", \"price\"])\n",
        "\n",
        "sales = spark.createDataFrame([\n",
        "    (1, 100),\n",
        "    (2, 150),\n",
        "    (3, 50),\n",
        "    (4, 10),\n",
        "    (5, 15)\n",
        "], [\"product_id\", \"units_sold\"])\n",
        "\n",
        "# rename to avoid ambigious reference\n",
        "products_for_join = products.withColumnRenamed(\"product_id\", \"id\")\n",
        "\n",
        "window_spec = Window.orderBy(F.col(\"revenue\").desc()).partitionBy(\"category\")\n",
        "\n",
        "revenue = products_for_join.join(sales, sales.product_id == products_for_join.id, how = 'inner')\\\n",
        "                  .withColumn(\"revenue\", F.col(\"units_sold\") * F.col(\"price\"))\\\n",
        "                  .withColumn(\"revenue_rank\", F.row_number().over(window_spec))\\\n",
        "                  .filter(F.col(\"revenue_rank\").between(1,2))\\\n",
        "                  .select(\"category\", \"product_name\", \"revenue\")\n",
        "\n",
        "revenue.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EeFJ5IpdC-d",
        "outputId": "d8f6dded-2d55-44bd-faa8-6d816a4c1cef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+-------+\n",
            "|   category|product_name|revenue|\n",
            "+-----------+------------+-------+\n",
            "|      Books|   Product B|   2250|\n",
            "|      Books|   Product A|   1000|\n",
            "|Electronics|   Product E|   2700|\n",
            "|Electronics|   Product D|   2000|\n",
            "+-----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10: Booking Gap Analysis\n",
        "\n",
        "A hotel wants to analyze gaps in room bookings. For each room, return the number of days between consecutive bookings (ordered by start date). Include bookings that have gaps only (i.e., if the next booking doesn't start the day after the previous ends).\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "bookings = spark.createDataFrame([\n",
        "    (101, \"2024-04-01\", \"2024-04-03\"),\n",
        "    (101, \"2024-04-05\", \"2024-04-07\"),\n",
        "    (101, \"2024-04-07\", \"2024-04-09\"),\n",
        "    (102, \"2024-04-01\", \"2024-04-02\"),\n",
        "    (102, \"2024-04-10\", \"2024-04-12\")\n",
        "], [\"room_id\", \"start_date\", \"end_date\"])\n",
        "```\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "+--------+----------------+----------------+-----------+\n",
        "|room_id |prev_end_date   |next_start_date |gap_days   |\n",
        "+--------+----------------+----------------+-----------+\n",
        "|101     |2024-04-03      |2024-04-05      |2          |\n",
        "|102     |2024-04-02      |2024-04-10      |8          |\n",
        "+--------+----------------+----------------+-----------+\n",
        "```"
      ],
      "metadata": {
        "id": "96Za_RqmZPmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bookings = spark.createDataFrame([\n",
        "    (101, \"2024-04-01\", \"2024-04-03\"),\n",
        "    (101, \"2024-04-05\", \"2024-04-07\"),\n",
        "    (101, \"2024-04-07\", \"2024-04-09\"),\n",
        "    (102, \"2024-04-01\", \"2024-04-02\"),\n",
        "    (102, \"2024-04-10\", \"2024-04-12\")\n",
        "], [\"room_id\", \"start_date\", \"end_date\"])\n",
        "\n",
        "window_spec = Window.orderBy(F.col(\"start_date\")).partitionBy(\"room_id\")\n",
        "\n",
        "bookings.withColumn(\"next_start_date\", F.lead(F.col(\"start_date\"), 1).over(window_spec))\\\n",
        "        .withColumn(\"days_gap\", F.date_diff(\"next_start_date\", \"end_date\"))\\\n",
        "        .filter((F.col(\"days_gap\")!=0)&((F.col(\"days_gap\").isNotNull())))\\\n",
        "        .select(\"room_id\", F.col(\"end_date\").alias(\"prev_end_date\"), \"next_start_date\", \"days_gap\")\\\n",
        "        .show()"
      ],
      "metadata": {
        "id": "uRQqnU-J3mas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8997c130-223e-4043-9f14-2fb27d287e24"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------+--------+\n",
            "|room_id|prev_end_date|next_start_date|days_gap|\n",
            "+-------+-------------+---------------+--------+\n",
            "|    101|   2024-04-03|     2024-04-05|       2|\n",
            "|    102|   2024-04-02|     2024-04-10|       8|\n",
            "+-------+-------------+---------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q0y_-dmhQGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}