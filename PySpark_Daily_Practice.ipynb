{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCSB2fxB+LR+TLbkHWlr13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyan9/pyspark-learning-journey/blob/main/PySpark_Daily_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "d5855f33-de4e-4dc1-902c-bfc8663dda83"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a24066d6210>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b1e2e89ee675:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "0377509d-0b70-4f89-e226-e21461509203"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# Reading Data\n",
        "\n",
        "For this example, I am going to use a data set from this [github repo](https://github.com/afaqueahmad7117/spark-experiments.git)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/afaqueahmad7117/spark-experiments.git\n",
        "\n",
        "# Load datasets from the cloned repo\n",
        "transactions_df = spark.read.parquet(\"spark-experiments/data/data_skew/transactions.parquet\")\n",
        "customers_df = spark.read.parquet(\"spark-experiments/data/data_skew/customers.parquet\")\n",
        "\n",
        "print(\"Transactions Dataset Schema:\")\n",
        "transactions_df.printSchema()\n",
        "print(\"Customers Dataset Schema:\")\n",
        "customers_df.printSchema()"
      ],
      "metadata": {
        "id": "NfXVooOy8kJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0be013f-a8e2-4511-fa14-67dd2f147a3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spark-experiments'...\n",
            "remote: Enumerating objects: 544, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 544 (delta 7), reused 0 (delta 0), pack-reused 536 (from 1)\u001b[K\n",
            "Receiving objects: 100% (544/544), 702.60 MiB | 24.83 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Updating files: 100% (351/351), done.\n",
            "Transactions Dataset Schema:\n",
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- expense_type: string (nullable = true)\n",
            " |-- amt: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "Customers Dataset Schema:\n",
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- birthday: string (nullable = true)\n",
            " |-- zip: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩 **Day 1 - 2025/04/14**"
      ],
      "metadata": {
        "id": "hwEA0-uvHAAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Schema Validation & Type Conversion (Easy)**\n",
        "\n",
        "Your ETL pipeline ingests raw data with all columns as strings. Convert the `amt` (transaction amount) to DoubleType and `age` to IntegerType. Validate by showing the schema post-conversion."
      ],
      "metadata": {
        "id": "L4TMklh0G1e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBNVmWuIGYm",
        "outputId": "2cba1d10-200c-4be4-adba-017698e302bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df = transactions_df.withColumn(\"amt\", F.col(\"amt\").cast(DoubleType()))\n",
        "transactions_df.printSchema()\n",
        "\n",
        "transactions_df.show(3) # this triggers the transformation"
      ],
      "metadata": {
        "id": "JgYb9Af68h3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ca042c-6c35-44ba-c98d-ae403681d362"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- expense_type: string (nullable = true)\n",
            " |-- amt: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|    city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment|10.42|  boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel|44.34|portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment| 3.18| chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Time-Based Aggregations (Medium)**\n",
        "Scenario: The business wants monthly expense reports. Calculate total monthly expenses per customer, preserving the original schema's year and month columns. Handle potential nulls in amt.\n",
        "\n",
        "```\n",
        "# Expected output schema\n",
        "# |-- cust_id: string\n",
        "# |-- year: string\n",
        "# |-- month: string\n",
        "# |-- total_expense: double\n",
        "```"
      ],
      "metadata": {
        "id": "fMaPtAdxJbqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_transactions_per_customer = transactions_df.groupBy(\"cust_id\",\"year\",\"month\").agg(F.sum(\"amt\").alias(\"total_expense\"))\\\n",
        "                                                    .orderBy(\"year\",\"month\",\"total_expense\")\n",
        "monthly_transactions_per_customer.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTaX9tlJKae",
        "outputId": "c45924e9-5887-4d49-ed4a-8868bce60267"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+------------------+\n",
            "|   cust_id|year|month|     total_expense|\n",
            "+----------+----+-----+------------------+\n",
            "|C42POJ8QKI|2010|    1|298.94000000000005|\n",
            "|CC5E1YOY7N|2010|    1|302.53000000000003|\n",
            "|CCQ557SM5V|2010|    1|332.96000000000004|\n",
            "|CLESRZVUWU|2010|    1|334.14000000000004|\n",
            "|COV6YRAYE9|2010|    1|362.21999999999997|\n",
            "+----------+----+-----+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Quality Check (Medium)**\n",
        "Scenario: Your team needs to validate customer data. Find:\n",
        "\n",
        "1. Customers with invalid ZIP codes (non-5-digit format)\n",
        "2. Transactions with future dates (dates beyond today's date)\n",
        "\n",
        "Return counts for both anomalies."
      ],
      "metadata": {
        "id": "-YbDsg9OK3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.select(\"cust_id\", \"zip\")\\\n",
        "            .filter(F.length(F.col(\"zip\")) != 5)\\\n",
        "            .show()"
      ],
      "metadata": {
        "id": "C6GtBQQlKJCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1552d4c1-4d77-4bc1-b0eb-6f3e2377dacf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|cust_id|zip|\n",
            "+-------+---+\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*There are no records with zip codes with not equal to 5 digits*"
      ],
      "metadata": {
        "id": "-eJZ-9l2l2z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Customer-Transaction Enrichment (Hard)**\n",
        "\n",
        "Scenario: Create a master dataset showing each transaction enriched with customer demographics. Optimize for:\n",
        "\n",
        "Fast joins using broadcast join where appropriate\n",
        "\n",
        "Handling null values in customer data\n",
        "\n",
        "Preserving original transaction order*\n",
        "\n",
        "```\n",
        "# Target schema\n",
        "# |-- txn_id: string\n",
        "# |-- date: string\n",
        "# |-- expense_type: string\n",
        "# |-- amt: double\n",
        "# |-- city: string\n",
        "# |-- name: string\n",
        "# |-- age: int\n",
        "# |-- gender: string\n",
        "```"
      ],
      "metadata": {
        "id": "eUCBlAFRmFYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df_for_join = transactions_df.select(\"txn_id\", \"cust_id\", \"date\", \"expense_type\", \"amt\", F.col(\"city\").alias(\"txn_city\"))\n",
        "\n",
        "customers_df_for_join = customers_df.select(\"cust_id\", \"name\", \"age\", \"gender\")\n",
        "\n",
        "master_df = transactions_df_for_join.join(F.broadcast(customers_df_for_join),\n",
        "                              customers_df_for_join.cust_id == transactions_df_for_join.cust_id,\n",
        "                              how='inner')\n",
        "\n",
        "master_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGeEEjFrlm7P",
        "outputId": "a3ddaa88-b893-46c2-b1bd-4a8dec214fb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|         txn_id|   cust_id|      date| expense_type|   amt|   txn_city|   cust_id|    name|age|gender|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|TZ5SMKZY9S03OQJ|C0YDPQWPBJ|2018-10-07|Entertainment| 10.42|     boston|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYIAPPNU066CJ5R|C0YDPQWPBJ|2016-03-27| Motor/Travel| 44.34|   portland|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TETSXIK4BLXHJ6W|C0YDPQWPBJ|2011-04-11|Entertainment|  3.18|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TQKL1QFJY3EM8LO|C0YDPQWPBJ|2018-02-22|    Groceries|268.97|los_angeles|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYL6DFP09PPXMVB|C0YDPQWPBJ|2010-10-16|Entertainment|  2.66|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Window Function Analysis (Hard)**\n",
        "\n",
        "Scenario: For customer retention analysis, calculate:\n",
        "\n",
        "a) Days since previous transaction per customer\n",
        "\n",
        "b) Rolling 30-day average spend per customer\n",
        "\n",
        "Use appropriate window functions and handle partition boundaries."
      ],
      "metadata": {
        "id": "c2zTCFvQnzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Days since previous transaction per customer\n",
        "# I'm assuming we are comparing it to today's date\n",
        "\n",
        "# group by cust_id, get last transaction date, check the difference between today's date and last transaction date\n",
        "transactions_df.groupBy(\"cust_id\").agg(F.max(\"date\").alias(\"last_transaction_date\"))\\\n",
        "                .withColumn(\"days_since_last_txn\", F.date_diff(F.current_date(), F.col(\"last_transaction_date\")))\\\n",
        "                .show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8UwdQykmWlA",
        "outputId": "24b30189-85be-4502-c765-87d27db871be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+-------------------+\n",
            "|   cust_id|last_transaction_date|days_since_last_txn|\n",
            "+----------+---------------------+-------------------+\n",
            "|C007YEYTX9|           2020-09-27|               1662|\n",
            "|C00B971T1J|           2020-12-27|               1571|\n",
            "|C00WRSJF1Q|           2020-12-27|               1571|\n",
            "|C01AZWQMF3|           2019-03-27|               2212|\n",
            "|C01BKUFRHA|           2020-09-27|               1662|\n",
            "+----------+---------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'date' is in timestamp format\n",
        "transactions_df = transactions_df.withColumn(\"date_ts\", F.col(\"date\").cast(\"timestamp\"))\n",
        "\n",
        "# Create a time-based window: last 30 days per customer\n",
        "window_30_days = Window.partitionBy(\"cust_id\")\\\n",
        "                       .orderBy(F.col(\"date_ts\").cast(\"long\"))\\\n",
        "                       .rangeBetween(-30 * 86400, 0)  # last 30 days in seconds\n",
        "\n",
        "# First aggregate daily total per customer\n",
        "daily_total_df = transactions_df.groupBy(\"cust_id\", \"date_ts\").agg(F.sum(\"amt\").alias(\"daily_total\"))\n",
        "\n",
        "# Apply rolling average over the 30-day window\n",
        "result_df = daily_total_df.withColumn(\"rolling_30_days_avg\", F.avg(\"daily_total\").over(window_30_days))\n",
        "\n",
        "result_df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_47U4Io6Ne",
        "outputId": "3cd50e4e-4ea3-4867-fb14-97022b949e35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-----------+-------------------+\n",
            "|   cust_id|            date_ts|daily_total|rolling_30_days_avg|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "|C007YEYTX9|2012-02-01 00:00:00|      74.62|              74.62|\n",
            "|C007YEYTX9|2012-02-02 00:00:00|     293.11|            183.865|\n",
            "|C007YEYTX9|2012-02-03 00:00:00|      146.7|  171.4766666666667|\n",
            "|C007YEYTX9|2012-02-04 00:00:00|     3647.9|          1040.5825|\n",
            "|C007YEYTX9|2012-02-05 00:00:00|     261.46|            884.758|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩 **Day 2 - 2025/04/16**"
      ],
      "metadata": {
        "id": "lK5enpK7rJAg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25srmM2fqnVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}