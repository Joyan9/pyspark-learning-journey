{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1VVCXww2FxEa8+uzXOOJy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "df2f145b-f0a3-4bc2-b5bf-ccf3195cebd5"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .config('spark.ui.port', '4050') \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,430 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,832 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,246 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,692 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Fetched 26.2 MB in 9s (2,986 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b16e00e6e50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5f32dff0114b:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4050, path ='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "688VkZySBOVT",
        "outputId": "cadc4fd8-6ce6-470c-fb48-7be221dae12b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# Reading Data\n",
        "\n",
        "For this example, I am going to use a data set from this [github repo](https://github.com/afaqueahmad7117/spark-experiments.git)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/afaqueahmad7117/spark-experiments.git\n",
        "\n",
        "# Load datasets from the cloned repo\n",
        "transactions_df = spark.read.parquet(\"spark-experiments/data/data_skew/transactions.parquet\")\n",
        "customers_df = spark.read.parquet(\"spark-experiments/data/data_skew/customers.parquet\")\n",
        "\n",
        "print(\"Transactions Dataset Schema:\")\n",
        "transactions_df.printSchema()\n",
        "print(\"Customers Dataset Schema:\")\n",
        "customers_df.printSchema()"
      ],
      "metadata": {
        "id": "NfXVooOy8kJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640c6296-f342-4ad3-f89a-625faebd4b03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spark-experiments'...\n",
            "remote: Enumerating objects: 544, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 544 (delta 7), reused 0 (delta 0), pack-reused 536 (from 1)\u001b[K\n",
            "Receiving objects: 100% (544/544), 702.60 MiB | 19.53 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Updating files: 100% (351/351), done.\n",
            "Transactions Dataset Schema:\n",
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- expense_type: string (nullable = true)\n",
            " |-- amt: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "Customers Dataset Schema:\n",
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- birthday: string (nullable = true)\n",
            " |-- zip: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 1 - 2025/04/14**"
      ],
      "metadata": {
        "id": "hwEA0-uvHAAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Schema Validation & Type Conversion (Easy)**\n",
        "\n",
        "Your ETL pipeline ingests raw data with all columns as strings. Convert the `amt` (transaction amount) to DoubleType and `age` to IntegerType. Validate by showing the schema post-conversion."
      ],
      "metadata": {
        "id": "L4TMklh0G1e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBNVmWuIGYm",
        "outputId": "2cba1d10-200c-4be4-adba-017698e302bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df = transactions_df.withColumn(\"amt\", F.col(\"amt\").cast(DoubleType()))\n",
        "transactions_df.printSchema()\n",
        "\n",
        "transactions_df.show(3) # this triggers the transformation"
      ],
      "metadata": {
        "id": "JgYb9Af68h3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ca042c-6c35-44ba-c98d-ae403681d362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- cust_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- expense_type: string (nullable = true)\n",
            " |-- amt: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|    city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment|10.42|  boston|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel|44.34|portland|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment| 3.18| chicago|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Time-Based Aggregations (Medium)**\n",
        "Scenario: The business wants monthly expense reports. Calculate total monthly expenses per customer, preserving the original schema's year and month columns. Handle potential nulls in amt.\n",
        "\n",
        "```\n",
        "# Expected output schema\n",
        "# |-- cust_id: string\n",
        "# |-- year: string\n",
        "# |-- month: string\n",
        "# |-- total_expense: double\n",
        "```"
      ],
      "metadata": {
        "id": "fMaPtAdxJbqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_transactions_per_customer = transactions_df.groupBy(\"cust_id\",\"year\",\"month\").agg(F.sum(\"amt\").alias(\"total_expense\"))\\\n",
        "                                                    .orderBy(\"year\",\"month\",\"total_expense\")\n",
        "monthly_transactions_per_customer.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTaX9tlJKae",
        "outputId": "c45924e9-5887-4d49-ed4a-8868bce60267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+------------------+\n",
            "|   cust_id|year|month|     total_expense|\n",
            "+----------+----+-----+------------------+\n",
            "|C42POJ8QKI|2010|    1|298.94000000000005|\n",
            "|CC5E1YOY7N|2010|    1|302.53000000000003|\n",
            "|CCQ557SM5V|2010|    1|332.96000000000004|\n",
            "|CLESRZVUWU|2010|    1|334.14000000000004|\n",
            "|COV6YRAYE9|2010|    1|362.21999999999997|\n",
            "+----------+----+-----+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Quality Check (Medium)**\n",
        "Scenario: Your team needs to validate customer data. Find:\n",
        "\n",
        "1. Customers with invalid ZIP codes (non-5-digit format)\n",
        "2. Transactions with future dates (dates beyond today's date)\n",
        "\n",
        "Return counts for both anomalies."
      ],
      "metadata": {
        "id": "-YbDsg9OK3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.select(\"cust_id\", \"zip\")\\\n",
        "            .filter(F.length(F.col(\"zip\")) != 5)\\\n",
        "            .show()"
      ],
      "metadata": {
        "id": "C6GtBQQlKJCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1552d4c1-4d77-4bc1-b0eb-6f3e2377dacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|cust_id|zip|\n",
            "+-------+---+\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*There are no records with zip codes with not equal to 5 digits*"
      ],
      "metadata": {
        "id": "-eJZ-9l2l2z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Customer-Transaction Enrichment (Hard)**\n",
        "\n",
        "Scenario: Create a master dataset showing each transaction enriched with customer demographics. Optimize for:\n",
        "\n",
        "Fast joins using broadcast join where appropriate\n",
        "\n",
        "Handling null values in customer data\n",
        "\n",
        "Preserving original transaction order*\n",
        "\n",
        "```\n",
        "# Target schema\n",
        "# |-- txn_id: string\n",
        "# |-- date: string\n",
        "# |-- expense_type: string\n",
        "# |-- amt: double\n",
        "# |-- city: string\n",
        "# |-- name: string\n",
        "# |-- age: int\n",
        "# |-- gender: string\n",
        "```"
      ],
      "metadata": {
        "id": "eUCBlAFRmFYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df_for_join = transactions_df.select(\"txn_id\", \"cust_id\", \"date\", \"expense_type\", \"amt\", F.col(\"city\").alias(\"txn_city\"))\n",
        "\n",
        "customers_df_for_join = customers_df.select(\"cust_id\", \"name\", \"age\", \"gender\")\n",
        "\n",
        "master_df = transactions_df_for_join.join(F.broadcast(customers_df_for_join),\n",
        "                              customers_df_for_join.cust_id == transactions_df_for_join.cust_id,\n",
        "                              how='inner')\n",
        "\n",
        "master_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGeEEjFrlm7P",
        "outputId": "a3ddaa88-b893-46c2-b1bd-4a8dec214fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|         txn_id|   cust_id|      date| expense_type|   amt|   txn_city|   cust_id|    name|age|gender|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "|TZ5SMKZY9S03OQJ|C0YDPQWPBJ|2018-10-07|Entertainment| 10.42|     boston|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYIAPPNU066CJ5R|C0YDPQWPBJ|2016-03-27| Motor/Travel| 44.34|   portland|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TETSXIK4BLXHJ6W|C0YDPQWPBJ|2011-04-11|Entertainment|  3.18|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TQKL1QFJY3EM8LO|C0YDPQWPBJ|2018-02-22|    Groceries|268.97|los_angeles|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "|TYL6DFP09PPXMVB|C0YDPQWPBJ|2010-10-16|Entertainment|  2.66|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|\n",
            "+---------------+----------+----------+-------------+------+-----------+----------+--------+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Window Function Analysis (Hard)**\n",
        "\n",
        "Scenario: For customer retention analysis, calculate:\n",
        "\n",
        "a) Days since previous transaction per customer\n",
        "\n",
        "b) Rolling 30-day average spend per customer\n",
        "\n",
        "Use appropriate window functions and handle partition boundaries."
      ],
      "metadata": {
        "id": "c2zTCFvQnzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Days since previous transaction per customer\n",
        "# I'm assuming we are comparing it to today's date\n",
        "\n",
        "# group by cust_id, get last transaction date, check the difference between today's date and last transaction date\n",
        "transactions_df.groupBy(\"cust_id\").agg(F.max(\"date\").alias(\"last_transaction_date\"))\\\n",
        "                .withColumn(\"days_since_last_txn\", F.date_diff(F.current_date(), F.col(\"last_transaction_date\")))\\\n",
        "                .show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8UwdQykmWlA",
        "outputId": "24b30189-85be-4502-c765-87d27db871be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+-------------------+\n",
            "|   cust_id|last_transaction_date|days_since_last_txn|\n",
            "+----------+---------------------+-------------------+\n",
            "|C007YEYTX9|           2020-09-27|               1662|\n",
            "|C00B971T1J|           2020-12-27|               1571|\n",
            "|C00WRSJF1Q|           2020-12-27|               1571|\n",
            "|C01AZWQMF3|           2019-03-27|               2212|\n",
            "|C01BKUFRHA|           2020-09-27|               1662|\n",
            "+----------+---------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'date' is in timestamp format\n",
        "transactions_df = transactions_df.withColumn(\"date_ts\", F.col(\"date\").cast(\"timestamp\"))\n",
        "\n",
        "# Create a time-based window: last 30 days per customer\n",
        "window_30_days = Window.partitionBy(\"cust_id\")\\\n",
        "                       .orderBy(F.col(\"date_ts\").cast(\"long\"))\\\n",
        "                       .rangeBetween(-30 * 86400, 0)  # last 30 days in seconds\n",
        "\n",
        "# First aggregate daily total per customer\n",
        "daily_total_df = transactions_df.groupBy(\"cust_id\", \"date_ts\").agg(F.sum(\"amt\").alias(\"daily_total\"))\n",
        "\n",
        "# Apply rolling average over the 30-day window\n",
        "result_df = daily_total_df.withColumn(\"rolling_30_days_avg\", F.avg(\"daily_total\").over(window_30_days))\n",
        "\n",
        "result_df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_47U4Io6Ne",
        "outputId": "3cd50e4e-4ea3-4867-fb14-97022b949e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-----------+-------------------+\n",
            "|   cust_id|            date_ts|daily_total|rolling_30_days_avg|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "|C007YEYTX9|2012-02-01 00:00:00|      74.62|              74.62|\n",
            "|C007YEYTX9|2012-02-02 00:00:00|     293.11|            183.865|\n",
            "|C007YEYTX9|2012-02-03 00:00:00|      146.7|  171.4766666666667|\n",
            "|C007YEYTX9|2012-02-04 00:00:00|     3647.9|          1040.5825|\n",
            "|C007YEYTX9|2012-02-05 00:00:00|     261.46|            884.758|\n",
            "+----------+-------------------+-----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš© **Day 2 - 2025/04/16**"
      ],
      "metadata": {
        "id": "lK5enpK7rJAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Incremental Data Load Strategy (Medium)**\n",
        "\n",
        "Scenario: New transactions arrive daily. Write a PySpark job to:\n",
        "\n",
        "Load only new transactions (those with date > last_processed_date)\n",
        "\n",
        "Deduplicate using txn_id (keep latest record if duplicates exist)\n",
        "\n",
        "Update a master transactions table without reprocessing old data"
      ],
      "metadata": {
        "id": "cBpxXOAgukih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# latest transaction date in dataset is 2020-12-27\n",
        "\n",
        "# let's assume we only want to process records after 2020-09-01 and none before it\n",
        "\n",
        "# transactions_df is the raw data set that keeps populating everyday\n",
        "# master_transactions is the cleaned version of transactions_df with column last_processed_date\n",
        "# so we check what was the last_processed_date and filter using that value in the transactions_df\n",
        "\n",
        "\n",
        "# Cast date column to date type\n",
        "transactions_df = transactions_df.withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
        "\n",
        "# Define last processed date\n",
        "last_processed_date = \"2020-09-01\" # or max value from master_transactions\n",
        "\n",
        "# Filter only new transactions\n",
        "new_transactions = transactions_df.filter(F.col(\"date\") > F.lit(last_processed_date))\n",
        "\n",
        "# Deduplicate on txn_id by keeping the latest record per txn_id\n",
        "window_spec = Window.partitionBy(\"txn_id\").orderBy(F.col(\"date\").desc())\n",
        "\n",
        "deduped_new_txns = new_transactions.withColumn(\"rank\", F.row_number().over(window_spec))\\\n",
        "                                   .filter(F.col(\"rank\") == 1)\\\n",
        "                                   .drop(\"rank\")\n",
        "\n",
        "# Load master table as example\n",
        "master_transactions = transactions_df.filter(F.col(\"date\") <= F.lit(last_processed_date))\n",
        "\n",
        "# Append new deduped data to master\n",
        "updated_master = master_transactions.unionByName(deduped_new_txns)\n",
        "\n",
        "updated_master.orderBy(\"date\", ascending=False).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25srmM2fqnVA",
        "outputId": "924b80e4-2ef0-4205-c54d-5d8bd886446a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "|   cust_id|start_date|end_date|         txn_id|      date|year|month|day| expense_type|  amt|         city|            date_ts|\n",
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "|CO0JUS2B89|2013-03-01|    NULL|T09MG32WEEJYDTX|2020-12-27|2020|   12| 27|    Groceries|42.43|     portland|2020-12-27 00:00:00|\n",
            "|C4XSZX5AEQ|2012-11-01|    NULL|T093856A0IX0QKY|2020-12-27|2020|   12| 27|Entertainment|26.19|san_francisco|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-10-01|    NULL|T0I1IFIJMRCAPLO|2020-12-27|2020|   12| 27|Entertainment|25.56|    san_diego|2020-12-27 00:00:00|\n",
            "|CW57H4XRF4|2012-12-01|    NULL|T0INHCOKS6AYENZ|2020-12-27|2020|   12| 27|Entertainment|25.18|       boston|2020-12-27 00:00:00|\n",
            "|CRO4QOP409|2013-12-01|    NULL|T0A1STBKX7DM2GK|2020-12-27|2020|   12| 27|Entertainment|76.77|     new_york|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T09G6U9ZBQZVR99|2020-12-27|2020|   12| 27|Entertainment|27.16|       boston|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2012-12-01|    NULL|T00Z5JVTTS0F6TG|2020-12-27|2020|   12| 27|Entertainment|13.16|     new_york|2020-12-27 00:00:00|\n",
            "|CLXYTOOXJK|2013-04-01|    NULL|T00FL1TZRI4NWD7|2020-12-27|2020|   12| 27|Entertainment| 9.29|     new_york|2020-12-27 00:00:00|\n",
            "|CFUHOFZSHE|2013-02-01|    NULL|T0AB1WL6UO1KUK4|2020-12-27|2020|   12| 27|Entertainment|29.22| philadelphia|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-11-01|    NULL|T09O73JJBCDVV7W|2020-12-27|2020|   12| 27|Entertainment| 12.2|    san_diego|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-07-01|    NULL|T01Q2Y3BYA6AO3M|2020-12-27|2020|   12| 27| Motor/Travel|98.11|san_francisco|2020-12-27 00:00:00|\n",
            "|CQU9LG15ON|2013-05-01|    NULL|T01CDZ526V706VD|2020-12-27|2020|   12| 27|Entertainment| 8.73|     portland|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T0AME7FHS70MZFS|2020-12-27|2020|   12| 27|Entertainment|33.98|       boston|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-07-01|    NULL|T0ACN20D7KTUQ24|2020-12-27|2020|   12| 27|Entertainment|18.62|    san_diego|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2011-09-01|    NULL|T02JY52IQQ0UT68|2020-12-27|2020|   12| 27|Entertainment|36.99|  los_angeles|2020-12-27 00:00:00|\n",
            "|C0YDPQWPBJ|2012-08-01|    NULL|T033GDLC4H7Z85Y|2020-12-27|2020|   12| 27|Entertainment|16.49|san_francisco|2020-12-27 00:00:00|\n",
            "|CHNFNR89ZV|2012-02-01|    NULL|T0ANZS80ILS4DQV|2020-12-27|2020|   12| 27|Entertainment|10.92|     portland|2020-12-27 00:00:00|\n",
            "|CFWOR4F86Q|2012-11-01|    NULL|T0AYLHF0P9EZEL2|2020-12-27|2020|   12| 27|Entertainment| 7.27|  los_angeles|2020-12-27 00:00:00|\n",
            "|CG8UBUD4V3|2013-07-01|    NULL|T04UOY0L3S9NM2E|2020-12-27|2020|   12| 27|    Groceries|193.2|       boston|2020-12-27 00:00:00|\n",
            "|CDZ683BJL4|2012-11-01|    NULL|T03AKK8R2MZ7Z8U|2020-12-27|2020|   12| 27|    Groceries|68.84|     new_york|2020-12-27 00:00:00|\n",
            "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-----+-------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Geospatial UDF Optimization (Hard)**\n",
        "Scenario: Calculate distances between customer ZIP codes and transaction cities.\n",
        "\n",
        "Create a UDF to convert ZIP codes to latitude/longitude (mock this)"
      ],
      "metadata": {
        "id": "iCz4uuoYxwwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lat_long(zip_code):\n",
        "  zip_int = int(str(zip_code).zfill(5))  # pad zeros if needed\n",
        "  latitude = 30.0 + (zip_int % 1000) * 0.01\n",
        "  longitude = -100.0 - (zip_int % 1000) * 0.01\n",
        "  return (latitude, longitude)\n",
        "\n",
        "udf_schema = StructType([StructField(\"latitude\", DoubleType()),\n",
        "                         StructField(\"longitude\", DoubleType())\n",
        "                         ])\n",
        "\n",
        "zip_to_lat_long_udf = F.udf(to_lat_long, udf_schema)\n",
        "\n",
        "# Apply the UDF\n",
        "df_with_coords = customers_df.withColumn(\"coords\", zip_to_lat_long_udf(F.col(\"zip\")))\n",
        "\n",
        "# Split into separate lat/lon columns\n",
        "df_with_coords = df_with_coords.withColumn(\"latitude\", F.col(\"coords.latitude\"))\\\n",
        "                               .withColumn(\"longitude\", F.col(\"coords.longitude\"))\\\n",
        "                               .drop(\"coords\")\n",
        "\n",
        "df_with_coords.show()"
      ],
      "metadata": {
        "id": "G_O_w12ku42w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab462e1-17dd-406a-b396-cf3b5a6c820b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "|   cust_id|          name|age|gender|  birthday|  zip|        city|          latitude|longitude|\n",
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "|C007YEYTX9|  Aaron Abbott| 34|Female| 7/13/1991|97823|      boston|38.230000000000004|  -108.23|\n",
            "|C00B971T1J|  Aaron Austin| 37|Female|12/16/2004|30332|     chicago|             33.32|  -103.32|\n",
            "|C00WRSJF1Q|  Aaron Barnes| 29|Female| 3/11/1977|23451|      denver|             34.51|  -104.51|\n",
            "|C01AZWQMF3| Aaron Barrett| 31|  Male|  7/9/1998|46613| los_angeles|             36.13|  -106.13|\n",
            "|C01BKUFRHA|  Aaron Becker| 54|  Male|11/24/1979|40284|   san_diego|             32.84|  -102.84|\n",
            "|C01RGUNJV9|    Aaron Bell| 24|Female| 8/16/1968|86331|      denver|             33.31|  -103.31|\n",
            "|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|    new_york|             30.78|  -100.78|\n",
            "|C01WMZQ7PN|   Aaron Brady| 51|Female| 8/20/1994|52204|philadelphia|             32.04|  -102.04|\n",
            "|C021567NJZ|  Aaron Briggs| 57|  Male| 3/10/1990|22008|philadelphia|             30.08|  -100.08|\n",
            "|C023M6MKR3|   Aaron Bryan| 29|  Male| 4/10/1976|05915|philadelphia|             39.15|  -109.15|\n",
            "|C0248N0EK3|  Aaron Burton| 26|Female| 8/27/1964|50477| los_angeles|             34.77|  -104.77|\n",
            "|C02C54RPNL|  Aaron Burton| 46|  Male| 5/29/1976|75857|     seattle|             38.57|  -108.57|\n",
            "|C02ERIY1O4|  Aaron Cannon| 50|  Male| 5/23/1965|70209|    portland|             32.09|  -102.09|\n",
            "|C02EVK2JWT|  Aaron Carter| 36|Female| 6/21/1993|89011|      denver|             30.11|  -100.11|\n",
            "|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|    new_york|             33.37|  -103.37|\n",
            "|C030A69V1L|  Aaron Clarke| 55|  Male| 4/28/1999|77176|philadelphia|             31.76|  -101.76|\n",
            "|C033JBNUYU|Aaron Ferguson| 27|  Male| 6/21/1959|73150|      denver|              31.5|   -101.5|\n",
            "|C034RB2MQ6|    Aaron Ford| 63|  Male|  7/8/1988|90592|     chicago|             35.92|  -105.92|\n",
            "|C036GAJ3BV|Aaron Franklin| 22|Female| 3/14/1961|01187|   san_diego|             31.87|  -101.87|\n",
            "|C03U340T3R| Aaron Gardner| 59|Female| 3/18/1975|31502|      denver|             35.02|  -105.02|\n",
            "+----------+--------------+---+------+----------+-----+------------+------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Skewed Join Mitigation (Hard)**\n",
        "\n",
        "Optimize this join between transactions_df and customers_df, where one of the customers has massive amounts of transactions as compared to others\n",
        "\n",
        "```\n",
        "+----------+--------+\n",
        "|   cust_id|   count|\n",
        "+----------+--------+\n",
        "|C0YDPQWPBJ|17539732|\n",
        "|C3KUDEN3KO|    7999|\n",
        "|CBW3FMEAU7|    7999|\n",
        "|C89FCEGPJP|    7999|\n",
        "|CHNFNR89ZV|    7998|\n",
        "+----------+--------+\n",
        "only showing top 5 rows\n",
        "```"
      ],
      "metadata": {
        "id": "RBZJaKx6oDD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first approach to rely on Spark's adaptive query execution to take the best route for the join\n",
        "joined_df_aqe = transactions_df.join(customers_df, customers_df.cust_id == transactions_df.cust_id, how='inner')\n",
        "\n",
        "joined_df_aqe.explain(mode=\"formatted\")\n",
        "joined_df_aqe.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-S28DZroHNr",
        "outputId": "871b0291-3ce9-48e3-a10d-e0e86fba9737"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (7)\n",
            "+- BroadcastHashJoin Inner BuildRight (6)\n",
            "   :- Filter (2)\n",
            "   :  +- Scan parquet  (1)\n",
            "   +- BroadcastExchange (5)\n",
            "      +- Filter (4)\n",
            "         +- Scan parquet  (3)\n",
            "\n",
            "\n",
            "(1) Scan parquet \n",
            "Output [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/transactions.parquet]\n",
            "PushedFilters: [IsNotNull(cust_id)]\n",
            "ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,month:string,day:string,expense_type:string,amt:string,city:string>\n",
            "\n",
            "(2) Filter\n",
            "Input [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Condition : isnotnull(cust_id#0)\n",
            "\n",
            "(3) Scan parquet \n",
            "Output [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/customers.parquet]\n",
            "PushedFilters: [IsNotNull(cust_id)]\n",
            "ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
            "\n",
            "(4) Filter\n",
            "Input [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Condition : isnotnull(cust_id#22)\n",
            "\n",
            "(5) BroadcastExchange\n",
            "Input [7]: [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=103]\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [cust_id#0]\n",
            "Right keys [1]: [cust_id#22]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) AdaptiveSparkPlan\n",
            "Output [18]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|   cust_id|    name|age|gender| birthday|  zip|  city|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|C0YDPQWPBJ|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+----------+--------+---+------+---------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second approach we can use salting\n",
        "\n",
        "# Add salt only for the skewed customer\n",
        "num_salts = 10  # number of salts for distributing skew\n",
        "\n",
        "transactions_df_salted = transactions_df.withColumn(\n",
        "    \"salt\",\n",
        "    F.when(F.col(\"cust_id\") == \"C0YDPQWPBJ\", F.floor(F.rand(seed=42) * num_salts).cast(\"int\"))\n",
        "    .otherwise(F.lit(None))\n",
        ")"
      ],
      "metadata": {
        "id": "bKdmCIj_oUnH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Expand the skewed customer row\n",
        "skewed_rows = customers_df.filter(F.col(\"cust_id\") == \"C0YDPQWPBJ\").collect()\n",
        "salted_customers = []\n",
        "\n",
        "for row in skewed_rows:\n",
        "    for salt_val in range(num_salts):\n",
        "        row_dict = row.asDict()\n",
        "        row_dict[\"salt\"] = salt_val\n",
        "        salted_customers.append(Row(**row_dict))\n",
        "\n",
        "# Combine with rest of customers (unsalted)\n",
        "non_skewed_customers = customers_df.filter(F.col(\"cust_id\") != \"C0YDPQWPBJ\")\n",
        "salted_skewed_customers_df = spark.createDataFrame(salted_customers)\n",
        "\n",
        "# Add salt column with null to non-skewed customers\n",
        "non_skewed_customers = non_skewed_customers.withColumn(\"salt\", F.lit(None).cast(\"int\"))\n",
        "\n",
        "customers_df_salted = non_skewed_customers.unionByName(salted_skewed_customers_df)\n",
        "\n",
        "# Join using both cust_id and salt\n",
        "joined_df_salted = transactions_df_salted.join(\n",
        "    customers_df_salted,\n",
        "    on=[\"cust_id\", \"salt\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "joined_df_salted.explain(mode=\"formatted\")\n",
        "joined_df_salted.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSJlDiJ6qVoU",
        "outputId": "e54be566-a634-4c6a-9733-4a99df19ba8f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (12)\n",
            "+- Project (11)\n",
            "   +- SortMergeJoin Inner (10)\n",
            "      :- Sort (5)\n",
            "      :  +- Exchange (4)\n",
            "      :     +- Filter (3)\n",
            "      :        +- Project (2)\n",
            "      :           +- Scan parquet  (1)\n",
            "      +- Sort (9)\n",
            "         +- Exchange (8)\n",
            "            +- Filter (7)\n",
            "               +- Scan ExistingRDD (6)\n",
            "\n",
            "\n",
            "(1) Scan parquet \n",
            "Output [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-experiments/data/data_skew/transactions.parquet]\n",
            "ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,month:string,day:string,expense_type:string,amt:string,city:string>\n",
            "\n",
            "(2) Project\n",
            "Output [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, CASE WHEN (cust_id#0 = C0YDPQWPBJ) THEN cast(FLOOR((rand(42) * 10.0)) as int) END AS salt#325]\n",
            "Input [11]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10]\n",
            "\n",
            "(3) Filter\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Condition : (isnotnull(cust_id#0) AND isnotnull(salt#325))\n",
            "\n",
            "(4) Exchange\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Arguments: hashpartitioning(cust_id#0, cast(salt#325 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=257]\n",
            "\n",
            "(5) Sort\n",
            "Input [12]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325]\n",
            "Arguments: [cust_id#0 ASC NULLS FIRST, cast(salt#325 as bigint) ASC NULLS FIRST], false, 0\n",
            "\n",
            "(6) Scan ExistingRDD\n",
            "Output [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L], MapPartitionsRDD[53] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
            "\n",
            "(7) Filter\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Condition : (isnotnull(cust_id#370) AND isnotnull(salt#377L))\n",
            "\n",
            "(8) Exchange\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: hashpartitioning(cust_id#370, salt#377L, 200), ENSURE_REQUIREMENTS, [plan_id=258]\n",
            "\n",
            "(9) Sort\n",
            "Input [8]: [cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "Arguments: [cust_id#370 ASC NULLS FIRST, salt#377L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(10) SortMergeJoin\n",
            "Left keys [2]: [cust_id#0, cast(salt#325 as bigint)]\n",
            "Right keys [2]: [cust_id#370, salt#377L]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(11) Project\n",
            "Output [18]: [cust_id#0, salt#325, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#371, age#372, gender#373, birthday#374, zip#375, city#376]\n",
            "Input [20]: [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, salt#325, cust_id#370, name#371, age#372, gender#373, birthday#374, zip#375, city#376, salt#377L]\n",
            "\n",
            "(12) AdaptiveSparkPlan\n",
            "Output [18]: [cust_id#0, salt#325, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#371, age#372, gender#373, birthday#374, zip#375, city#376]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "|   cust_id|salt|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|    name|age|gender| birthday|  zip|  city|\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97| los_angeles|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2011-07-01|2020-09-01|T4UTFAGFKIGZGKG|2019-02-10|2019|    2| 10|Entertainment|   6.8|philadelphia|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TSA7RJ3GSQSG4KX|2012-11-17|2012|   11| 17|Entertainment|   2.5|     chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2011-07-01|2020-09-01|TAQMZO2U0MBT9K9|2013-02-01|2013|    2|  1|    Groceries| 31.57|    new_york|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "|C0YDPQWPBJ|   2|2010-07-01|2018-12-01|TODEIOF2REW2GKB|2017-03-25|2017|    3| 25|     Gambling| 164.5|    new_york|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
            "+----------+----+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+--------+---+------+---------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "joined_df_aqe.cache().count()\n",
        "print(\"AQE Join Time:\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "joined_df_salted.cache().count()\n",
        "print(\"Salted Join Time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auRBHKGZquWJ",
        "outputId": "3ecfd636-489e-4649-b475-3c3b0fd56cd7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AQE Join Time: 428.58775758743286\n",
            "Salted Join Time: 256.59440326690674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2bFm2-_rVQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}