{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91cf85ff-2060-458e-9ae5-2db125c7f6f2",
   "metadata": {},
   "source": [
    "# Spark Executor Tuning\n",
    "***How do you decide how many executors you need, and what configuration?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a135f-fe1e-4e52-bfb0-4413c6b6ba7f",
   "metadata": {},
   "source": [
    "```bash\n",
    "spark-submit \\\n",
    "  --class org.example.MySparkApp \\ # Main class for Java/Scala applications\n",
    "  --master yarn \\ # Spark master URL (e.g., yarn, local[*])\n",
    "  --deploy-mode cluster \\ # Whether to deploy in client or cluster mode\n",
    "  --num-executors 5 \\\n",
    "  --executor-memory 2G \\ # Memory per executor (e.g., 2G)\n",
    "  --executor-cores 2 \\  # Number of cores per executor\n",
    "  myapp.jar 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebf571-12ca-4198-b4b3-c06fc575e85e",
   "metadata": {},
   "source": [
    "**3 Main things to decide**\n",
    "-  No. of executors\n",
    "-  No. of cores within each\n",
    "-  RAM of each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad1d6c-43e1-4a76-b737-3d1d7096d26e",
   "metadata": {},
   "source": [
    "***Per Node Leave out 1 Core and 1GB for Hadoop/YARN/OS Daemons***\n",
    "\n",
    "Let's work with this scenario. \n",
    "- You have 5 nodes\n",
    "- Each node has 12 cores and 48GB RAM (Yup super powerful)\n",
    "  \n",
    "### 1. Fat Executors\n",
    "- Occupies large proportion of resources on a Node\n",
    "- The no. of executors in this case would be 1 per node. 1 executor on the node will occupy all the 11 cores and 47GB of RAM (we left out 1 core and 1 GB)\n",
    "- Therefore total no. of executors = 5\n",
    "- **Pros**\n",
    "    - Increased task-level parallelism\n",
    "    - Enhanced data locality - reduces chances of shuffle; reduces network traffic as you don't need to move data\n",
    "- **Cons**\n",
    "    - Under utilisation leads to wastage and excessive costs\n",
    "    - Failure of single node or executor can lead to significant impact. Recovery would be slower\n",
    "    - Rate of read data from HDFS or write data to HDFS (HDFS Throughput) might suffer. Fat executors can lead to execessive garbage collection (during GC application needs to be paused) \n",
    "\n",
    "### 2. Thin Executors\n",
    "- The opposite of fat executors is thin, they occupy the min resources\n",
    "- So in each node you would have 11 executors each with 1 core and ~4 GB RAM (47/11).\n",
    "- Total no. of executors = 11*5 = 55\n",
    "- **Pros**\n",
    "    - Increased executor-level parallelism -> Great for processing small jobs\n",
    "    - Fault tolerant -> since smaller amount of work is done per executor, it can easily restart \n",
    "- **Cons**\n",
    "    - Reduced data locality -> No. of partitions of data within each executor would be limited\n",
    "    - High network traffic -> needs to bring in data from different locations\n",
    " \n",
    "### 3. Optimal Executors\n",
    "##### How to Create an Optimal Executor --> 4 Rules\n",
    "1. Leave out 1 Core and 1 GB of RAM for Hadoop/YARN/OS Daemons per Node\n",
    "2. Leave out 1 Executor or (1Core, 1GB Ram) YARN Application Manager at Cluster-level\n",
    "3. General rule of thumb is 3-5 Cores per executors, more than can lead to execessive garbage collection\n",
    "4. Spare out some memory for Memory Overhead => max(384MB, 10% of executor memory)\n",
    "\n",
    "- If we follow the above rules and try to create an optimal executor\n",
    "    - 5 Nodes; 12 Cores; 48 RAM;\n",
    "    -  Leaving out 1 Core and 1 GB of RAM for Hadoop/YARN/OS Daemons per Node.\n",
    "    -  Therefore,\n",
    "        -  total memory = 47*5 = 235GB;\n",
    "        -  total cores = 11*5 = 55 Cores\n",
    "    - Leaving out 1Core, 1GB Ram YARN Application Manager at Cluster-level =>\n",
    "        -  total memory left = 234GB;\n",
    "        -  total cores left = 54 Cores\n",
    "    - So let's take 5 Cores (i.e. tasks) per executors\n",
    "        - total no. of executors = 54/4 ~ 10 Executors with 5 Cores\n",
    "        - Memory Per Executors = 234/10 ~ 23 GB\n",
    "    - Leaving out some memory for Memory Overhead => max(384MB, 0.1*23GB) = 2.3GB\n",
    "        - Actual memory per executor = 23GB - 2.3GB ~ 20GB\n",
    "    - Total memory per executor = 20 GB\n",
    "        - Execution memory (60%) = 20 GB × 0.6 = 12 GB (We can assume dynamic allocation)\n",
    "        - Storage memory (40%) = 20 GB × 0.4 = 8 GB\n",
    "        - Execution memory per core = 12 GB ÷ 5 cores = 2.4 GB per core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f641c0-161c-45b6-8439-9df71a45c6fa",
   "metadata": {},
   "source": [
    "### Why haven't we considered the size of the data?\n",
    "- While talking about the size of the data we need to talk about the size of the partition\n",
    "- Because each core can process 1 partition at a time\n",
    "- taking the optimal executor configuration, we have 4GB Per Core (20/5). So as long as the partition of data is less than 4GB then we should be fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406feaa3-8fea-45ae-85a5-cdb4d4cc6a52",
   "metadata": {},
   "source": [
    "spark cluster size -- 200 cores and 100 gb RAM\r\n",
    "data to be processed --100 gb\r\n",
    "give the calculation of spark for driver memory, driver cores, executor memory, overhead memory, number of executo\n",
    "\n",
    "\n",
    "--- \n",
    "\r\n",
    "\r\n",
    "**Rule #1: For Hadoop/YARN/OS Daemons**\r\n",
    "- Assuming this is a cluster with 10 nodes (200 cores ÷ 20 cores per node = 10 nodes)\r\n",
    "- Reserve per node: 1 core and 1 GB\r\n",
    "- Total reserved: 10 cores and 10 GB\r\n",
    "\r\n",
    "**Rule #2: For YARN Application Manager**\r\n",
    "- Reserve: 1 core and 1 GB\r\n",
    "\r\n",
    "So available resources:\r\n",
    "- Cores: 200 - 10 - 1 = 189 cores\r\n",
    "- Memory: 100 GB - 10 GB - 1 GB = 89 GB\r\n",
    "\r\n",
    "Now let's configure using remaining rules:\r\n",
    "\r\n",
    "**Rule #3: Use 3-5 cores per executor**\r\n",
    "- Let's use 4 (3-5 best practice) cores per executor\r\n",
    "- Number of executors = 189 ÷ 4 ≈ 47 executors\r\n",
    "\r\n",
    "**Rule #4: Calculate memory per executor and overhead**\r\n",
    "- Available memory per executor before overhead = 89 GB ÷ 47 ≈ 1.89 GB\r\n",
    "- Memory overhead = max(384MB, 10% of 1.89 GB) = max(384MB, 189MB) = 384MB\r\n",
    "- Final executor memory =tions n thecount for all system reservations\r\n",
    "\r\n",
    "Given your data size of 100 GB, each executor would handle:\r\n",
    "100 GB ÷ 47 ≈ 2.13 GB of data\r\n",
    "Per core: 2.13 GB ÷ 4 ≈ 532 MB, which down any part of these calculations in more detail?nfigurations based on specific workload characteristics?rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd7205-2dee-4cd0-b4eb-c804528b6c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
