{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ffbc9e-fb41-4ae7-80e7-00be68d931ac",
   "metadata": {},
   "source": [
    "# Bucketing\n",
    "- Dividing the data into manageable chunks for better performance, similar to partitioning\n",
    "- Main formula for bucketing\n",
    "  - $hash(bucketingColValue)mod(no. of buckets)$\n",
    "  - hash function gives a large value and with the mod function it can be brought within the limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f35e8c-17c0-4767-8dc2-60e4ba6abfd1",
   "metadata": {},
   "source": [
    "## How Bucketing Helps in Joins\n",
    "- If you store your data without partitioning or bucketing, join operations will typically involve **shuffle, sort, and merge** steps, which are computationally expensive due to the need to redistribute and sort data across nodes.\n",
    "\n",
    "- If you partition your data on a high cardinality column, it can lead to the **small file problem**, where each partition ends up being very small and inefficient to process\n",
    "\n",
    "- To avoid the limitations of partitioning, you can bucket your data by specifying a fixed number of buckets. Bucketing works well for join optimization, especially for large datasets.\n",
    "    - Bucketing works by applying a hash function to the column used for bucketing (e.g., `product_id`). The hash function ensures that the same value (e.g., `product_id = 1`) always maps to the same bucket. As a result, rows with the same `product_id` from two tables (e.g., `Orders` and `Products`) will be stored in the same bucket. This eliminates the need for shuffling during a join operation, significantly improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f951e05-502b-4038-af9a-88f599d31b79",
   "metadata": {},
   "source": [
    "### Optimal Conditions for Joins on Bucketed Tables\n",
    "1. If both tables are bucketed on the same key and have the same number of buckets => **No shuffle**\n",
    "2. If both tables are bucketed on the same key but have different number of buckets => **One of the tables would be suffled**\n",
    "3. If both tables are bucketed but on different keys even though they have same number of buckets => **Full Shuffle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237111d3-69e0-4eff-b06e-6d7c981c8101",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "## How to Decide the Number of Buckets?\r\n",
    "\r\n",
    "- The **optimal bucket size** is typically between **128 MB and 200 MB**, depending on your system's configuration and the underlying file system (e.g., HDFS).\r\n",
    "\r\n",
    "- To calculate the **number of buckets**, use the formula:  \r\n",
    "  \\[\r\n",
    "  \\text{Number of Buckets} = \\frac{\\text{Size of Dataset (in MB)}}{\\text{Optimal Bucket Size (in Mnteg#er.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Example:\r\n",
    "If your dataset is 10 GB (10,240 MB) and the optimal bucket size is 200 MB:\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{Number of Buckets} = \\frac{10,240}{200} = 51.2\r\n",
    "\\]\r\n",
    "\r\n",
    "You would set the number of buckets to **51**. \r\n",
    "\r\n",
    "This ensures that each bucket is sized appropriately for efficient processing while avoiding overhead caused by excessively small or large buckets.sively small or large buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22faf0e9-2586-4daf-91ad-932f631c936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eee4063-3d28-4d01-8863-90e837612736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(\"spark.driver.memory\",\"10g\")\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"bucketing\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce753789-5cea-4117-a266-a11bce1af115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|order_id|product_id|customer_id|quantity|         order_date|total_amount|\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|       1|        80|         10|       4|2023-03-20 00:00:00|        1003|\n",
      "|       2|        69|         30|       3|2023-12-11 00:00:00|         780|\n",
      "|       3|        61|         20|       4|2023-04-26 00:00:00|        1218|\n",
      "|       4|        62|         44|       3|2023-08-26 00:00:00|        2022|\n",
      "|       5|        78|         46|       4|2023-08-05 00:00:00|        1291|\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_file = \"orders.csv\"\n",
    "df_orders = spark.read.csv(orders_file, header=True, inferSchema=True)\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3accf8f1-5137-422c-a8da-c71fca090985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|product_id|product_name|   category|  brand|price|stock|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|         1|   Product_1|Electronics|Brand_4|   26|  505|\n",
      "|         2|   Product_2|    Apparel|Brand_4|  489|   15|\n",
      "|         3|   Product_3|    Apparel|Brand_4|  102|  370|\n",
      "|         4|   Product_4|  Groceries|Brand_1|   47|  433|\n",
      "|         5|   Product_5|  Groceries|Brand_3|  244|  902|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_file = \"products.csv\"\n",
    "df_products = spark.read.csv(products_file, header=True, inferSchema=True)\n",
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70506daf-6334-4ee9-b253-7f6790b99caa",
   "metadata": {},
   "source": [
    "## Comparing Join Operation with and w/o Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e9e3c-24bf-429f-8013-03e8359d5612",
   "metadata": {},
   "source": [
    "### W/O Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d3e4035-b742-4f17-aab7-a03fd1b1666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set broadcast off for now\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c40febd-8cb6-4ee2-a861-154799fb4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- SortMergeJoin Inner (9)\n",
      "      :- Sort (4)\n",
      "      :  +- Exchange (3)\n",
      "      :     +- Filter (2)\n",
      "      :        +- Scan csv  (1)\n",
      "      +- Sort (8)\n",
      "         +- Exchange (7)\n",
      "            +- Filter (6)\n",
      "               +- Scan csv  (5)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [6]: [order_id#17, product_id#18, customer_id#19, quantity#20, order_date#21, total_amount#22]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/orders.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [order_id#17, product_id#18, customer_id#19, quantity#20, order_date#21, total_amount#22]\n",
      "Condition : isnotnull(product_id#18)\n",
      "\n",
      "(3) Exchange\n",
      "Input [6]: [order_id#17, product_id#18, customer_id#19, quantity#20, order_date#21, total_amount#22]\n",
      "Arguments: hashpartitioning(product_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=118]\n",
      "\n",
      "(4) Sort\n",
      "Input [6]: [order_id#17, product_id#18, customer_id#19, quantity#20, order_date#21, total_amount#22]\n",
      "Arguments: [product_id#18 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan csv \n",
      "Output [6]: [product_id#78, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "\n",
      "(6) Filter\n",
      "Input [6]: [product_id#78, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Condition : isnotnull(product_id#78)\n",
      "\n",
      "(7) Exchange\n",
      "Input [6]: [product_id#78, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Arguments: hashpartitioning(product_id#78, 200), ENSURE_REQUIREMENTS, [plan_id=119]\n",
      "\n",
      "(8) Sort\n",
      "Input [6]: [product_id#78, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Arguments: [product_id#78 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [product_id#18]\n",
      "Right keys [1]: [product_id#78]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [11]: [product_id#18, order_id#17, customer_id#19, quantity#20, order_date#21, total_amount#22, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Input [12]: [order_id#17, product_id#18, customer_id#19, quantity#20, order_date#21, total_amount#22, product_id#78, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [11]: [product_id#18, order_id#17, customer_id#19, quantity#20, order_date#21, total_amount#22, product_name#79, category#80, brand#81, price#82, stock#83]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.join(df_products, on=\"product_id\", how=\"inner\").explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25480464-7c5a-4583-bb5b-21bfaccacee8",
   "metadata": {},
   "source": [
    "***Steps 3 and 7 show the two full shuffles***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01523e1d-50f7-436b-86af-b6189e02c5c3",
   "metadata": {},
   "source": [
    "### With Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad5e7a5-dd23-4f6b-b5cd-dacc54217a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.write.bucketBy(4, col=\"product_id\").mode(\"overwrite\").saveAsTable(\"orders_buckected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d3cf72-1c6f-4888-b902-959cd0e7a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.write.bucketBy(4, col=\"product_id\").mode(\"overwrite\").saveAsTable(\"products_buckected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890a4151-d068-4f7b-96eb-94e631082af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bucketed = spark.table(\"orders_buckected\")\n",
    "df_products_bucketed = spark.table(\"products_buckected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ba3446-1226-47a1-a2d1-b41adf4cde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- Project (8)\n",
      "   +- SortMergeJoin Inner (7)\n",
      "      :- Sort (3)\n",
      "      :  +- Filter (2)\n",
      "      :     +- Scan parquet spark_catalog.default.orders_buckected (1)\n",
      "      +- Sort (6)\n",
      "         +- Filter (5)\n",
      "            +- Scan parquet spark_catalog.default.products_buckected (4)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.orders_buckected\n",
      "Output [6]: [order_id#209, product_id#210, customer_id#211, quantity#212, order_date#213, total_amount#214]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/spark-warehouse/orders_buckected]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount:int>\n",
      "SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [order_id#209, product_id#210, customer_id#211, quantity#212, order_date#213, total_amount#214]\n",
      "Condition : isnotnull(product_id#210)\n",
      "\n",
      "(3) Sort\n",
      "Input [6]: [order_id#209, product_id#210, customer_id#211, quantity#212, order_date#213, total_amount#214]\n",
      "Arguments: [product_id#210 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(4) Scan parquet spark_catalog.default.products_buckected\n",
      "Output [6]: [product_id#221, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/spark-warehouse/products_buckected]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "(5) Filter\n",
      "Input [6]: [product_id#221, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "Condition : isnotnull(product_id#221)\n",
      "\n",
      "(6) Sort\n",
      "Input [6]: [product_id#221, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "Arguments: [product_id#221 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(7) SortMergeJoin\n",
      "Left keys [1]: [product_id#210]\n",
      "Right keys [1]: [product_id#221]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [11]: [product_id#210, order_id#209, customer_id#211, quantity#212, order_date#213, total_amount#214, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "Input [12]: [order_id#209, product_id#210, customer_id#211, quantity#212, order_date#213, total_amount#214, product_id#221, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [11]: [product_id#210, order_id#209, customer_id#211, quantity#212, order_date#213, total_amount#214, product_name#222, category#223, brand#224, price#225, stock#226]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_bucketed.join(df_products_bucketed,\n",
    "                        on=\"product_id\", \n",
    "                        how=\"inner\").explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e498a7-6041-495e-b243-a674c6549acb",
   "metadata": {},
   "source": [
    "***Voila! No Exchange?!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f832ee-4573-4f5f-9330-2628016b3889",
   "metadata": {},
   "source": [
    "### Summary: Comparing Join Operations With and Without Bucketing\n",
    "\n",
    "When comparing join operations in Spark with and without bucketing, the key differences lie in the handling of data shuffling and the efficiency of the execution plan.\n",
    "\n",
    "#### **Without Bucketing**\n",
    "- The join operation involves **shuffle-sort-merge**, which is resource-intensive due to the following:\n",
    "  - **Exchange**: Data is shuffled across the cluster based on the join key (`product_id`).\n",
    "  - **Sort**: Both datasets are sorted after the shuffle to enable the merge.\n",
    "  - The physical plan shows multiple stages, including **Exchange** and **Sort** steps, making the process more expensive in terms of time and computation.\n",
    "\n",
    "#### **With Bucketing**\n",
    "- When data is bucketed on the join key (`product_id`):\n",
    "  - Both datasets are pre-partitioned into the same number of buckets, ensuring that matching keys are stored in the same bucket.\n",
    "  - The **Exchange** step is eliminated as there is no need for data shuffling during the join.\n",
    "  - The physical plan is simplified, showing only the **SortMergeJoin**, with the absence of **Exchange**, resulting in a more efficient execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13d768-9579-46a4-8539-0c21f9703064",
   "metadata": {},
   "source": [
    "## Comparing Aggregation Operation with and w/o Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8c551-5547-42ba-86f7-2aefec53854f",
   "metadata": {},
   "source": [
    "### W/O Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b58206-acfd-4681-b223-ac3064e35670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|total_amount|\n",
      "+----------+------------+\n",
      "|        31|        9994|\n",
      "|        85|       10630|\n",
      "|        65|       12806|\n",
      "|        53|       14410|\n",
      "|        78|       12652|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [product_id#18, total_amount#22]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/orders.csv]\n",
      "ReadSchema: struct<product_id:int,total_amount:int>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [2]: [product_id#18, total_amount#22]\n",
      "Keys [1]: [product_id#18]\n",
      "Functions [1]: [partial_sum(total_amount#22)]\n",
      "Aggregate Attributes [1]: [sum#287L]\n",
      "Results [2]: [product_id#18, sum#288L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [product_id#18, sum#288L]\n",
      "Arguments: hashpartitioning(product_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=317]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [product_id#18, sum#288L]\n",
      "Keys [1]: [product_id#18]\n",
      "Functions [1]: [sum(total_amount#22)]\n",
      "Aggregate Attributes [1]: [sum(total_amount#22)#277L]\n",
      "Results [2]: [product_id#18, sum(total_amount#22)#277L AS total_amount#278L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [product_id#18, total_amount#278L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_grouped = df_orders.groupby(\"product_id\").agg(F.sum(\"total_amount\").alias(\"total_amount\"))\n",
    "df_orders_grouped.show(5)\n",
    "df_orders_grouped.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1f0e1-ee93-4538-ba39-05015f327da8",
   "metadata": {},
   "source": [
    "***There is an exchange involved***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1f877-e184-484f-a01d-0f71f1dd4897",
   "metadata": {},
   "source": [
    "### With Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c90f9247-7f43-4634-9944-a47e32219408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|total_amount|\n",
      "+----------+------------+\n",
      "|        46|       10902|\n",
      "|        65|       12806|\n",
      "|        38|       14283|\n",
      "|        95|       20949|\n",
      "|        12|       17187|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- HashAggregate (3)\n",
      "   +- HashAggregate (2)\n",
      "      +- Scan parquet spark_catalog.default.orders_buckected (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.orders_buckected\n",
      "Output [2]: [product_id#210, total_amount#214]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/spark-warehouse/orders_buckected]\n",
      "ReadSchema: struct<product_id:int,total_amount:int>\n",
      "SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [2]: [product_id#210, total_amount#214]\n",
      "Keys [1]: [product_id#210]\n",
      "Functions [1]: [partial_sum(total_amount#214)]\n",
      "Aggregate Attributes [1]: [sum#312L]\n",
      "Results [2]: [product_id#210, sum#313L]\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [product_id#210, sum#313L]\n",
      "Keys [1]: [product_id#210]\n",
      "Functions [1]: [sum(total_amount#214)]\n",
      "Aggregate Attributes [1]: [sum(total_amount#214)#302L]\n",
      "Results [2]: [product_id#210, sum(total_amount#214)#302L AS total_amount#303L]\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [2]: [product_id#210, total_amount#303L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_bucketed_grouped = df_orders_bucketed.groupby(\"product_id\").agg(F.sum(\"total_amount\").alias(\"total_amount\"))\n",
    "df_orders_bucketed_grouped.show(5)\n",
    "df_orders_bucketed_grouped.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab7b20-da69-452f-866b-3cba16c8042b",
   "metadata": {},
   "source": [
    "***No exchange***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdd7ea-8adf-4056-bb1f-35bede46abbd",
   "metadata": {},
   "source": [
    "### Comparison of GroupBy Operation with and without Bucketing\r\n",
    "\r\n",
    "#### **Without Bucketing**\r\n",
    "- **Physical Plan Overview:**\r\n",
    "  - **Exchange Stage (Step 3):** A `hashpartitioning` operation is required to distribute the data based on the `product_id` column across partitions. This ensures that all rows with the same `product_id` are grouped together for aggregation.\r\n",
    "  - **Multiple Aggregation Stages:**\r\n",
    "    - **Partial Aggregation (Step 2):** A `HashAggregate` operation is performed at the initial stage to calculate partial sums for each partition.\r\n",
    "    - **Final Aggregation (Step 4):** Another `HashAggregate` combines the partial results after data has been shuffled and grouped in the exchange stage.\r\n",
    "  - **Data Source (Step 1):** A CSV file is scanned, and the schema indicates the data includes `product_id` and `total_amount`.\r\n",
    "\r\n",
    "- **Inefficiencies:**\r\n",
    "  - **Exchange Overhead:** The `Exchange` stage involves shuffling data across the cluster, which can be expensive in terms of time and resources.\r\n",
    "  - **Non-Bucketed Source:** The data lacks inherent structure for optimized partitioning, requiring additional computational effort.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### **With Bucketing**\r\n",
    "- **Physical Plan Overview:**\r\n",
    "  - **No Exchange Stage:** The exchange operation is eliminated due to pre-bucketing of the data on the `product_id` column. This ensures data is already distributed into buckets during the write stage.\r\n",
    "  - **Efficient Aggregation Stages:**\r\n",
    "    - **Partial Aggregation (Step 2):** Similar to the non-bucketed case, partial sums are calculated for each bucket.\r\n",
    "    - **Final Aggregation (Step 3):** Combines results from partial aggregations directly without requiring shuffling.\r\n",
    "  - **Data Source (Step 1):** Data is read from a pre-bucketed Parquet file, with `Batched` and `Bucketed` optimizations applied. The `SelectedBucketsCount` confirms that all buckets are used.\r\n",
    "\r\n",
    "- **Efficiencies:**\r\n",
    "  - **Eliminated Exchange Stage:** Data is already partitioned into buckets, which avoids the costly shuffling process.\r\n",
    "  - **Optimized Reads:** The bucketed data structure enables faster access and processing.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Key Observations**\r\n",
    "| **Aspect**           | **Without Bucketing**                                          | **With Bucketing**                                       |\r\n",
    "|-----------------------|---------------------------------------------------------------|---------------------------------------------------------|\r\n",
    "| **Exchange Operation**| Present, requires `hashpartitioning`.                         | Absent, as data is already bucketed.                   |\r\n",
    "| **Data Shuffling**    | Significant overhead due to repartitioning.                   | Eliminated due to pre-bucketing.                       |\r\n",
    "| **Aggregation**       | Two stages: Partial + Final aggregation.                      | Two stages, but no shuffling needed between them.       |\r\n",
    "| **Storage Format**    | CSV (non-bucketed), slower reads.                             | Parquet (bucketed), supports batched and optimized reads. |\r\n",
    "| **Performance Impact**| Higher computational and I/O overhead.                       | Faster and more efficient due to bucketed partitioning. |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Conclusion**\r\n",
    "Bucketing significantly improves the efficiency of `groupBy` operations by eliminating the need for data shuffling (`Exchange` stage) and leveraging optimized storage formats like Parquet. This optimization is particularly beneficial for large datasets where reducing shuffle and partitioning overhead can lead to substantial performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4e2c-e132-4449-ab50-9a746c640e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
