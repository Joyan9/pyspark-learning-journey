{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013fe0e1-cc6b-4c82-beee-86efb3776e40",
   "metadata": {},
   "source": [
    "# Dynamic Partition Pruning\n",
    "## 1. First let's understand Static Partition Pruning\n",
    "- This type of partition pruning (skipping partitions that are not required) occurs before the query execution\n",
    "- Spark looks at the WHERE clause that references the partition columns. Say for instance, `df.filter(F.col(\"date\")=='2025-01-01')` => if `df` is partitioned on date then only the 2025-01-01 partition will be read\n",
    "- Happens during query compilation phase\n",
    "\n",
    "## 2. Dynamic Pruning\n",
    "- This type of pruning is done during the run-time\n",
    "- It's useful in the case of joins where one dataset depends on the value from the other. Say if you wanted to analyse top-performing stores and you two tables `sales` and `popular_stores`. During the join operation with the help of dynamic partition pruning you will not need to read all the partitions from `sales` (presumably the larger table) - instead only the stores that are present in `popular_stores` table\n",
    "- For Dynamic Pruning to be effective the other side of the data set (the look-up table) should be small\n",
    "- **Why is it called Dynamic?**\n",
    "    1. It's occurs during run-time\n",
    "    2. The pruning condition is not static or hard-coded; the filter condition is not known before-hand\n",
    "- One of the data sets needs to be partitioned on the filter column for Dynamic Pruning to work -> say in the above example, if the sales dataset was not partitioned on store_id, then Spark would have to do a complete table scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6ac1c9-4347-4a66-91fc-8e8ca3ea706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(\"spark.driver.memory\",\"10g\")\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"DPP\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b2847b-3c9d-45ac-9814-6eb821e4d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- activity_id: integer (nullable = true)\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- listen_datetime: timestamp (nullable = true)\n",
      " |-- listen_duration: integer (nullable = true)\n",
      " |-- listen_hour: integer (nullable = true)\n",
      " |-- listen_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_listening = spark.read.parquet(\"listening-activity-partitioned\")\n",
    "df_listening.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53e5de1-12bf-461c-9ff7-618bf316c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------------------+---------------+-----------+-----------+\n",
      "|activity_id|song_id|     listen_datetime|listen_duration|listen_hour|listen_date|\n",
      "+-----------+-------+--------------------+---------------+-----------+-----------+\n",
      "|       4456|     16|2023-07-18 10:15:...|            151|         10| 2023-07-18|\n",
      "|       4457|     65|2023-07-18 10:15:...|            181|         10| 2023-07-18|\n",
      "|       4458|     60|2023-07-18 10:15:...|            280|         10| 2023-07-18|\n",
      "|       4459|      3|2023-07-18 10:15:...|            249|         10| 2023-07-18|\n",
      "|       4460|     45|2023-07-18 10:15:...|            130|         10| 2023-07-18|\n",
      "+-----------+-------+--------------------+---------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_listening.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfd6c39-5ed5-42f9-accc-20a56c40814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+--------------------+\n",
      "|song_id| title|artist_id|        release_date|\n",
      "+-------+------+---------+--------------------+\n",
      "|      1|Song_1|        2|2021-10-15 10:15:...|\n",
      "|      2|Song_2|       45|2020-12-07 10:15:...|\n",
      "|      3|Song_3|       25|2022-07-11 10:15:...|\n",
      "|      4|Song_4|       25|2019-03-09 10:15:...|\n",
      "|      5|Song_5|       26|2019-09-07 10:15:...|\n",
      "+-------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_songs = spark.read.csv(\"Spotify_Songs.csv\", header=True, inferSchema=True)\n",
    "df_songs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a237446-5244-41fc-9fd2-8e9e08847194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: integer (nullable = true)\n",
      " |-- release_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_songs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a341de-ae36-4ee1-885f-d4f5b784a712",
   "metadata": {},
   "source": [
    "### **Problem Statement:** Measure the listening activity of songs on their release dates, for all songs released after 2019-12-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67ae8bb-46cf-4a62-9643-ef20497d5095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+--------------------+------------+\n",
      "|song_id| title|artist_id|    release_datetime|release_date|\n",
      "+-------+------+---------+--------------------+------------+\n",
      "|      1|Song_1|        2|2021-10-15 10:15:...|  2021-10-15|\n",
      "|      2|Song_2|       45|2020-12-07 10:15:...|  2020-12-07|\n",
      "|      3|Song_3|       25|2022-07-11 10:15:...|  2022-07-11|\n",
      "|      4|Song_4|       25|2019-03-09 10:15:...|  2019-03-09|\n",
      "|      5|Song_5|       26|2019-09-07 10:15:...|  2019-09-07|\n",
      "+-------+------+---------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's create a date column that can be used for DPP of listening activity\n",
    "df_songs = df_songs.withColumnRenamed(\"release_date\", \"release_datetime\")\\\n",
    "                    .withColumn(\"release_date\", F.to_date(F.col(\"release_datetime\"), \"yyyy-MM-dd\"))\n",
    "df_songs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8beed64-8858-4fd9-b202-e78f0db53f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only songs released after 2019-12-31\n",
    "df_selected_songs = df_songs.filter(F.col(\"release_date\") > F.lit('2019-12-31'))\n",
    "\n",
    "# Join listenting activity with selected songs\n",
    "df_joined = df_listening.join(df_selected_songs,\n",
    "                             on=(df_listening.song_id==df_selected_songs.song_id) & (df_listening.listen_date == df_songs.release_date),\n",
    "                             how=\"inner\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53138acf-63da-42f6-8cec-505c4be3f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (8)\n",
      "+- BroadcastHashJoin Inner BuildRight (7)\n",
      "   :- Filter (2)\n",
      "   :  +- Scan parquet  (1)\n",
      "   +- BroadcastExchange (6)\n",
      "      +- Project (5)\n",
      "         +- Filter (4)\n",
      "            +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [6]: [activity_id#0, song_id#1, listen_datetime#2, listen_duration#3, listen_hour#4, listen_date#5]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/listening-activity-partitioned]\n",
      "PartitionFilters: [isnotnull(listen_date#5), dynamicpruningexpression(listen_date#5 IN dynamicpruning#148)]\n",
      "PushedFilters: [IsNotNull(song_id)]\n",
      "ReadSchema: struct<activity_id:int,song_id:int,listen_datetime:timestamp,listen_duration:int,listen_hour:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [activity_id#0, song_id#1, listen_datetime#2, listen_duration#3, listen_hour#4, listen_date#5]\n",
      "Condition : isnotnull(song_id#1)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/Spotify_Songs.csv]\n",
      "PushedFilters: [IsNotNull(song_id)]\n",
      "ReadSchema: struct<song_id:int,title:string,artist_id:int,release_date:timestamp>\n",
      "\n",
      "(4) Filter\n",
      "Input [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "Condition : (((gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) >= 2020-01-01 00:00:00) AND isnotnull(song_id#55)) AND isnotnull(cast(gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [song_id#55, title#56, artist_id#57, release_date#58 AS release_datetime#90, cast(gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date) AS release_date#95]\n",
      "Input [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [5]: [song_id#55, title#56, artist_id#57, release_datetime#90, release_date#95]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, int, true], input[4, date, true]),false), [plan_id=90]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [2]: [song_id#1, listen_date#5]\n",
      "Right keys [2]: [song_id#55, release_date#95]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) AdaptiveSparkPlan\n",
      "Output [11]: [activity_id#0, song_id#1, listen_datetime#2, listen_duration#3, listen_hour#4, listen_date#5, song_id#55, title#56, artist_id#57, release_datetime#90, release_date#95]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 1 Hosting Expression = listen_date#5 IN dynamicpruning#148\n",
      "AdaptiveSparkPlan (12)\n",
      "+- Project (11)\n",
      "   +- Filter (10)\n",
      "      +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(9) Scan csv \n",
      "Output [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/jovyan/Spotify_Songs.csv]\n",
      "PushedFilters: [IsNotNull(song_id)]\n",
      "ReadSchema: struct<song_id:int,title:string,artist_id:int,release_date:timestamp>\n",
      "\n",
      "(10) Filter\n",
      "Input [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "Condition : (((gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) >= 2020-01-01 00:00:00) AND isnotnull(song_id#55)) AND isnotnull(cast(gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)))\n",
      "\n",
      "(11) Project\n",
      "Output [5]: [song_id#55, title#56, artist_id#57, release_date#58 AS release_datetime#90, cast(gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date) AS release_date#95]\n",
      "Input [4]: [song_id#55, title#56, artist_id#57, release_date#58]\n",
      "\n",
      "(12) AdaptiveSparkPlan\n",
      "Output [5]: [song_id#55, title#56, artist_id#57, release_datetime#90, release_date#95]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1def7-897a-4b29-9c9d-387515e2cf10",
   "metadata": {},
   "source": [
    "### Understanding the Query Plan with DPP\n",
    "\n",
    "\r\n",
    "1. First, there are two main data sources:\r\n",
    "   - A partitioned parquet file containing listening activity (Scan 1)\r\n",
    "   - A CSV file containing song information (Scan 3)\r\n",
    "\r\n",
    "The key indicator of dynamic partition pruning is in Scan (1):\r\n",
    "```\r\n",
    "PartitionFilters: [isnotnull(listen_date#5), dynamicpruningexpression(listen_date#5 IN dynamicpruning#148)]\r\n",
    "```\r\n",
    "\r\n",
    "Here's how the execution flows:\r\n",
    "\r\n",
    "1. The songs table (Scan 3) is filtered first to get songs released after 2020-01-01:\r\n",
    "```\r\n",
    "Condition : (((gettimestamp(release_date#58, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) >= 2020-01-01 00:00:00)\r\n",
    "```\r\n",
    "\r\n",
    "2. This filtered songs dataset is broadcast (BroadcastExchange 6) since it's likely smaller than the listening activity data\r\n",
    "\r\n",
    "3. The dynamic pruning happens through a subquery (shown at bottom as \"Subquery:1\"):\r\n",
    "   - It collects the release dates from the filtered songs\r\n",
    "   - These dates are used to create a filter condition: `listen_date#5 IN dynamicpruning#148`\r\n",
    "   - This filter is then pushed down to the parquet scan, meaning Spark will only read partitions of the listening activity data that match release dates from the songs table\r\n",
    "\r\n",
    "The efficiency comes from:\r\n",
    "- Only reading relevant partitions from the large listening activity dataset\r\n",
    "- Broadcasting the smaller filtered songs dataset\r\n",
    "- Using the release dates from songs to dynamically prune partitions before reading them\r\n",
    "\r\n",
    "This is particularly efficient because:\r\n",
    "1. The songs table is filtered first (removing pre-2020 songs)\r\n",
    "2. The resulting release dates are used to prune partitions in listening activity\r\n",
    "3. Only partitions that could potentially match in the join are read from disk\r\n",
    "\r\n",
    "Without dynamic pruning, Spark would need to read all partitions of the listening activity data before performing the join filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ce5a2-3465-4e60-adea-aefc7560c828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
