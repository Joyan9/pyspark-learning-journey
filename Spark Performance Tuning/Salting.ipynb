{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fe68ec-0167-4ecf-9ffc-a6d156da9dcc",
   "metadata": {},
   "source": [
    "# Salting\n",
    "- Salting refers to the technique in which you add a column to a skewed dataset such that it splits the data much more evenly.\n",
    "- It's one of the ways to handle Data Skew\n",
    "- Say for instance, you have 3 partitions and one of them has 1M records as compared to others having just 10-20 records. With salting you would be adding a column that will split the records evenly (~330K records per partitions)\n",
    "\n",
    "## The Salting Process\n",
    "\r\n",
    "1. First you choose a salt number (let's say 3)\r\n",
    "\r\n",
    "2F**or the LARG **dataset (typically the orders/transactions table):\r\n",
    "   - Add a RANDOM salt number between 0 and 2 to each row\r\n",
    "   - Important: We don't replicate this data\r\n",
    "   - Each row gets just ONE random salt ue\r\n",
    "\r",
    "**\n",
    "3. ForAe SMA**LLER dataset (typically the dimension/lookup table):\r\n",
    "   - REPLICATE each row 3 times (one for each salt 0,1,2)\r\n",
    "   - This is where we use explode or crossJoin\r\n",
    "\r\n",
    "4. The distribution happens when we join:\r\n",
    "   - Records only match when BOTH the join key AND salt values match\r\n",
    "   - This naturally distributes the data like me to clarify any part of this further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5890de48-21b6-48c7-bd63-c6c133c711d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"salting\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f5ff72-6be8-44eb-85ed-abbd09ae9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Query Execution is enabled by default in Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c8932a1-a048-4a62-9ec6-00f87bfd27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46dd6902-c11b-45ca-85d2-87c0e7014ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|1    |\n",
      "|2    |\n",
      "|3    |\n",
      "|4    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a uniform dataset\n",
    "df_uniform = spark.createDataFrame([i for i in range(1000000)], IntegerType())\n",
    "df_uniform.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbc98cf3-16c8-4ba7-9963-76000c8ef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition|count |\n",
      "+---------+------+\n",
      "|0        |124928|\n",
      "|1        |124928|\n",
      "|2        |124928|\n",
      "|3        |124928|\n",
      "|4        |124928|\n",
      "|5        |124928|\n",
      "|6        |124928|\n",
      "|7        |125504|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.spark_partition_id() returns the id of the partition for each row\n",
    "(\n",
    "    df_uniform\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show(15, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a03a4ad-1731-40c4-8515-0b1f2577bdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets create a skewed data set\n",
    "df0 = spark.createDataFrame([0] * 999990, IntegerType()).repartition(1)\n",
    "df1 = spark.createDataFrame([1] * 15, IntegerType()).repartition(1)\n",
    "df2 = spark.createDataFrame([2] * 10, IntegerType()).repartition(1)\n",
    "df3 = spark.createDataFrame([3] * 5, IntegerType()).repartition(1)\n",
    "df_skew = df0.union(df1).union(df2).union(df3)\n",
    "df_skew.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de13e5c-d4f3-4352-97df-f94b25dd25d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|999990|\n",
      "|        1|    15|\n",
      "|        2|    10|\n",
      "|        3|     5|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8213cc-37d9-4ce1-baf1-e7c747b03475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "* Project (21)\n",
      "+- * SortMergeJoin Inner (20)\n",
      "   :- * Sort (15)\n",
      "   :  +- Exchange (14)\n",
      "   :     +- Union (13)\n",
      "   :        :- Exchange (3)\n",
      "   :        :  +- * Filter (2)\n",
      "   :        :     +- * Scan ExistingRDD (1)\n",
      "   :        :- Exchange (6)\n",
      "   :        :  +- * Filter (5)\n",
      "   :        :     +- * Scan ExistingRDD (4)\n",
      "   :        :- Exchange (9)\n",
      "   :        :  +- * Filter (8)\n",
      "   :        :     +- * Scan ExistingRDD (7)\n",
      "   :        +- Exchange (12)\n",
      "   :           +- * Filter (11)\n",
      "   :              +- * Scan ExistingRDD (10)\n",
      "   +- * Sort (19)\n",
      "      +- Exchange (18)\n",
      "         +- * Filter (17)\n",
      "            +- * Scan ExistingRDD (16)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [1]: [value#241]\n",
      "Arguments: [value#241], MapPartitionsRDD[179] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [1]: [value#241]\n",
      "Condition : isnotnull(value#241)\n",
      "\n",
      "(3) Exchange\n",
      "Input [1]: [value#241]\n",
      "Arguments: SinglePartition, REPARTITION_BY_NUM, [plan_id=1492]\n",
      "\n",
      "(4) Scan ExistingRDD [codegen id : 2]\n",
      "Output [1]: [value#243]\n",
      "Arguments: [value#243], MapPartitionsRDD[184] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(5) Filter [codegen id : 2]\n",
      "Input [1]: [value#243]\n",
      "Condition : isnotnull(value#243)\n",
      "\n",
      "(6) Exchange [codegen id : 1]\n",
      "Input [1]: [value#243]\n",
      "Arguments: SinglePartition, REPARTITION_BY_NUM, [plan_id=1494]\n",
      "\n",
      "(7) Scan ExistingRDD [codegen id : 3]\n",
      "Output [1]: [value#245]\n",
      "Arguments: [value#245], MapPartitionsRDD[189] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(8) Filter [codegen id : 3]\n",
      "Input [1]: [value#245]\n",
      "Condition : isnotnull(value#245)\n",
      "\n",
      "(9) Exchange [codegen id : 2]\n",
      "Input [1]: [value#245]\n",
      "Arguments: SinglePartition, REPARTITION_BY_NUM, [plan_id=1496]\n",
      "\n",
      "(10) Scan ExistingRDD [codegen id : 4]\n",
      "Output [1]: [value#247]\n",
      "Arguments: [value#247], MapPartitionsRDD[194] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(11) Filter [codegen id : 4]\n",
      "Input [1]: [value#247]\n",
      "Condition : isnotnull(value#247)\n",
      "\n",
      "(12) Exchange [codegen id : 3]\n",
      "Input [1]: [value#247]\n",
      "Arguments: SinglePartition, REPARTITION_BY_NUM, [plan_id=1498]\n",
      "\n",
      "(13) Union\n",
      "\n",
      "(14) Exchange\n",
      "Input [1]: [value#241]\n",
      "Arguments: hashpartitioning(value#241, 3), ENSURE_REQUIREMENTS, [plan_id=1500]\n",
      "\n",
      "(15) Sort [codegen id : 5]\n",
      "Input [1]: [value#241]\n",
      "Arguments: [value#241 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) Scan ExistingRDD [codegen id : 6]\n",
      "Output [1]: [value#214]\n",
      "Arguments: [value#214], MapPartitionsRDD[166] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(17) Filter [codegen id : 6]\n",
      "Input [1]: [value#214]\n",
      "Condition : isnotnull(value#214)\n",
      "\n",
      "(18) Exchange\n",
      "Input [1]: [value#214]\n",
      "Arguments: hashpartitioning(value#214, 3), ENSURE_REQUIREMENTS, [plan_id=1506]\n",
      "\n",
      "(19) Sort [codegen id : 7]\n",
      "Input [1]: [value#214]\n",
      "Arguments: [value#214 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(20) SortMergeJoin [codegen id : 8]\n",
      "Left keys [1]: [value#241]\n",
      "Right keys [1]: [value#214]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project [codegen id : 8]\n",
      "Output [1]: [value#241]\n",
      "Input [2]: [value#241, value#214]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_c1 = df_skew.join(df_uniform, \"value\", 'inner')\n",
    "df_joined_c1.show(3)\n",
    "df_joined_c1.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a586cdad-da5c-43d1-a760-272567a792d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|partition|count  |\n",
      "+---------+-------+\n",
      "|0        |1000005|\n",
      "|1        |15     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_joined_c1\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .show(5, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfe428-6ad5-4df2-a002-2a98be768df7",
   "metadata": {},
   "source": [
    "### Joining with Salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a65ceb-aea6-417d-928c-c2b746c95a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SALT_NUMBER = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "SALT_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fafb970-d588-4a98-aaf6-a68678cc1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|value|salt|\n",
      "+-----+----+\n",
      "|    0|   1|\n",
      "|    0|   2|\n",
      "|    0|   1|\n",
      "|    0|   0|\n",
      "|    0|   2|\n",
      "|    0|   2|\n",
      "|    0|   0|\n",
      "|    0|   0|\n",
      "|    0|   0|\n",
      "|    0|   2|\n",
      "+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew = df_skew.withColumn(\"salt\", (F.rand()*SALT_NUMBER).cast(\"int\"))\n",
    "df_skew.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f98574b-687e-4f26-a331-dd50777f48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform = df_uniform.withColumn(\"salt_values_array\",\n",
    "                       F.array([F.lit(i) for i in range(SALT_NUMBER)])\n",
    "                       )\\\n",
    "                        .withColumn(\"salt\", \n",
    "                                   F.explode(F.col(\"salt_values_array\"))\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cc1e9a9-d4b8-4519-9e67-5724101c89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+----+\n",
      "|value|salt_values_array|salt|\n",
      "+-----+-----------------+----+\n",
      "|    0|        [0, 1, 2]|   0|\n",
      "|    0|        [0, 1, 2]|   1|\n",
      "|    0|        [0, 1, 2]|   2|\n",
      "|    0|        [0, 1, 2]|   0|\n",
      "|    0|        [0, 1, 2]|   1|\n",
      "|    0|        [0, 1, 2]|   2|\n",
      "|    0|        [0, 1, 2]|   0|\n",
      "|    0|        [0, 1, 2]|   1|\n",
      "|    0|        [0, 1, 2]|   2|\n",
      "|    1|        [0, 1, 2]|   0|\n",
      "+-----+-----------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uniform.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c916a440-2d01-4b09-8a2e-23d309493c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_salted = df_skew.join(df_uniform, on = [\"value\", \"salt\"], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f13e0852-24b1-46d2-8060-9fd5f5cce004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+\n",
      "|value|partition|  count|\n",
      "+-----+---------+-------+\n",
      "|    0|        0|1000293|\n",
      "|    0|        1|1002156|\n",
      "|    0|        2| 997521|\n",
      "|    1|        0|     24|\n",
      "|    1|        1|     21|\n",
      "|    2|        0|     12|\n",
      "|    2|        1|      9|\n",
      "|    2|        2|      9|\n",
      "|    3|        1|      9|\n",
      "|    3|        2|      6|\n",
      "+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_joined_salted\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"value\",\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"value\",\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11626f2-3f5c-45a4-9abd-807ce5b4328c",
   "metadata": {},
   "source": [
    "**You can see that value `0` has been almost evenly distributed in different partitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a6990-6542-4f78-b301-10e2b1938b9b",
   "metadata": {},
   "source": [
    "## Salting in Aggregation\n",
    "n\r\n",
    "1. Choose salt number (say N=3)\r\n",
    "\r\n",
    "2. Assign random salt (0,1,2) to each row\r\n",
    "   Example if aggregating customer purchases:\r\n",
    "   ```\r\n",
    "   (customer1, $100) -> (customer1, $100, salt=0)\r\n",
    "   (customer1, $200) -> (customer1, $200, salt=2)\r\n",
    "   (customer1, $300) -> (customer1, $300, salt=1)\r\n",
    "   ```\r\n",
    "\r\n",
    "3. First shuffle & groupBy(value, salt):\r\n",
    "   - Data gets distributed using hash(customer, salt)\r\n",
    "   - Instead of all customer1 records going to one partition:\r\n",
    "     - (customer1, salt=0) records go to partition X\r\n",
    "     - (customer1, salt=1) records go to partition Y\r\n",
    "     - (customer1, salt=2) records go to partition Z\r\n",
    "\r\n",
    "4. Partial aggregation in each partition:\r\n",
    "   ```\r\n",
    "   Partition X: (customer1, salt=0) -> $100\r\n",
    "   Partition Y: (customer1, salt=1) -> $300\r\n",
    "   Partition Z: (customer1, salt=2) -> $200\r\n",
    "   ```\r\n",
    "\r\n",
    "5. Final groupBy(value) to combine results:\r\n",
    "   - Combines all partial aggregations for each customer\r\n",
    "   - Final result: customer1 -> $600\r\n",
    "\r\n",
    "The key advantage is that the expensive computation (initial aggregation) is distributed across partitions, making it much le of how this works in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68447392-0105-409f-b927-4c2f9164ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|value| count|\n",
      "+-----+------+\n",
      "|    0|999990|\n",
      "|    2|    10|\n",
      "|    3|     5|\n",
      "|    1|    15|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew.groupBy(\"value\",\"salt\").agg(F.count(\"value\").alias(\"count\"))\\\n",
    "        .groupby(\"value\").agg(F.sum(\"count\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bc2db-7345-4d19-a051-2f95c7b1ebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
